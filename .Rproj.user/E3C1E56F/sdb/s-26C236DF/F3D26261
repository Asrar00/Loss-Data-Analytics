{
    "collab_server" : "",
    "contents" : "#Technical Supplement: Statistical Inference\n\n## Overview of Statistical Inference\n\n-   A set of data (a **sample**) has been collected that is considered\n    representative of a larger set (the **population**). This\n    relationship is known as the **sampling frame**.\n\n-   Often, we can describe the distribution of the population in terms\n    of a limited (finite) number of terms called **parameters**. These\n    are referred to as *parametric distributions*. With\n    **nonparametric** analysis, we do not limit ourselves to only a\n    few parameters.\n\n-   The **statistical inference** goal is to say something about\n    the (larger) population based on the observed sample (we “*infer*,”\n    not “*deduce*”). There are three types of statements:\n\n    1.  **Estimation**\n\n    2.  **Hypothesis Testing**\n\n    3.  **Prediction**\n\n#### Wisconsin Property Fund\n\n-   Discuss ideas of statistical inference in the context of a sample\n    from the Wisconsin Property Fund\n\n-   Specifically, consider 1,377 *individual* claims from 2010\n    experience (slightly different from the analysis of 403 average\n    claims in Chapter 1)\n\n  -------------------- --------- ---------- -------- -------- ---------- ------------ -----------\n                                      First                        Third                 Standard\n                         Minimum   Quartile   Median     Mean   Quartile      Maximum   Deviation\n  Claims                       1        788    2,250   26,620      6,171   12,920,000     368,030\n  Logarithmic Claims           0      6.670    7.719    7.804      8.728       16.370       1.683\n  -------------------- --------- ---------- -------- -------- ---------- ------------ -----------\n\n```{r ClaimDistn1, fig.cap='Distribution of Claims', out.width='80%', fig.asp=.75, fig.align='center'}\nClaimLev <- read.csv(\"Data/CLAIMLEVEL.csv\", header=TRUE)\nClaimLevBC10<-subset(ClaimLev,Year==2010); nrow(ClaimLevBC10)\npar(mfrow=c(1, 2))\nhist(ClaimLevBC10$Claim, main=\"\", xlab=\"Claims\")\nhist(log(ClaimLevBC10$Claim), main=\"\", xlab=\"Logarithmic Claims\")\n```\n\n\n#### Sampling Frame\n\n-   In statistics, a sampling frame **error** occurs when the sampling\n    frame, the list from which the sample is drawn, is not an adequate\n    approximation of the population of interest.\n\n-   For the property fund example, the sample consists of all 2010\n    claims\n\n    -   The population might be all claims that could have potentially\n        occurred in 2010.\n\n    -   Or, it might be all claims that could potentially occur, such as\n        in 2010, 2011, and so forth\n\n-   A sample must be a representative subset of a population, or\n    “universe,” of interest. If the sample is not representative, taking\n    a larger sample does not eliminate bias; you simply repeat the same\n    mistake over again and again.\n\n\n-   A sample should be a representative subset of a population, or\n    “universe,” of interest.\n\n-   Formally\n\n    -   We assume that the random variable $X$ represents a draw from a\n        population with distribution function F(.)\n\n    -   We make several such draws ($n$), each unrelated to one another\n        (statistically independent)\n\n    -   Sometimes we say that $X_1, \\ldots, X_n$ is a random sample\n        (with replacement) from F(.)\n\n    -   Sometimes we say that $X_1, \\ldots, X_n$ are identically and\n        independently distributed ($iid$)\n\n#### Describing the Population\n\n-   We think of the random variable $X$ as a draw from the population\n    with distribution function F(.)\n\n-   There are several ways to summarize F(.). We might consider the\n    mean, standard deviation, 95th percentile, and so on.\n\n    -   Because these summary stats do not depend on a specific\n        parametric reference, they are **nonparametric**\n        summary measures.\n\n-   In contrast, we can think of logarithmic claims as normally\n    distributed with mean $\\mu$ and standard deviation $\\sigma$, that\n    is, claims have a *lognormal* distribution\n\n-   We will also look at the gamma distribution, with parameters\n    $\\alpha$ and $\\theta$, as a claims model\n\n    -   The normal, lognormal, and gamma are examples of\n        **parametric** distributions.\n\n    -   The quantities $\\mu$, $\\sigma$, $\\alpha$, and $\\theta$ are known\n        as *parameters*. When we know the parameters of a distribution\n        family, then we have knowledge of the entire distribution.\n\n## Estimation and Prediction\n\n#### Estimation\n\n-   Use $\\theta$ to denote a summary of the population.\n\n    -   Parametric - It can be a parameter from a distribution such as\n        $\\mu$ or $\\sigma$.\n\n    -   Nonparametric - It can also be a nonparametric summary such as\n        the mean or standard deviation.\n\n-   Let $\\hat{\\theta} =\\hat{\\theta}(X_1, \\ldots, X_n)$ be a function of\n    the sample that provides proxy, or **estimate**, of $\\theta$. It is\n    a function of the sample $X_1, \\ldots, X_n$.\n\n-   In our property fund case,\n\n    -   7.804 is a (nonparametric) estimate of the population expected\n        logarithmic claim and 1.683 is an estimate of the corresponding\n        standard deviation.\n\n    -   These are (parametric) estimates of the normal distribution for\n        logarithmic claims\n\n    -   The estimate of the expected claim using the lognormal\n        distribution is 10,106.8 (=$\\exp(7.804+1.683^2/2))$.\n\n#### Lognormal Distribution and Estimation\n\n-   Assume that claims follow a lognormal distribution, so that\n    logarithmic claims follow the familiar normal distribution.\n\n-   Specifically, assume $\\ln X$ has a normal distribution with mean\n    $\\mu$ and variance $\\sigma^2$, sometimes denoted as\n    $X \\sim N(\\mu, \\sigma^2)$.\n\n-   For the property data, estimates are $\\hat{\\mu} =7.804$ and\n    $\\hat{\\sigma} = 1.683$. The “hat” notation is common. These are said\n    to be **point estimates**, a single approximation of the\n    corresponding parameter.\n\n-   Under general maximum likelihood theory (that we will do in a little\n    bit), these estimates typically have a normal distribution for\n    large samples.\n\n    -   Using notation, $\\hat{\\theta}$ has an approximate normal\n        distribution with mean $\\theta$ and variance, say,\n        $\\mathrm{Var}(\\hat{\\theta})$.\n\n    -   Take the square root of the variance and plug-in the estimate to\n        define $se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}$. A\n        **standard error** is an estimated standard deviation.\n\n    -   The next step in the mathematical statistics theory is to\n        establish that $(\\hat{\\theta}-\\theta)/se(\\hat{\\theta})$ has a\n        $t$-distribution with “degrees of freedom” (a parameter of\n        the distribution) equal to the sample size minus the dimension\n        of $\\theta$.\n\n\n-   Assume that claims follow a lognormal distribution, so that\n    logarithmic claims follow the familiar normal distribution.\n\n-   Under general maximum likelihood theory\n\n    -    $\\hat{\\theta}$ has an approximate normal distribution with mean\n        $\\theta$ and variance, say, $\\mathrm{Var}(\\hat{\\theta})$.\n\n    -   Take the square root of the variance and plug-in the estimate to\n        define $se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}$. A\n        **standard error** is an estimated standard deviation.\n\n    -   $(\\hat{\\theta}-\\theta)/se(\\hat{\\theta})$ has a $t$-distribution\n        with “degrees of freedom” (a parameter of the distribution)\n        equal to the sample size minus the dimension of $\\theta$.\n\n    -   As an application, we can invert this result to get a\n        **confidence interval** for $\\theta$.\n\n-   A pair of statistics, $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$, provide\n    an interval of the form $[\\hat{\\theta}_1, \\hat{\\theta}_2]$ This\n    interval is a $1-\\alpha$ confidence interval for $\\theta$ if\n    $\\Pr\\left(\\hat{\\theta}_1 \\le \\theta \\le \\hat{\\theta}_2\\right) \\ge 1-\\alpha.$\n\n-   For example,\n    $\\hat{\\theta}_1 = \\hat{\\mu} - (t-value) \\hat{\\sigma}/\\sqrt{n}$ and\n    $\\hat{\\theta}_2 = \\hat{\\mu} + (t-value) \\hat{\\sigma}/\\sqrt{n}$\n    provide a confidence interval for $\\theta=\\mu$. When\n    $\\alpha = 0.05$, $t-value \\approx 1.96$.\n\n-   For the property fund, (7.715235, 7.893208) is a 95% confidence\n    interval for $\\mu$.\n\n#### Lognormal Distribution and Hypothesis Testing\n\nAn important statistical inference procedure involves verifying ideas\nabout parameters.\n\n-   To illustrate, in the property fund, assume that mean logarithmic\n    claims have historically been approximately been\n    $\\mu_0 = log(5000)= 8.517$. I might want to use 2010 data to see\n    whether the mean of the distribution has changed. I also might want\n    to test whether it has increased.\n\n-   The actual 2010 average was $\\hat{\\mu} =7.804$. Is this a\n    significant departure from $\\mu_0 = 8.517$?\n\n-   One way to think about it is in terms of standard errors. The\n    deviation is $(8.517-7.804)/(1.683/\\sqrt{1377}) = 15.72$\n    standard errors. This is highly unlikely assuming an approximate\n    normal distribution.\n\n-   One hypothesis testing procedure begin with the calculation the test\n    statistic $t-stat=(\\hat{\\theta}-\\theta_0)/se(\\hat{\\theta})$. Here,\n    $\\theta_0$ is an assumed value of the parameter.\n\n-   Then, one rejects the hypothesized value if the test statistic\n    $t-stat$ is “unusual.” To gauge “unusual,” use the same\n    $t$-distribution as introduced for confidence intervals.\n\n-   If you only want to know about a difference, this is known as a\n    “two-sided” test; use the same $t-value$ as the case for\n    confidence intervals.\n\n-   If you want to investigate whether there has been an increase (or\n    decrease), then use a “one-sided” test.\n\n-   Another useful concept in hypothesis testing is the $p$-value, which\n    is short hand for probability value. For a data set, a $p$-value is\n    defined to be the smallest significance level for which the null\n    hypothesis would be rejected.\n\n#### Property Fund – Other Distributions\n\n-   For numerical stability and extensions to regression applications,\n    statistical packages often work with transformed version of\n    parameters\n\n-   The following estimates are from the **R** package **VGAM**\n    (the function)\n\n  -------------- ----------- ---------- ----------\n  Distribution     Parameter   Standard   $t$-stat\n                    Estimate      Error \n  Gamma               10.190      0.050    203.831\n                      -1.236      0.030    -41.180\n  Lognormal            7.804      0.045    172.089\n                       0.520      0.019     27.303\n  Pareto               7.733      0.093     82.853\n                      -0.001      0.054     -0.016\n  GB2                  2.831      1.000      2.832\n                       1.203      0.292      4.120\n                       6.329      0.390     16.220\n                       1.295      0.219      5.910\n  -------------- ----------- ---------- ----------\n\n## Maximum Likelihood Theory\n\n### Likelihood Function\n\n-   Let $\\mathrm{f}(\\cdot;\\boldsymbol\\theta)$ be the probability mass\n    function if $X$ is discrete or the probability density function if\n    it is continuous.\n\n-   The likelihood is a function of the parameters\n    ($\\boldsymbol \\theta$) with the data ($\\mathbf{x}$) fixed rather\n    than a function of the data with the parameters fixed.\n\n-   Define the *log-likelihood function*,\n    $$L(\\boldsymbol \\theta) = L(\\mathbf{x};\\boldsymbol \\theta ) = \\ln \\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta) = \\sum_{i=1}^n \\ln \\mathrm{f}(x_i;\\boldsymbol \\theta),$$\n    evaluated at a realization $\\mathbf{x}$.\n\n-   In the case of independence, the joint density function can be\n    expressed as a product of the marginal density functions and, by\n    taking logarithms, we can work with sums.\n\n#### Example. Pareto Distribution\n\n-   Suppose that $X_1, \\ldots, X_n$ represent a random sample from a\n    single-parameter Pareto with cumulative distribution function:\n    $$\\mathrm{F}(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x>500 .$$\n\n-   In this case, the single parameter is $\\theta = \\alpha$.\n\n-   The corresponding probability density function is\n    $\\mathrm{f}(x) = 500^{\\alpha} \\alpha x^{-\\alpha-1}$ and the\n    logarithmic likelihood is\n    $$L(\\boldsymbol \\alpha) = \\sum_{i=1}^n \\ln \\mathrm{f}(x_i;\\alpha) = n \\alpha \\ln 500 +n \\ln \\alpha -(\\alpha+1)  \\sum_{i=1}^n \\ln x_i .$$\n\n#### Properties of Likelihood Functions\n\n-   One basic property of likelihood functions is:\n    $$\\label{E11:ScoreZero}\n    \\mathrm{E} \\left( \\frac{ \\partial}{\\partial \\boldsymbol \\theta}\n    L(\\boldsymbol \\theta) \\right) = \\mathbf 0$$\n\n-   The derivative of the log-likelihood function,\n    $\\partial L(\\boldsymbol \\theta)/\\partial \\boldsymbol \\theta$, is\n    called the *score function*.\n\n-   To see this, $$\\begin{aligned}\n    \\mathrm{E} \\left( \\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) \\right)\n    &= \\mathrm{E} \\left( \\frac{\\frac{\\partial}{\\partial \\boldsymbol \\theta}\\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta)}{\\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta )}  \\right)\n    = \\int\\frac{\\partial}{\\partial \\boldsymbol \\theta} \\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta ) d \\mathbf y \\\\\n    &= \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\int \\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta ) d \\mathbf y\n    = \\frac{\\partial}{\\partial \\boldsymbol \\theta} 1 = \\mathbf 0.\\end{aligned}$$\n\n\n-   Another basic property is: $$\n    \\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta\n    \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right)\n    + \\mathrm{E} \\left( \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial\n    \\boldsymbol \\theta} \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial\n    \\boldsymbol \\theta^{\\prime}}\n     \\right) = \\mathbf 0.$$\n\n-   With this, we can define the *information matrix*\n    $$\n    \\mathbf{I}(\\boldsymbol \\theta) = \\mathrm{E} \\left( \\frac{ \\partial\n    L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta} \\frac{ \\partial\n    L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta^{\\prime}}\n     \\right) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta\n    \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right).$$\n\n-   In general\n    $$\\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta)\n    =\\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln \\prod_{i=1}^n\n    \\mathrm{f}(x_i;\\boldsymbol \\theta ) =\\sum_{i=1}^n \\frac{\n    \\partial}{\\partial \\boldsymbol \\theta}\n    \\ln \\mathrm{f}(x_i;\\boldsymbol \\theta ).$$ has a large sample\n    **normal distribution** with mean **0** and variance\n    $\\mathbf{I}(\\boldsymbol \\theta)$.\n\n#### Maximum Likelihood Estimators\n\n-   The value of $\\boldsymbol \\theta$, say $\\boldsymbol \\theta_{MLE}$,\n    that maximizes $\\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta)$ is called\n    the *maximum likelihood estimator*.\n\n-   Maximum likelihood estimators are values of the parameters\n    $\\boldsymbol \\theta$ that are “most likely” to have been produced by\n    the data.\n\n-   Because $\\ln(\\cdot)$ is a one-to-one function, we can also determine\n    $\\boldsymbol \\theta_{MLE}$ by maximizing the log-likelihood\n    function, $L(\\boldsymbol \\theta)$.\n\n**Example. Course C/Exam 4. May 2000, 21.** You are given the following\nfive observations: 521, 658, 702, 819, 1217. You use the\nsingle-parameter Pareto with cumulative distribution function:\n$$\\mathrm{F}(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x>500 .$$\nCalculate the maximum likelihood estimate of the parameter $\\alpha$.\n\n#### Instructor Notes\n\n**Example. Course C/Exam 4. May 2000, 21.** You are given the following\nfive observations: 521, 658, 702, 819, 1217. You use the\nsingle-parameter Pareto with cumulative distribution function:\n$$\\mathrm{F}(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x>500 .$$\nCalculate the maximum likelihood estimate of the parameter $\\alpha$.\n\n*Solution*. With $n=5$, the logarithmic likelihood is\n$$L(\\alpha ) =  \\sum_{i=1}^5 \\ln \\mathrm{f}(x_i;\\alpha ) =  5 \\alpha \\ln 500 + 5 \\ln \\alpha\n-(\\alpha+1) \\sum_{i=1}^5 \\ln x_i.$$ Solving for the root of the score\nfunction yields\n$$\\frac{ \\partial}{\\partial \\alpha } L(\\alpha ) =    5  \\ln 500 + 5 / \\alpha -  \\sum_{i=1}^5 \\ln x_i\n=_{set} 0 \\Rightarrow \\alpha_{MLE} = \\frac{5}{\\sum_{i=1}^5 \\ln x_i - 5  \\ln 500 } = 2.453 .$$\n\n#### Asymptotic Normality of Maximum Likelihood Estimators\n\n-   Under broad conditions, $\\boldsymbol \\theta_{MLE}$ has a large\n    sample normal distribution with mean $\\boldsymbol \\theta$ and\n    variance $\\left( \\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1}$.\n\n-   $2 \\left( L(\\boldsymbol \\theta_{MLE}) - L(\\boldsymbol \\theta) \\right)$\n    has a chi-square distribution with degrees of freedom equal to the\n    dimension of $\\boldsymbol \\theta$ .\n\n-   These are critical results upon which much of estimation and\n    hypothesis testing is based.\n\n    **Example. Course C/Exam 4. Nov 2000, 13.** A sample of ten\n    observations comes from a parametric family\n    $f(x,; \\theta_1, \\theta_2)$ with log-likelihood function\n    $$L(\\theta_1, \\theta_2)= \\sum_{i=1}^{10} f(x_i; \\theta_1, \\theta_2) = -2.5 \\theta_1^2 - 3\n    \\theta_1 \\theta_2 - \\theta_2^2 + 5 \\theta_1 + 2 \\theta_2 + k,$$\n    where $k$ is a constant. Determine the estimated covariance matrix\n    of the maximum likelihood estimator, $\\hat{\\theta_1}, \\hat{\\theta_2}$.\n\n#### Instructor Notes\n\n**Example. Course C/Exam 4. Nov 2000, 13.** A sample of ten observations\ncomes from a parametric family $f(x,; \\theta_1, \\theta_2)$ with\nlog-likelihood function\n$$L(\\theta_1, \\theta_2)= \\sum_{i=1}^{10} f(x_i; \\theta_1, \\theta_2) = -2.5 \\theta_1^2 - 3\n\\theta_1 \\theta_2 - \\theta_2^2 + 5 \\theta_1 + 2 \\theta_2 + k,$$ where\n$k$ is a constant. Determine the estimated covariance matrix of the\nmaximum likelihood estimator, $\\hat{\\theta_1}, \\hat{\\theta_2}$.\n\n*Solution*. The matrix of second derivatives is $$\\left(\n\\begin{array}{cc}\n  \\frac{ \\partial ^2}{\\partial \\theta_1 ^2 } L & \\frac{ \\partial ^2}{\\partial \\theta_1 \\partial \\theta_2 } L  \\\\\n  \\frac{ \\partial ^2}{\\partial \\theta_1 \\partial \\theta_2 } L & \\frac{ \\partial ^2}{\\partial \\theta_1 ^2 } L\n\\end{array} \\right) =\n\\left(\n\\begin{array}{cc}\n  -5 & -3  \\\\\n  -3 & -2\n\\end{array} \\right)$$ Thus, the information matrix is:\n$$\\mathbf{I}(\\theta_1, \\theta_2) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta\n\\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right) = \\left(\n\\begin{array}{cc}\n  5 & 3  \\\\\n  3 & 2\n\\end{array} \\right)$$ and\n$$\\mathbf{I}^{-1}(\\theta_1, \\theta_2) = \\frac{1}{5(2) - 3(3)}\\left(\n\\begin{array}{cc}\n  2 & -3  \\\\\n  -3 & 5\n\\end{array} \\right) = \\left(\n\\begin{array}{cc}\n  2 & -3  \\\\\n  -3 & 5\n\\end{array} \\right) .$$\n\n#### Maximum Likelihood Estimation (MLE)\n\n-   Why use maximum likelihood estimation?\n\n    -   General purpose tool - works in many situations (data can be\n        censored, truncated, include covariates, time-dependent, and\n        so forth)\n\n    -   It is “optimal,” the best, in the sense that it has the smallest\n        variance among the class of all unbiased estimators. (Caveat:\n        for large sample sizes).\n\n-   A drawback: Generally, maximum likelihood estimators are computed\n    iteratively, no closed-form solution.\n\n    -   For example, you may recall a “Newton-Raphson” iterative\n        algorithm from calculus\n\n    -   Iterative algorithms require starting values. For some problems,\n        the choice of a close starting value is critical.\n\n#### MLE and Statistical Significance\n\nOne important type inference is to say whether a parameter estimate is\n“statistically significant”\n\n-   We learned earlier that $\\boldsymbol \\theta_{MLE}$ has a large\n    sample normal distribution with mean $\\boldsymbol \\theta$ and\n    variance $\\left( \\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1}$.\n\n-   Look to the $j$th element of $\\boldsymbol \\theta_{MLE}$, say\n    $\\theta_{MLE,j}$.\n\n-   Define $se(\\theta_{MLE,j})$, the standard error (estimated\n    standard deviation) to be square root of the $j$ diagonal element of\n    $\\left( \\mathbf{I}(\\boldsymbol \\theta)_{MLE} \\right)^{-1}$.\n\n-   To assess the hypothesis that $\\theta_j$ is 0, we look at the\n    rescaled estimate\n    $t(\\theta_{MLE,j})=\\theta_{MLE,j}/se(\\theta_{MLE,j})$. It is said to\n    be a $t$-statistic or $t$-ratio.\n\n-   Under this hypothesis, it has a $t$-distribution with degrees of\n    freedom equal to the sample size minus the dimension of\n    $\\boldsymbol \\theta_{MLE}$.\n\n-   For most actuarial applications, the $t$-distribution is very close\n    to the (standard) normal distribution. Thus, sometimes this ratio is\n    also known a $z$-statistic or “$z$-score.”\n\n####Assessing Statistical Significance\n\n-   If the $t$-statistic $t(\\theta_{MLE,j})$ exceeds a cut-off (in\n    absolute value), then the $j$th variable is said to be\n    “statistically significant.”\n\n    -   For example, if we use a 5% significance level, then the cut-off\n        is 1.96 using a normal distribution approximation.\n\n    -   More generally, using a $100 \\alpha \\%$ significance level, then\n        the cut-off is a $100(1-\\alpha/2)\\%$ quantile from a\n        $t$-distribution using degrees of freedom equal to the sample\n        size minus the dimension of $\\boldsymbol \\theta_{MLE}$.\n\n-   Another useful concept in hypothesis testing is the $p$-value,\n    shorthand for probability value.\n\n    -   For a data set, a $p$-value is defined as the smallest\n        significance level for which the null hypothesis would\n        be rejected.\n\n    -   The $p$-value is a useful summary statistic for the data analyst\n        to report because it allows the reader to understand the\n        strength of the deviation from the null hypothesis.\n\n#### MLE and Model Validation\n\nAnother important type inference is to select a model from two choices,\nwhere one choice is a subset of the other\n\n-   Suppose that we have a (large) model and determine the maximum\n    likelihood estimator, $\\boldsymbol \\theta_{MLE}$.\n\n-   Now assume that $p$ elements in $\\boldsymbol \\theta$ are equal to\n    zero and determine the maximum likelihood estimator over the\n    remaining set. Call this estimator $\\boldsymbol \\theta_{Reduced}$\n\n-   The statistic,\n    $LRT= 2 \\left( L(\\boldsymbol \\theta_{MLE}) - L(\\boldsymbol \\theta_{Reduced}) \\right)$,\n    is called the likelihood ratio (a difference of the logs is the log\n    of the ratio. Hence, the term “ratio.”)\n\n-   Under the hypothesis that the reduce model is correct, the\n    likelihood ratio has a chi-square distribution with degrees of\n    freedom equal to $p$, the number of variables set equal to zero.\n\n-   This allows us to judge which of the two models is correct. If the\n    statistic $LRT$ is large relative to the chi-square distribution,\n    then we reject the simpler, reduced, model in favor of the\n    larger one.\n\n### Information Criteria\n\n-   These statistics can be used when comparing several alternative\n    models that are not necessarily nested. One picks the model that\n    minimizes the criterion.\n\n-   *Akaike’s Information Criterion*\n    $$AIC = -2 \\times L(\\boldsymbol \\theta_{MLE}) + 2 \\times (number~of~parameters)$$\n\n    -   The additional term $2 \\times \\text{(number of parameters)}$ is a\n        penalty for the complexity of the model.\n\n    -   Other things equal, a more complex model means more parameters,\n        resulting in a larger value of the criterion.\n\n-   *Bayesian Information Criterion*, defined as\n    $$BIC = -2 \\times L(\\boldsymbol \\theta_{MLE}) + (number~of~parameters) \\times \\ln (number~of~observations)$$\n\n    -   This measure gives greater weight to the number of parameters.\n\n    -   Other things being equal, $BIC$ will suggest a more parsimonious\n        model than $AIC$.\n\n#### Property Fund Information Criteria\n\n-   Both the $AIC$ and $BIC$ statistics suggest that the *GB2* is the\n    best fitting model whereas gamma is the worst.\n\n  Distribution          AIC        BIC\n  -------------- ---------- ----------\n  Gamma            28,305.2   28,315.6\n  Lognormal        26,837.7   26,848.2\n  Pareto           26,813.3   26,823.7\n  GB2              26,768.1   26,789.0\n\n#### Property Fund Fitted Distributions\n\n-   In this graph, black represents actual (smoothed) logarithmic claims\n\n-   Best approximated by green which is fitted GB2\n\n-   Pareto (purple) and Lognormal (lightblue) are also pretty good\n\n-   Worst are the exponential (in red) and gamma (in dark blue)\n\n\n```{r FitClaimDistn, fig.cap='Fitted Claims Distribution', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}\nClaimLev <- read.csv(\"Data/CLAIMLEVEL.csv\", header=TRUE); nrow(ClaimLev)\nClaimData<-subset(ClaimLev,Year==2010); \n#Use \"VGAM\" library for estimation of parameters \nlibrary(VGAM)\nfit.LN <- vglm(Claim ~ 1, family=lognormal, data = ClaimData)\nfit.gamma <- vglm(Claim ~ 1, family=gamma2, data = ClaimData)\n  theta.gamma<-exp(coef(fit.gamma)[1])/exp(coef(fit.gamma)[2]) \n  alpha.gamma<-exp(coef(fit.gamma)[2])\nfit.exp <- vglm(Claim ~ 1, exponential, data = ClaimData)\nfit.pareto <- vglm(Claim ~ 1, paretoII, loc=0, data = ClaimData)\n\n###################################################\n#  Inference assuming a GB2 Distribution - this is more complicated\n# The likelihood functon of GB2 distribution (negative for optimization)\nlikgb2 <- function(param) {\n  a1 <- param[1]\n  a2 <- param[2]\n  mu <- param[3]\n  sigma <- param[4]\n  yt <- (log(ClaimData$Claim)-mu)/sigma\n  logexpyt<-ifelse(yt>23,yt,log(1+exp(yt)))\n  logdens <- a1*yt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpyt -log(ClaimData$Claim) \n  return(-sum(logdens))\n}\n#  \"optim\" is a general purpose minimization function\ngb2bop <- optim(c(1,1,0,1),likgb2,method=c(\"L-BFGS-B\"),\n                lower=c(0.01,0.01,-500,0.01),upper=c(500,500,500,500),hessian=TRUE)\n###################################################\n# Plotting the fit using densities (on a logarithmic scale)\nplot(density(log(ClaimData$Claim)), ylim=c(0,0.36),main=\"\", xlab=\"Log Expenditures\")\nx <- seq(0,15,by=0.01)\nfexp_ex = dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1)*exp(x)\nlines(x,fexp_ex, col=\"red\")\nfgamma_ex = dgamma(exp(x), shape = alpha.gamma, scale=theta.gamma)*exp(x)\nlines(x,fgamma_ex,col=\"blue\")\nfpareto_ex = dparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1]))*exp(x)\nlines(x,fpareto_ex,col=\"purple\")\nflnorm_ex = dlnorm(exp(x), mean = coef(fit.LN)[1], sd = exp(coef(fit.LN)[2]))*exp(x)\nlines(x,flnorm_ex, col=\"lightblue\")\n# density for GB II\ngb2density <- function(x){\n  a1 <- gb2bop$par[1]\n  a2 <- gb2bop$par[2]\n  mu <- gb2bop$par[3]\n  sigma <- gb2bop$par[4]\n  xt <- (log(x)-mu)/sigma\n  logexpxt<-ifelse(xt>23,yt,log(1+exp(xt)))\n  logdens <- a1*xt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpxt -log(x) \n  exp(logdens)\n}\nfGB2_ex = gb2density(exp(x))*exp(x)\nlines(x,fGB2_ex, col=\"green\")\n```\n<h5 style=\"text-align: center;\"><a id=\"display.FitClaimDistn.1\" href=\"javascript:togglecode('display.FitClaimDistn.2','display.FitClaimDistn.1');\"><i><strong>R Code for Fitted Claims Distributions</strong></i></a> </h5>\n<div id=\"display.FitClaimDistn.2\" style=\"display: none\">\n\n\n```\n# R Code to fit several claims distributions\nClaimLev <- read.csv(\"Data/CLAIMLEVEL.csv\", header=TRUE); nrow(ClaimLev)\nClaimData<-subset(ClaimLev,Year==2010); \n#Use \"VGAM\" library for estimation of parameters \nlibrary(VGAM)\nfit.LN <- vglm(Claim ~ 1, family=lognormal, data = ClaimData)\nfit.gamma <- vglm(Claim ~ 1, family=gamma2, data = ClaimData)\n  theta.gamma<-exp(coef(fit.gamma)[1])/exp(coef(fit.gamma)[2]) \n  alpha.gamma<-exp(coef(fit.gamma)[2])\nfit.exp <- vglm(Claim ~ 1, exponential, data = ClaimData)\nfit.pareto <- vglm(Claim ~ 1, paretoII, loc=0, data = ClaimData)\n\n###################################################\n#  Inference assuming a GB2 Distribution - this is more complicated\n# The likelihood functon of GB2 distribution (negative for optimization)\nlikgb2 <- function(param) {\n  a1 <- param[1]\n  a2 <- param[2]\n  mu <- param[3]\n  sigma <- param[4]\n  yt <- (log(ClaimData$Claim)-mu)/sigma\n  logexpyt<-ifelse(yt>23,yt,log(1+exp(yt)))\n  logdens <- a1*yt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpyt -log(ClaimData$Claim) \n  return(-sum(logdens))\n}\n#  \"optim\" is a general purpose minimization function\ngb2bop <- optim(c(1,1,0,1),likgb2,method=c(\"L-BFGS-B\"),\n                lower=c(0.01,0.01,-500,0.01),upper=c(500,500,500,500),hessian=TRUE)\n###################################################\n# Plotting the fit using densities (on a logarithmic scale)\nplot(density(log(ClaimData$Claim)), ylim=c(0,0.36),main=\"\", xlab=\"Log Expenditures\")\nx <- seq(0,15,by=0.01)\nfexp_ex = dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1)*exp(x)\nlines(x,fexp_ex, col=\"red\")\nfgamma_ex = dgamma(exp(x), shape = alpha.gamma, scale=theta.gamma)*exp(x)\nlines(x,fgamma_ex,col=\"blue\")\nfpareto_ex = dparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1]))*exp(x)\nlines(x,fpareto_ex,col=\"purple\")\nflnorm_ex = dlnorm(exp(x), mean = coef(fit.LN)[1], sd = exp(coef(fit.LN)[2]))*exp(x)\nlines(x,flnorm_ex, col=\"lightblue\")\n# density for GB II\ngb2density <- function(x){\n  a1 <- gb2bop$par[1]\n  a2 <- gb2bop$par[2]\n  mu <- gb2bop$par[3]\n  sigma <- gb2bop$par[4]\n  xt <- (log(x)-mu)/sigma\n  logexpxt<-ifelse(xt>23,yt,log(1+exp(xt)))\n  logdens <- a1*xt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpxt -log(x) \n  exp(logdens)\n}\nfGB2_ex = gb2density(exp(x))*exp(x)\nlines(x,fGB2_ex, col=\"green\")\n```\n\n</div>\n\n",
    "created" : 1498593686350.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "102057617",
    "id" : "F3D26261",
    "lastKnownWriteTime" : 1484856794,
    "last_content_update" : 1484856794,
    "path" : "D:/Dropbox/AAOnlineTextProject/LossDataAnalyticsJune2017/Chapters/TexSuppStatisticalInference_16Dec2016Jan19.Rmd",
    "project_path" : "Chapters/TexSuppStatisticalInference_16Dec2016Jan19.Rmd",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}