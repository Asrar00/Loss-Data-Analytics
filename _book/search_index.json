[
["index.html", "Loss Data Analytics Preface", " Loss Data Analytics An open text authored by the Actuarial Community 2017-06-27 Preface Book Description Loss Data Analytics is an interactive, online, freely available text. The online version contains many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. A subset of the book is available for offline reading in pdf and EPUB formats. The online text will be available in multiple languages to promote access to a worldwide audience. What will success look like? The online text will be freely available to a worldwide audience. The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. Moreover, a subset of the book will be available in pdf format for low-cost printing. The online text will be available in multiple languages to promote access to a worldwide audience. How will the text be used? This book will be useful in actuarial curricula worldwide. It will cover the loss data learning objectives of the major actuarial organizations. Thus, it will be suitable for classroom use at universities as well as for use by independent learners seeking to pass professional actuarial examinations. Moreover, the text will also be useful for the continuing professional development of actuaries and other professionals in insurance and related financial risk management industries. Why is this good for the profession? An online text is a type of open educational resource (OER). One important benefit of an OER is that it equalizes access to knowledge, thus permitting a broader community to learn about the actuarial profession. Moreover, it has the capacity to engage viewers through active learning that deepens the learning process, producing analysts more capable of solid actuarial work. Why is this good for students and teachers and others involved in the learning process? Cost is often cited as an important factor for students and teachers in textbook selection (see a recent post on the $400 textbook). Students will also appreciate the ability to “carry the book around” on their mobile devices. Why loss data analytics? Although the intent is that this type of resource will eventually permeate throughout the actuarial curriculum, one has to start somewhere. Given the dramatic changes in the way that actuaries treat data, loss data seems like a natural place to start. The idea behind the name loss data analytics is to integrate classical loss data models from applied probability with modern analytic tools. In particular, we seek to recognize that big data (including social media and usage based insurance) are here and high speed computation s readily available. Project Goal The project goal is to have the actuarial community author our textbooks in a collaborative fashion. To get involved, please visit our Loss Data Analytics Project Site. "],
["introduction-to-loss-data-analytics.html", "Chapter 1 Introduction to Loss Data Analytics 1.1 Relevance of Analytics 1.2 Variable Types 1.3 Insurance Company Operations 1.4 Case Study: Wisconsin Property Fund 1.5 Further Resources and Contributors", " Chapter 1 Introduction to Loss Data Analytics Chapter Preview. This book introduces readers to methods of analyzing insurance data. Section 1 begins with a discussion of why the use of data is important in the insurance industry. Yes, this is obvious but we need to make a strong case for it as this is the whole premise of the book. Next, Section 2 provides an overview of the types of data that one encounters. There are many types from which to choose; your first step in the analysis of data is identify a broad class to help direct you to the appropriate tools and techniques. Section 3 gives a general overview of the purposes of analyzing insurance data which is reinforced in the Section 4 case study. Naturally, there is a huge gap between identifying the broad class of variables and what we learn from the data; this gap is covered through the methods and techniques of data analysis covered in the rest of the text. 1.1 Relevance of Analytics In this section, you learn how to: Motivate the relevance of insurance Describe analytics Describe data generating events associated with the timeline of a typical insurance contract This book introduces the process of using data to make decisions in an insurance context. It does not assume that readers are familiar with insurance but introduces insurance concepts as needed. Insurance may not be as entertaining as the sports industry nor as widely familiar as the agricultural industry but it does affect the financial livelihoods of many. By almost any measure, insurance is a major economy activity. On a global level, insurance premiums comprised about 6.3% of the world gross domestic product (GDP) in 2013, (Insurance Information Institute 2015). To illustrate, premiums accounted for 17.6% of GDP in Taiwan (the highest in the study) and represented 7.5% of GDP in the United States. On a personal level, almost everyone owning a home has insurance to protect themselves in the event of a fire, hailstorm, or some other calamitous event. Almost every country requires insurance for those driving a car. So, although not particulary entertaining nor widely familiar, insurance is an important piece of the economy and relevant to individual livelihoods. Insurance is a data-driven industry. Like other major corporations, insurers use data when trying to decide how much to pay employees, how many employees to retain, how to market their services, how to forecast financial trends, and so on. Although each industry retains its own nuances, these represent general areas of activities that are not specific to the insurance industry. You will find that the data methods and tools introduced in this text relevant for these general areas. Moreover, when introducing data methods, we will focus on losses that potentially arise from obligations in insurance contracts. This could be the amount of damage to one’s apartment under a renter’s insurance agreement, the amount needed to compensate someone that you hurt in a driving accident, and the like. We will call these insurance claims or loss amounts. With this focus, we will be able to introduce generally applicable statistical tools in techniques in real-life situations where the tools can be used directly. 1.1.1 What is Analytics? Insurance is a data-driven industry and analytics is a key to deriving information from data. But what is analytics? Making data-driven business decisions has been described as business analytics, business intelligence, and data science. These terms, among others, are sometimes used interchangeably and sometimes used separately, referring to distinct domains of applications. As an example of such distinctions, business intelligence may focus on processes of collecting data, often through databases and data warehouses, whereas business analytics utilizes tools and methods for statistical analyses of data. In contrast to these two terms that emphasize business applications, the term data science can encompass broader applications in many scientific domains. For our purposes, we use the term analytics to refer to the process of using data to make decisions. This process involves gathering data, understanding models of uncertainty, making general inferences, and communicating results. 1.1.2 Short-term Insurance This text will focus on short-term insurance contracts. By short-term, we mean contracts where the insurance coverage is typically provided for six months or a year. If you are new to insurance, then it is probably easiest to think about an insurance policy that covers the contents of an apartment or house that you are renting (known as renters insurance) or the contents and property of a building that is owned by you or a friend (known as homeowners insurance). Another easy example is automobile insurance. In the event of an accident, this policy may cover damage to your vehicle, damage to other vehicles in the accident, as well as medical expenses of those injured in the accident. In the US, policies such as renters and homeowners are known as property insurance whereas a policy such as auto that covers medical damages to people is known as casualty insurance. In the rest of the world, these are both known as nonlife or general insurance, to distinguish them from life insurance. Both life and nonlife insurances are important. To illustrate, (Insurance Information Institute 2015) estimates that direct insurance premiums in the world for 2013 was 2,608,091 for life and 2,032,850 for nonlife; these figures are in millions of US dollars. As noted earlier, the total represents 6.3% of the world GDP. Put another way, life accounts for 56.2% of insurance premiums and 3.5% of world GDP, nonlife accounts for 43.8% of insurance premiums and 2.7% of world GDP. Both life and nonlife represent important economic activities and are worthy of study in their own right. Yet, life insurance considerations differ from nonlife. In life insurance, the default is to have a multi-year contract. For example, if a person 25 years old purchases a whole life policy that pays upon death of the insured and that person does not die until age 100, then the contract is in force for 75 years. We think of this as a long-term contract. Further, in life insurance, the benefit amount is often stipulated in the contract provisions. In contrast, most short-term contracts provide for reimbursement of insured losses which are unknown before the accident. (Of course, there are usually limits placed on the reimbursement amounts.) In a multi-year life insurance contract, the time value of money plays a prominent role. In contrast, in a short-term nonlife contract, the random amount of reimbursement takes priority. In both life and nonlife insurances, the frequency of claims is very important. For many life insurance contracts, the insured event (such as death) happens only once. In contrast, for nonlife insurances such as automobile, it is common for individuals (especially young male drivers) to get into more than one accident during a year. So, our models need to reflect this observation; we will introduce different frequency models than you may have seen when studying life insurance. For short-term insurance, the framework of the probabilistic model is straightforward. We think of a one-period model (the period length, e.g., six months, will be specified in the situation). At the beginning of the period, the insured pays the insurer a known premium that is agreed upon by both parties to the contract. At the end of the period, the insurer reimburses the insured for a (possibly multivariate) random loss that we will denote as \\(y\\). This framework will be developed as we proceed but we first focus on integrating this framework with concerns about how the data may arise and what we can accomplish with this framework. 1.1.3 Insurance Processes One way to describe the data arising from operations of a company that sells insurance products is to adopt a granular approach. In this micro oriented view, we can think specifically about what happens to a contract at various stages of its existence. Consider Figure 1.1 that traces a timeline of a typical insurance contract. Throughout the existence of the contract, the company regularly processes events such as premium collection and valuation, described in Section 1.3; these are marked with an x on the timeline. Further, non-regular and unanticipated events also occur. To illustrate, times \\(\\mathrm{t}_2\\) and \\(\\mathrm{t}_4\\) mark the event of an insurance claim (some contracts, such as life insurance, can have only a single claim). Times \\(\\mathrm{t}_3\\) and \\(\\mathrm{t}_5\\) mark the events when a policyholder wishes to alter certain contract features, such as the choice of a deductible or the amount of coverage. Moreover, from a company perspective, one can even think about the contract initiation (arrival, time \\(\\mathrm{t}_1\\)) and contract termination (departure, time \\(\\mathrm{t}_6\\)) as uncertain events. Figure 1.1: Timeline of a Typical Insurance Policy. Arrows mark the occurrences of random events. Each x marks the time of scheduled events that are typically non-random. 1.2 Variable Types In this section, you learn how to: Describe different types of variables typically encountered in insurance practice Classify a variable into the appropriate category Before discussing how to use insurance data to make decisions, it is helpful to first describe common features of data. In general, people, firms, and other entities that we want to understand are described in a dataset by numerical characteristics. As these characteristics vary by entity, they are commonly known as variables. To manage insurance systems, it will be critical to understand the distribution of each variable and how they are associated with one another. We will encounter datasets that have many variables (high dimensional) and so it useful to begin by classifying them into different types. As will be seen, this classification is not strict; there is overlap among the types. Nonetheless, the classification summarized in Table 1.1 and explained in the remainder of this section provide a solid first step in framing a dataset. Table: (#tab:VarTypes) Variable Types \\[{\\small \\begin{matrix} \\begin{array}{l|l} \\hline \\textbf{Variable Type} &amp; \\textbf{Example} \\\\\\hline Qualitative &amp; \\\\ \\text{Binary} &amp; \\text{Sex} \\\\ \\text{Categorical (Unordered, Nominal)} &amp; \\text{Territory (e.g., state/province) in which an insured resides} \\\\ \\text{Ordered Category (Ordinal)} &amp; \\text{Claimant satisfaction (five point scale ranging from 1=dissatisfied} \\\\ &amp; ~~~ \\text{to 5 =satisfied)} \\\\\\hline Quantitative &amp; \\\\ \\text{Continuous} &amp; \\text{Policyholder&#39;s age, weight, income} \\\\ \\text{Discrete} &amp; \\text{Amount of deductible} \\\\ \\text{Count} &amp; \\text{Number of insurance claims} \\\\ \\text{Combinations of} &amp; \\text{Policy losses, mixture of 0&#39;s (for no loss)} \\\\ ~~~ \\text{Discrete and Continuous} &amp; ~~~\\text{and positive claim amount} \\\\ \\text{Interval Variable} &amp; \\text{Driver Age: 16-24 (young), 25-54 (intermediate),} \\\\ &amp; ~~~\\text{55 and over (senior)} \\\\ \\text{Circular Data} &amp; \\text{Time of day measures of customer arrival} \\\\ \\hline Multivariate ~ Variable &amp; \\\\ \\text{High Dimensional Data} &amp; \\text{Characteristics of a firm purchasing worker&#39;s compensation} \\\\ &amp; ~~~\\text{insurance (location of plants, industry, number of employees,} \\\\ &amp;~~~\\text{and so on)} \\\\ \\text{Spatial Data} &amp; \\text{Longitude/latitude of the location an insurance hailstorm claim} \\\\ \\text{Missing Data} &amp; \\text{Policyholder&#39;s age (continuous/interval) and &quot;-99&quot; for} \\\\ &amp;~~~ \\text{&quot;not reported,&quot; that is, missing} \\\\ \\text{Censored and Truncated Data} &amp; \\text{Amount of insurance claims in excess of a deductible} \\\\ \\text{Aggregate Claims} &amp; \\text{Losses recorded for each claim in a motor vehicle policy.} \\\\ \\text{Stochastic Process Realizations} &amp; \\text{The time and amount of each occurrence of an insured loss} \\\\ \\hline \\end{array} \\end{matrix}} \\] 1.2.1 Qualitative Variables Let us start with the simplest type, a binary variable. As suggested by its name, a binary variable is one with only two possible values. Although not necessary, the two values are commonly taken to be a 0 and a 1. Binary variables are typically used to indicate whether or not an entity possesses an attribute. For example, we might code a variable in a dataset to be a 1 if an insured is female and a 0 if male. (An insured is a person who is covered under an insurance agreement.) More generally, a qualitative, or categorical, variable is one for which the measurement denotes membership in a set of groups, or categories. For example, if you were coding in which area of the country in which an insured resides, you might use a 1 for the northern part, 2 for southern, and 3 for everything else. A binary variable is a special type of categorical variable where there are only two categories. This location variable is an example of a nominal variable, one for which the levels have no natural ordering. Any analysis of nominal variables should not depend on the labeling of the categories. For example, instead of using a 1,2,3 for north, south, other, I should arrive at the same set of summary statistics if I used a 2,1,3 coding instead, interchanging north and south. In contrast, an ordinal variable is a type of categorical variable for which an ordering does exist. For example, with a survey to see how satisfied customers are with our claims servicing department, we might use a five point scale that ranges from 1 meaning dissatisfied to a 5 meaning satisfied. Ordinal variables provide a clear ordering of levels of a variable but the amount of separation between levels is unknown. 1.2.2 Quantitative Variables Unlike a qualitative variable, a quantitative variable is one in which numerical level is a realization from some scale so that the distance between any two levels of the scale takes on meaning. A continuous variable is one that can take on any value within a finite interval. For example, it is common to represent a policyholder’s age, weight, or income, as a continuous variable. In contrast, a discrete variable is one that takes on only a finite number of values in any finite interval. For example, when examining a policyholder’s choice of deductibles, it may be that values of 0, 250, 500, and 1000 are the only possible outcomes. Like a ordinal variable, these represent distinct categories that are ordered. Unlike an ordinal variable, the numerical difference between levels takes on economic meaning. A special type of discrete variable is a count variable, one with values on the nonnegative integers \\(0, 1, 2, \\ldots.\\) For example, we will be particularly interested in the number of claims arising from a policy during a given period. This is known as the claim frequency. Given that we will develop ways to analyze discrete variables, do we really need separate methods for dealing with continuous variables? After all, one can argue that few things in the physical world are truly continuous. For example, each currency has a smallest unit that is not subdivided further. (In the US, you cannot pay for anything smaller than one cent.) Nonetheless, models using continuous variables serve as excellent approximations to real-world discrete outcomes, in part due to their simplicity. It will be well worth our time and effort to develop models and analyze continuous and discrete variables differently. Having said that, some variables are inherently a combination of discrete and continuous components. For example, when we analyze the insured loss of a policyholder, we will encounter a discrete outcome at zero, representing no insured loss, and a continuous amount for positive outcomes, representing the amount of the insured loss. Another interesting variation is an interval variable, one that gives a range of possible outcomes. For example, instead of recording a driver’s age in year, it is common for insurers to group ages into three categories, (i) ages 16-24, representing young drives, (ii) ages 25-54, representing intermediate age drivers, and (iii) ages 55 and over, representing senior drivers. Circular data represent an interesting category typically not analyzed by insurers. As an example of circular data, suppose that you monitor calls to your customer service center and would like to know when is the peak time of the day for calls to arrive. In this context, one can think about the time of the day as a variable with realizations on a circle, e.g., imagine an analog picture of a clock. For circular data, the distance between observations at 00:15 and 00:45 are just as close as observations 23:45 and 00:15 (here, we use the convention HH:MM means hours and minutes). 1.2.3 Multivariate Variables Insurance data are typically are multivariate in the sense that we can take many measurements on a single entity. For example, when studying losses associated with a firm’s worker’s compensation plan, we might want to know the location of its manufacturing plants, the industry in which it operates, the number of employees, and so forth. If there are many variables, such data are also known as high dimensional. The usual strategy for analyzing multivariate data is to begin by examining each variable in isolation of the others. This is known as a univariate approach. By considering only one measurement, variables are scalars and, as described, can be thought broadly as either qualitative or quantitative. In contrast, for some variables, it makes little sense to only look a one dimensional aspects. For example, insurers typically organize spatial data by longitude and latitude to analyze the location of weather related insurance claims due hailstorms. Having only a single number, either longitude or latitude, provides little information in understanding geographical location. Another special case of a multivariate variable, less obvious, involves coding for missing data. Historically, some statistical packages used a -99 to report when a variable, such as policyholder’s age, was not available or not reported. This led to many unsuspecting analysts providing strange statistics when summarizing a set of data. When data are missing, it is better to think about the variable as two dimensions, one to indicate whether or not the variable is reported and the second providing the age (if reported). In the same way, insurance data are commonly censored and truncated. To illustrate, with automobile claims may be limited or censored by 500,000, the upper limit that the insurer will pay. The loss amount may be in excess of 500000 but the insurer is only aware of its payout. To record censored claims, a binary variable is used to indicate whether or not the claim is censored (limited) and a second variable is used to indicate the payout. In the same way, claims may be truncated by a deductible. Although there are many types of deductibles, in a common form the insurer pays the amount in excess of a deductible. To illustrate, suppose you have an auto policy with a 250 deductible. If you have a 1000 loss, then the insurer pays 750. If you have a 200 loss, then the insurer pays nothing. In principle, one would like to use a binary variable to indicate whether or not the claim has a deductible and a second variable is used to indicate the payout. As we will see, the tricky thing about deductibles is that for many sampling schemes, the insurer does not observe a claim if it the loss falls below the deductible amount. More on this topic later. Aggregate claims can also be coded as another special type of multivariate variable. In this situation, an insurer has potentially zero, one, two, or more claims, within a policy period. Each claim has its own level (possibly mediated by deductibles and upper limits) and there are an uncertain, or random, number of each claims for each individual. This is a case where the the dimension of the multivariate variable is not known in advance. Perhaps the most complicated type of multivariate variable is a realization of a stochastic process. You will recall that a stochastic process is little more than a collection of random variables. For example, in insurance, we might think about the times that claims arrive to an insurance company in a one year time horizon. This is a high dimensional variable that theoretically is infinite dimensional. Special techniques are required to understand realizations of stochastic processes that will not be addressed here. 1.3 Insurance Company Operations In this section, you learn how to: Describe five major operational areas of insurance companies. Identify the role of data and analytics opportunities within each operational area. Armed with insurance data and a method of organizing the data into variable types, the end goal is to use data to make decisions. Of course, we will need to learn more about methods of analyzing and extrapolating data but that is the purpose of the remaining chapters in the text. To begin, let us think about why we wish to do the analysis. To provide motivation, we take the insurer’s viewpoint (not a person) and introduce ways of bringing money in, paying it out, managing costs, and making sure that we have enough money to meet obligations. Specifically, in many insurance companies, it is customary to aggregate detailed insurance processes into larger operational units; many companies use these functional areas to segregate employee activities and areas of responsibilities. Actuaries and other financial analysts work within these units and use data for the following activities: Initiating Insurance. At this stage, the company makes a decision as to whether or not to take on a risk (the underwriting stage) and assign an appropriate premium (or rate). Insurance analytics has its actuarial roots in ratemaking, where analysts seek to determine the right price for the right risk. Renewing Insurance. Many contracts, particularly in general insurance, have relatively short durations such as 6 months or a year. Although there is an implicit expectation that such contracts will be renewed, the insurer has the opportunity to decline coverage and to adjust the premium. Analytics is also used at this policy renewal stage where the goal is to retain profitable customers. Claims Management. Analytics has long been used in (1) detecting and preventing claims fraud, (2) managing claim costs, including identifying the appropriate support for claims handling expenses, as well as (3) understanding excess layers for reinsurance and retention. Loss Reserving. Analytic tools are used to provide management with an appropriate estimate of future obligations and to quantify the uncertainty of the estimates. Solvency and Capital Allocation. Deciding on the requisite amount of capital and ways of allocating capital to alternative investment activities represent other important analytics activities. Companies must understand how much capital is needed so that they will have sufficient flow of cash available to meet their obligations. This is an important question that concerns not only company managers but also customers, company shareholders, regulatory authorities, as well as the public at large. Related to issues of how much capital is the question of how to allocate capital to differing financial projects, typically to maximize an investor’s return. Although this question can arise at several levels, insurance companies are typically concerned with how to allocate capital to different lines of business within a firm and to different subsidiaries of a parent firm. Although data is a critical component of solvency and capital allocation, other components including an economic framework and financial investments environment are also important. Because of the background needed to address these components, we will not address solvency and capital allocation issues further in this text. Nonetheless, for all operating functions, we emphasize that analytics in the insurance industry is not an exercise that a small group of analysts can do by themselves. It requires an insurer to make significant investments in their information technology, marketing, underwriting, and actuarial functions. As these areas represent the primary end goals of the analysis of data, additional background on each operational unit is provided in the following subsections. 1.3.1 Initiating Insurance Setting the price of an insurance good can be a perplexing problem. In manufacturing, the cost of a good is (relatively) known and provides a benchmark for assessing a market demand price. In other areas of financial services, market prices are available and provide the basis for a market-consistent pricing structure of products. In contrast, for many lines of insurance, the cost of a good is uncertain and market prices are unavailable. Expectations of the random cost is a reasonable place to start for a price, as this is the optimal price for a risk-neutral insurer. Thus, it has been traditional in insurance pricing to begin with the expected cost and to add to this so-called margins to account for the product’s riskiness, expenses incurred in servicing the product, and a profit/surplus allowance for the insurance company. For some lines of business, especially automobile and homeowners insurance, analytics has served to sharpen the market by making the calculation of the good’s expectation more precise. The increasing availability of the internet among consumers has promoted transparency in pricing. Insurers seek to increase their market share by refining their risk classification systems and employing skimming the cream underwriting strategies. Recent surveys (e.g., (Earnix 2013)) indicate that pricing is the most common use of analytics among insurers. Underwriting, the process of classifying risks into homogenous categories and assigning policyholders to these categories, lies at the core of ratemaking. Policyholders within a class have similar risk profiles and so are charged the same insurance price. This is the concept of an actuarially fair premium; it is fair to charge different rates to policyholders only if they can be separated by identifiable risk factors. To illustrate, an early contribution, Two Studies in Automobile Insurance Ratemaking, by (Bailey and LeRoy 1960) provided a catalyst to the acceptance of analytic methods in the insurance industry. This paper addresses the problem of classification ratemaking. It describes an example of automobile insurance that has five use classes cross-classified with four merit rating classes. At that time, the contribution to premiums for use and merit rating classes were determined independently of each other. Thinking about the interacting effects of different classification variables is a more difficult problem. 1.3.2 Renewing Insurance Insurance is a type of financial service and, like many service contracts, insurance coverage is often agreed upon for a limited time period, such as six months or a year, at which time commitments are complete. Particularly for general insurance, the need for coverage continues and so efforts are made to issue a new contract providing similar coverage. Renewal issues can also arise in life insurance, e.g., term (temporary) life insurance, although other contracts, such as life annuities, terminate upon the insured’s death and so issues of renewability are irrelevant. In absence of legal restrictions, at renewal the insurer has the opportunity to: accept or decline to underwrite the risk and determine a new premium, possibly in conjunction with a new classification of the risk. Risk classification and rating at renewal is based on two types of information. First, as at the initial stage, the insurer has available many rating variables upon which decisions can be made. Many variables will not change, e.g., sex, whereas others are likely to have changed, e.g., age, and still others may or may not change, e.g., credit score. Second, unlike the initial stage, at renewal the insurer has available a history of policyholder’s loss experience, and this history can provide insights into the policyholder that are not available from rating variables. Modifying premiums with claims history is known as experience rating, also sometimes referred to as merit rating. Experience rating methods are either applied retrospectively or prospectively. With retrospective methods, a refund of a portion of the premium is provided to the policyholder in the event of favorable (to the insurer) experience. Retrospective premiums are common in life insurance arrangements (where policyholders earned dividends in the U.S. and bonuses in the U.K.). In general insurance, prospective methods are more common, where favorable insured experience is rewarded through a lower renewal premium. Claims history can provide information about a policyholder’s risk appetite. For example, in personal lines it is common to use a variable to indicate whether or not a claim has occurred in the last three years. As another example, in a commercial line such as worker’s compensation, one may look to a policyholder’s average claim over the last three years. Claims history can reveal information that is hidden (to the insurer) about the policyholder. 1.3.3 Claims and Product Management In some of areas of insurance, the process of paying claims for insured events is relatively straightforward. For example, in life insurance, a simple death certificate is all that is needed as the benefit amount is provided in the contract terms. However, in non-life areas such as property and casualty insurance, the process is much more complex. Think about even a relatively simple insured event such as automobile accident. Here, it is often helpful to determine which party is at fault, one needs to assess damage to all of the vehicles and people involved in the incident, both insured and non-insured, the expenses incurred in assessing the damages, and so forth. The process of determining coverage, legal liability, and settling claims is known as claims adjustment. Insurance managers sometimes use the phrase claims leakage to mean dollars lost through claims management inefficiencies. There are many ways in which analytics can help manage the claims process, (Gorman and Swenson 2013). Historically, the most important has been fraud detection. The claim adjusting process involves reducing information asymmetry (the claimant knows exactly what happened; the company knows some of what happened). Mitigating fraud is an important part of claims management process. One can think about the management of claims severity as consisting of the following components: Claims triaging. Just as in the medical world, early identification and appropriate handling of high cost claims (patients, in the medical world), can lead to dramatic company savings. For example, in workers compensation, insurers look to achieve early identification of those claims that run the risk of high medical costs and a long payout period. Early intervention into those cases could give insurers more control over the handling of the claim, the medical treatment, and the overall costs with an earlier return-to-work. Claims processing. The goal is to use analytics to identify situations suitable for small claims handling processes and those for adjuster assignment to complex claims. Adjustment decisions. Once a complex claim has been identified and assigned to an adjuster, analytic driven routines can be established to aid subsequent decision-making processes. Such processes can also be helpful for adjusters in developing case reserves, an important input to the insurer’s loss reserves, Section 1.3.4. In addition to the insured’s reimbursement for insured losses, the insurer also needs to be concerned with another source of revenue outflow, expenses. Loss adjustment expenses are part of an insurer’s cost of managing claims. Analytics can be used to reduce expenses directly related to claims handling (allocated) as well as general staff time for overseeing the claims processes (unallocated). The insurance industry has high operating costs relative to other portions of the financial services sectors. In addition to claims payments, there are many other ways in which insurers use to data to manage their products. We have already discussed the need for analytics in underwriting, that is, risk classification at the initial acquisition stage. Insurers are also interested in which policyholders elect to renew their contract and, as with other products, monitor customer loyalty. Analytics can also be used to manage the portfolio, or collection, of risks that an insurer has acquired. When the risk is initially obtained, the insurer’s risk can be managed by imposing contract parameters that modify contract payouts. In Chapter xx introduces common modifications including coinsurance, deductibles, and policy upper limits. After the contract has been agreed upon with an insured, the insurer may still modify its net obligation by entering into a reinsurance agreement. This type of agreement is with a reinsurer, an insurer of an insurer. It is common for insurance companies to purchase insurance on its portfolio of risks to gain protection from unusual events, just as people and other companies do. 1.3.4 Loss Reserving An important feature that distinguishes insurance from other sectors of the economy is the timing of the exchange of considerations. In manufacturing, payments for goods are typically made at the time of a transaction. In contrast, for insurance, money received from a customer occurs in advance of benefits or services; these are rendered at a later date. This leads to the need to hold a reservoir of wealth to meet future obligations in respect to obligations made. The size of this reservoir of wealth, and the importance of ensuring its adequacy in regard to liabilities already assumed, is a major concern for the insurance industry. Setting aside money for unpaid claims is known as loss reserving; in some jurisdictions, reserves are also known as technical provisions. We saw in Figure 1.1 how future obligations arise naturally at a specific (valuation) date; a company must estimate these outstanding liabilities when determining its financial strength. Accurately determining loss reserves is important to insurers for many reasons. Loss reserves represent a loan that the insurer owes its customers. Under-reserving may result in a failure to meet claim liabilities. Conversely, an insurer with excessive reserves may present a weaker financial position than it truly has and lose market share. Reserves provide an estimate for the unpaid cost of insurance that can be used for pricing contracts. Loss reserving is required by laws and regulations. The public has a strong interest in the financial strength of insurers. In addition to the insurance company management and regulators, other stakeholders such as investors and customers make decisions that depend on company loss reserves. Loss reserving is a topic where there are substantive differences between life and general (also known as property and casualty, or non-life), insurance. In life insurance, the severity (amount of loss) is often not a source of concern as payouts are specified in the contract. The frequency, driven by mortality of the insured, is a concern. However, because of the length of time for settlement of life insurance contracts, the time value of money uncertainty as measured from issue to date of death can dominate frequency concerns. For example, for an insured who purchases a life contract at age 20, it would not be unusual for the contract to still be open in 60 years time. See, for example, (Bowers et al. 1986) or (Dickson, Hardy, and Waters 2013) for introductions to reserving for life insurance. 1.4 Case Study: Wisconsin Property Fund In this section,for a real case study such as the Wisconsin Property Fund, you learn how to: Describe how data generating events can produce data of interest to insurance analysts. Identify the type of each variable. Produce relevant summary statistics for each variable. Describe how these summary statistcs can be used in each of the major operational areas of an insurance company. Let us illustrate the kind of data under consideration and the goals that we wish to achieve by examining the Local Government Property Insurance Fund (LGPIF), an insurance pool administered by the Wisconsin Office of the Insurance Commissioner. The LGPIF was established to provide property insurance for local government entities that include counties, cities, towns, villages, school districts, and library boards. The fund insures local government property such as government buildings, schools, libraries, and motor vehicles. The fund covers all property losses except those resulting from flood, earthquake, wear and tear, extremes in temperature, mold, war, nuclear reactions, and embezzlement or theft by an employee. The property fund covers over a thousand local government entities who pay approximately $25 million in premiums each year and receive insurance coverage of about $75 billion. State government buildings are not covered; the LGPIF is for local government entities that have separate budgetary responsibilities and who need insurance to moderate the budget effects of uncertain insurable events. Coverage for local government property has been made available by the State of Wisconsin since 1911. 1.4.1 Fund Claims Variables At a fundamental level, insurance companies accept premiums in exchange for promises to indemnify a policyholder upon the uncertain occurrence of an insured event. This indemnification is known as a claim. A positive amount, also known as the severity of the claim, is a key financial expenditure for an insurer. So, knowing only the claim amount summarizes the reimbursement to the policyholder. Ignoring expenses, an insurer that examines only amounts paid would be indifferent to two claims of 100 when compared to one claim of 200, even though the number of claims differ. Nonetheless, it is common for insurers to study how often claims arise, known as the frequency of claims. The frequency is important for expenses, but it also influences contractual parameters (such as deductibles and policy limits) that are written on a per occurrence basis, is routinely monitored by insurance regulators, and is often a key driven in the overall indemnification obligation of the insurer. We shall consider the two claims variables, the severity and frequency, as the two main outcome variables that we wish to understand, model, and manage. To illustrate, in 2010 there were 1,110 policyholders in the property fund. Table 1.2 shows the distribution of the 1,377 claims. Almost two-thirds (0.637) of the policyholders did not have any claims and an additional 18.8% only had one claim. The remaining 17.5% (=1 - 0.637 - 0.188) had more than one claim; the policyholder with the highest number recorded 239 claims. The average number of claims for this sample was 1.24 (=1377/1110). Table 1.2: 2010 Claims Frequency Distribution Type Number 0 1 2 3 4 5 6 7 8 9 or more Sum Count 707 209 86 40 18 12 9 4 6 19 1,110 Proportion 0.637 0.188 0.077 0.036 0.016 0.011 0.008 0.004 0.005 0.017 1.000 R Code for Frequency Table Insample &lt;- read.csv(&quot;Insample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) Insample2010 &lt;- subset(Insample, Year==2010) table(Insample2010$Freq) For the severity distribution, one common approach is to examine the distribution of the sample of 1,377 claims. However, another common approach is to examine the distribution of the average claims of those policyholders with claims. In our 2010 sample, there were 403 (=1110-707) such policyholders. For 209 of these policyholders with one claim, the average claim equals the only claim they experienced. For the policyholder with highest frequency, the average claim is an average over 239 separately reported claim events. The total severity divided by the number of claims is also known as the pure premium or loss cost. Table 1.3 summarizes the sample distribution of average severities from the 403 policyholders; it shows that the average claim amount was 56,330 (all amounts are in US Dollars). However, the average gives only a limited look at the distribution. More information can be gleaned from the summary statistics which show a very large claim in the amount of 12,920,000. Figure 1.2 provides further information about the distribution of sample claims, showing a distribution that is dominated by this single large claim so that the histogram is not very helpful. Even when removing the large claim, you will find a distribution that is skewed to the right. A generally accepted technique is to work with claims in logarithmic units especially for graphical purposes; the corresponding figure in the right-hand panel is much easier to interpret. Table 1.3: 2010 Average Severity Distribution Minimum First Quartile Median Mean Third Quartile Maximum 167 2,226 4,951 56,330 11,900 12,920,000 Figure 1.2: Distribution of Positive Average Severities R Code for Severity Distribution Table and Figures Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) Insample2010 &lt;- subset(Insample, Year==2010) InsamplePos2010 &lt;- subset(Insample2010, yAvg&gt;0) # Table summary(InsamplePos2010$yAvg) length(InsamplePos2010$yAvg) # Figures par(mfrow=c(1, 2)) hist(InsamplePos2010$yAvg, main=&quot;&quot;, xlab=&quot;Average Claims&quot;) hist(log(InsamplePos2010$yAvg), main=&quot;&quot;, xlab=&quot;Logarithmic Average Claims&quot;) 1.4.2 Fund Rating Variables Developing models to represent and manage the two outcome variables, frequency and severity, is the focus of the early chapters of this text. However, when actuaries and other financial analysts use those models, they do so in the context of externally available variables. In general statistical terminology, one might call these explanatory or predictor variables; there are many other names in statistics, economics, psychology, and other disciplines. Because of our insurance focus, we call them rating variables as they will be useful in setting insurance rates and premiums. We earlier considered a sample of 1,110 observations which may seem like a lot. However, as we will seen in our forthcoming applications, because of the preponderance of zeros and the skewed nature of claims, actuaries typically yearn for more data. One common approach that we adopt here is to examine outcomes from multiple years, thus increasing the sample size. We will discuss the strengths and limitations of this strategy later but, at this juncture, just want to show the reader how it works. Specifically, Table 1.4 shows that we now consider policies over five years of data, years 2006, …, 2010, inclusive. The data begins in 2006 because there was a shift in claim coding in 2005 so that comparisons with earlier years are not helpful. To mitigate the effect of open claims, we consider policy years prior to 2011. An open claim means that all of the obligations are not known at the time of the analysis; for some claims, such an injury to a person in an auto accident or in the workplace, it can take years before costs are fully known. Table 1.4 shows that the average claim varies over time, especially with the high 2010 value due to a single large claim. The total number of policyholders is steadily declining and, conversely, the coverage is steadily increasing. The coverage variable is the amount of coverage of the property and contents. Roughly, you can think of it as the maximum possible payout of the insurer. For our immediate purposes, it is our first rating variable. Other things being equal, we would expect that policyholders with larger coverage will have larger claims. We will make this vague idea much more precise as we proceed. Table 1.4: Building and Contents Claims Summary Year Average Frequency Average Severity Average Coverage Number of Policyholders 2006 0.951 9,695 32,498,186 1,154 2007 1.167 6,544 35,275,949 1,138 2008 0.974 5,311 37,267,485 1,125 2009 1.219 4,572 40,355,382 1,112 2010 1.241 20,452 41,242,070 1,110 R Code for Building and Contents Claims Summary Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) library(doBy) T1A &lt;- summaryBy(Freq ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) T1B &lt;- summaryBy(yAvg ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) T1C &lt;- summaryBy(BCcov ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) Table1In &lt;- cbind(T1A[1],T1A[2],T1B[2],T1C[2],T1A[3]) names(Table1In) &lt;- c(&quot;Year&quot;, &quot;Average Frequency&quot;,&quot;Average Severity&quot;, &quot;Average&quot;,&quot;Number of Policyholders&quot;) Table1In For a different look at this five-year sample, Table 1.5 summarizes the distribution of our two outcomes, frequency and claims amount. In each case, the average exceeds the median, suggesting that the two distributions are right-skewed. In addition, the table summarizes our continuous rating variables, coverage and deductible amount. The table also suggests that these variables also have right-skewed distributions. Table 1.5: Summary of Claim Frequency and Severity, Deductibles, and Coverages Minimum Median Average Maximum Claim Frequency 0 0 1.109 263 Claim Severity 0 0 9,292 12,922,218 Deductible 500 1,000 3,365 100,000 Coverage (000’s) 8.937 11,354 37,281 2,444,797 R Code for Summary of Claim Frequency and Severity, Deductibles, and Coverages Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) t1&lt;- summaryBy(Insample$Freq ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x),m=mean(x),mb=max(x)) } ) names(t1) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t2 &lt;- summaryBy(Insample$yAvg ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t2) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t3 &lt;- summaryBy(Deduct ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t3) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t4 &lt;- summaryBy(BCcov/1000 ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t4) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) Table2 &lt;- rbind(t1,t2,t3,t4) Table2a &lt;- round(Table2,3) Rowlable &lt;- rbind(&quot;Claim Frequency&quot;,&quot;Claim Severity&quot;,&quot;Deductible&quot;,&quot;Coverage (000&#39;s)&quot;) Table2aa &lt;- cbind(Rowlable,as.matrix(Table2a)) Table2aa Table 1.6 describes the rating variables considered in this chapter. To handle the skewness, we henceforth focus on logarithmic transformations of coverage and deductibles. To get a sense of the relationship between the non-continuous rating variables and claims, Table 1.7 relates the claims outcomes to these categorical variables. Table 1.7 suggests substantial variation in the claim frequency and average severity of the claims by entity type. It also demonstrates higher frequency and severity for the \\({\\tt Fire5}\\) variable and the reverse for the \\({\\tt NoClaimCredit}\\) variable. The relationship for the \\({\\tt Fire5}\\) variable is counter-intuitive in that one would expect lower claim amounts for those policyholders in areas with better public protection (when the protection code is five or less). Naturally, there are other variables that influence this relationship. We will see that these background variables are accounted for in the subsequent multivariate regression analysis, which yields an intuitive, appealing (negative) sign for the \\({\\tt Fire5}\\) variable. Table: (#tab:VarDescr) Description of Rating Variables \\[{\\small \\begin{matrix} \\begin{array}{ l | l} \\hline Variable &amp; Description \\\\ \\hline \\text{EntityType} &amp; \\text{Categorical variable that is one of six types: (Village, City,} \\\\ &amp; ~~~~ \\text{County, Misc, School, or Town)} \\\\ \\text{LnCoverage} &amp; \\text{Total building and content coverage, in logarithmic millions of dollars}\\\\ \\text{LnDeduct} &amp; \\text{Deductible, in logarithmic dollars} \\\\ \\text{AlarmCredit} &amp; \\text{Categorical variable that is one of four types: (0, 5, 10, or 15)} \\\\ &amp; ~~~~ \\text{for automatic smoke alarms in main rooms} \\\\ \\text{NoClaimCredit} &amp; \\text{Binary variable to indicate no claims in the past two years} \\\\ \\text{Fire5 } &amp; \\text{Binary variable to indicate the fire class is below 5} \\\\ &amp; ~~~~ \\text{(The range of fire class is 0 to 10} \\\\ \\hline \\end{array} \\end{matrix}}\\] Table 1.7: Claims Summary by Entity Type, Fire Class, and No Claim Credit Variable Number of Policies Claim Frequency Average Severity EntityType Village 1,341 0.452 10,645 City 793 1.941 16,924 County 328 4.899 15,453 Misc 609 0.186 43,036 School 1,597 1.434 64,346 Town 971 0.103 19,831 Fire5=0 2,508 0.502 13,935 Fire5=1 3,131 1.596 41,421 NoClaimCredit=0 3,786 1.501 31,365 NoClaimCredit=1 1,853 0.310 30,499 Total 5,639 1.109 31,206 R Code for Claims Summary by Entity Type, Fire Class, and No Claim Credit ByVarSumm&lt;-function(datasub){ tempA &lt;- summaryBy(Freq ~ 1 , data = datasub, FUN = function(x) { c(m = mean(x), num=length(x)) } ) datasub1 &lt;- subset(datasub, yAvg&gt;0) tempB &lt;- summaryBy(yAvg ~ 1, data = datasub1,FUN = function(x) { c(m = mean(x)) } ) tempC &lt;- merge(tempA,tempB,all.x=T)[c(2,1,3)] tempC1 &lt;- as.matrix(tempC) return(tempC1) } datasub &lt;- subset(Insample, TypeVillage == 1); t1 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCity == 1); t2 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCounty == 1); t3 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeMisc == 1); t4 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeSchool == 1); t5 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeTown == 1); t6 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Fire5 == 0); t7 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Fire5 == 1); t8 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Insample$NoClaimCredit == 0); t9 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Insample$NoClaimCredit == 1); t10 &lt;- ByVarSumm(datasub) t11 &lt;- ByVarSumm(Insample) Tablea &lt;- rbind(t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11) Tableaa &lt;- round(Tablea,3) Rowlable &lt;- rbind(&quot;Village&quot;,&quot;City&quot;,&quot;County&quot;,&quot;Misc&quot;,&quot;School&quot;, &quot;Town&quot;,&quot;Fire5--No&quot;,&quot;Fire5--Yes&quot;,&quot;NoClaimCredit--No&quot;, &quot;NoClaimCredit--Yes&quot;,&quot;Total&quot;) Table4 &lt;- cbind(Rowlable,as.matrix(Tableaa)) Table4 Table 1.8 shows the claims experience by alarm credit. It underscores the difficulty of examining variables individually. For example, when looking at the experience for all entities, we see that policyholders with no alarm credit have on average lower frequency and severity than policyholders with the highest (15%, with 24/7 monitoring by a fire station or security company) alarm credit. In particular, when we look at the entity type School, the frequency is 0.422 and the severity 25,257 for no alarm credit, whereas for the highest alarm level it is 2.008 and 85,140. This may simply imply that entities with more claims are the ones that are likely to have an alarm system. Summary tables do not examine multivariate effects; for example, Table 1.7 ignores the effect of size (as we measure through coverage amounts) that affect claims. Table 1.8: Claims Summary by Entity Type and Alarm Credit Category Entity Type Claim Frequency Avg. Severity Num. Policies Claim Frequency Avg. Severity Num. Policies Village 0.326 11,078 829 0.278 8,086 54 City 0.893 7,576 244 2.077 4,150 13 County 2.140 16,013 50 - - 1 Misc 0.117 15,122 386 0.278 13,064 18 School 0.422 25,523 294 0.410 14,575 122 Town 0.083 25,257 808 0.194 3,937 31 Total 0.318 15,118 2,611 0.431 10,762 239 Claims Summary by Entity Type and Alarm Credit Category Entity Type Claim Frequency Avg. Severity Num. Policies Claim Frequency Avg. Severity Num. Policies Village 0.500 8,792 50 0.725 10,544 408 City 1.258 8,625 31 2.485 20,470 505 County 2.125 11,688 8 5.513 15,476 269 Misc 0.077 3,923 26 0.341 87,021 179 School 0.488 11,597 168 2.008 85,140 1,013 Town 0.091 2,338 44 0.261 9,490 88 Total 0.517 10,194 327 2.093 41,458 2,462 R Code for Claims Summary by Entity Type and Alarm Credit Category #Claims Summary by Entity Type and Alarm Credit ByVarSumm&lt;-function(datasub){ tempA &lt;- summaryBy(Freq ~ AC00 , data = datasub, FUN = function(x) { c(m = mean(x), num=length(x)) } ) datasub1 &lt;- subset(datasub, yAvg&gt;0) if(nrow(datasub1)==0) { n&lt;-nrow(datasub) return(c(0,0,n)) } else { tempB &lt;- summaryBy(yAvg ~ AC00, data = datasub1, FUN = function(x) { c(m = mean(x)) } ) tempC &lt;- merge(tempA,tempB,all.x=T)[c(2,4,3)] tempC1 &lt;- as.matrix(tempC) return(tempC1) } } AlarmC &lt;- 1*(Insample$AC00==1) + 2*(Insample$AC05==1)+ 3*(Insample$AC10==1)+ 4*(Insample$AC15==1) ByVarCredit&lt;-function(ACnum){ datasub &lt;- subset(Insample, TypeVillage == 1 &amp; AlarmC == ACnum); t1 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCity == 1 &amp; AlarmC == ACnum); t2 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCounty == 1 &amp; AlarmC == ACnum); t3 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeMisc == 1 &amp; AlarmC == ACnum); t4 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeSchool == 1 &amp; AlarmC == ACnum); t5 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeTown == 1 &amp; AlarmC ==ACnum); t6 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, AlarmC == ACnum); t7 &lt;- ByVarSumm(datasub) Tablea &lt;- rbind(t1,t2,t3,t4,t5,t6,t7) Tableaa &lt;- round(Tablea,3) Rowlable &lt;- rbind(&quot;Village&quot;,&quot;City&quot;,&quot;County&quot;,&quot;Misc&quot;,&quot;School&quot;, &quot;Town&quot;,&quot;Total&quot;) Table4 &lt;- cbind(Rowlable,as.matrix(Tableaa)) } Table4a &lt;- ByVarCredit(1) #Claims Summary by Entity Type and Alarm Credit==00 Table4b &lt;- ByVarCredit(2) #Claims Summary by Entity Type and Alarm Credit==05 Table4c &lt;- ByVarCredit(3) #Claims Summary by Entity Type and Alarm Credit==10 Table4d &lt;- ByVarCredit(4) #Claims Summary by Entity Type and Alarm Credit==15 1.4.3 Fund Operations We have now seen the Fund’s two outcome variables, a count variable for the number of claims and a continuous variable for the claims amount. We have also introduced a continuous rating variable, coverage, discrete quantitative variable, (logarithmic) deductibles, two binary rating variable, no claims credit and fire class, as well as two categorical rating variables, entity type and alarm credit. Subsequent chapters will explain how to analyze and model the distribution of these variables and their relationships. Before getting into these technical details, let us first think about where we want to go. General insurance company functional areas are described in Section 1.3; let us now think about how these areas might apply in the context of the property fund. Initiating Insurance Because this is a government sponsored fund, we do not have to worry about selecting good or avoiding poor risks; the fund is not allowed to deny a coverage application from a qualified local government entity. If we do not have to underwrite, what about how much to charge? We might look at the most recent experience in 2010, where the total fund claims were approximately 28.16 million USD (\\(=1377 \\text{ claims} \\times 20452 \\text{ average severity}\\)). Dividing that among 1,110 policyholders, that suggests a rate of 24,370 ( \\(\\approx\\) 28,160,000/1110). However, 2010 was a bad year; using the same method, our premium would be much lower based on 2009 data. This swing in premiums would defeat the primary purpose of the fund, to allow for a steady charge that local property managers could utilize in their budgets. Having a single price for all policyholders is nice but hardly seems fair. For example, Table 1.7 suggests that Schools have much higher claims than other entities and so should pay more. However, simply doing the calculation on an entity by entity basis is not right either. For example, we saw in Table 1.8 that had we used this strategy, entities with a 15% alarm credit (for good behavior, having top alarm systems) would actually wind up paying more. So, we have the data for thinking about the appropriate rates to charge but will need to dig deeper into the analysis. We will explore this topic further in Chapter 6 on premium calculation fundamentals. Selecting appropriate risks is introduced in Chapter 7 on risk classification. Renewing Insurance Although property insurance is typically a one-year contract, Table 1.4 suggests that policyholders tend to renew; this is typical of general insurance. For renewing policyholders, in addition to their rating variables we have their claims history and this claims history can be a good predictor of future claims. For example, Table 1.4 shows that policyholders without a claim in the last two years had much lower claim frequencies than those with at least one accident (0.310 compared to 1.501); a lower predicted frequency typically results in a lower premium. This is why it is common for insurers to use variables such as \\({\\tt NoClaimCredit}\\) in their rating. We will explore this topic further in Chapter 8 on experience rating. Claims Management Of course, the main story line of 2010 experience was the large claim of over 12 million USD, nearly half the claims for that year. Are there ways that this could have been prevented or mitigated? Are their ways for the fund to purchase protection against such large unusual events? Another unusual feature of the 2010 experience noted earlier was the very large frequency of claims (239) for one policyholder. Given that there were only 1,377 claims that year, this means that a single policyholder had 17.4 % of the claims. This also suggestions opportunities for managing claims, the subject of Chapter 9. Loss Reserving In our case study, we look only at the one year outcomes of closed claims (the opposite of open). However, like many lines of insurance, obligations from insured events to buildings such as fire, hail, and the like, are not known immediately and may develop over time. Other lines of business, including those were there are injuries to people, take much longer to develop. Chapter 10 introduces this concern and loss reserving, the discipline of determining how much the insurance company should retain to meet its obligations. 1.5 Further Resources and Contributors This book introduces loss data analytic tools that are most relevant to actuaries and other financial risk analysts. Here are a few reference cited in the chapter. Bailey, Robert A. and J. Simon LeRoy (1960). “Two studies in automobile ratemaking,” Proceedings of the Casualty Actuarial Society Casualty Actuarial Society, Vol. XLVII. Bowers, Newton L., Hans U. Gerber, James C. Hickman, Donald A. Jones, and Cecil J. Nesbitt (1986). Actuarial Mathematics. Society of Actuaries Itasca, Ill. Dickson, David C. M., Mary Hardy, and Howard R. Waters (2013). Actuarial Mathematics for Life Contingent Risks. Cambridge University Press. Earnix (2013). “2013 Insurance Predictive Modeling Survey,” Earnix and Insurance Services Office, Inc. [Retrieved on May 10, 2016]. Gorman, Mark and Stephen Swenson (2013). “Building believers: How to expand the use of predictive analytics in claims,” SAS, [Retrieved on May 10, 2016]. Insurance Information Institute (2015). “International Insurance Fact Book. [Retrieved on May 10, 2016]. Taylor, Gregory C. (2014). “Claims triangles/Loss reserves,” in Edward W. Frees, Glenn Meyers, and Richard A. Derrig eds. Predictive Modeling Applications in Actuarial Science, Cambridge. Cambridge University Press. Contributor Edward W. (Jed) Frees, University of Wisconsin-Madison, is the principal author of the initital version of this chapter. Email: jfrees@bus.wisc.edu for chapter comments and suggested improvements. Bibliography "],
["frequency-distributions.html", "Chapter 2 Frequency Distributions 2.1 How Frequency Augments Severity Information 2.2 Basic Frequency Distributions 2.3 The (\\(a, b\\), 0) Class 2.4 Estimating Frequency Distributions 2.5 Other Frequency Distributions 2.6 Mixture Distributions 2.7 Goodness of Fit 2.8 Exercises 2.9 Technical Supplement: Iterated Expectations", " Chapter 2 Frequency Distributions These are overheads from a course that provides some structure for this chapter. 2.1 How Frequency Augments Severity Information 2.1.0.1 Basic Terminology Claim - indemnification upon the occurrence of an insured event Loss - some authors use claim and loss interchangeably, others think of loss as the amount suffered by the insured whereas claim is the amount paid by the insurer Frequency - how often an insured event occurs, typically within a policy contract Count - In this chapter, we focus on count random variables that represent the number of claims, that is, how frequently an event occurs Severity - Amount, or size, of each payment for an insured event 2.1.0.2 The Importance of Frequency Insurers pay claims in monetary units, e.g., US dollars. So, why should they care about how frequently claims occur? Many ways to use claims modeling – easiest to motivate in terms of pricing for personal lines insurance Recall from Chapter 1 that setting the price of an insurance good can be a perplexing problem. In manufacturing, the cost of a good is (relatively) known Other financial service areas, market prices are available Insurance tradition: Start with an expected cost. Add “margins” to account for the product’s riskiness, expenses incurred in servicing the product, and a profit/surplus allowance for the insurance company. Think of the expected cost as the expected number of claims times the expected amount per claims, that is, expected frequency times severity. Claim amounts, or severities, will turn out to be relatively homogeneous for many lines of business and so we begin our investigations with frequency modeling. 2.1.0.3 Other Ways that Frequency Augments Severity Information Contractual - For example, deductibles and policy limits are often in terms of each occurrence of an insured event Behaviorial - Explanatory (rating) variables can have different effects on models of how often an event occurs in contrast to the size of the event. In healthcare, the decision to utilize healthcare by individuals is related primarily to personal characteristics whereas the cost per user may be more related to characteristics of the healthcare provider (such as the physician). Databases. Many insurers keep separate data files that suggest developing separate frequency and severity models. This recording process makes it natural for insurers to model the frequency and severity as separate processes. Policyholder file that is established when a policy is written. This file records much underwriting information about the insured(s), such as age, gender and prior claims experience, policy information such as coverage, deductibles and limitations, as well as the insurance claims event. Claims file, records details of the claim against the insurer, including the amount. (There may also be a “payments” file that records the timing of the payments although we shall not deal with that here.) Regulatory and Administrative Regulators routinely require the reporting of claims numbers as well as amounts. This may be due to the fact that there can be alternative definitions of an “amount,” e.g., paid versus incurred, and there is less potential error when reporting claim numbers. 2.2 Basic Frequency Distributions 2.2.1 Foundations Claim count \\(N\\) has support on the non-negative integers \\(k=0,1,2, \\ldots\\). The probability mass function is denoted as \\(\\Pr(N = k) = p_k\\) We can summarize the distribution through its moments The mean, or first moment, is \\[\\mathrm{E~} N = \\mu_1 = \\mu = \\sum^{\\infty}_{k=0} k p_k .\\] More generally, the \\(r\\)th moment is \\[\\mathrm{E~} N^r = \\mu_r^{\\prime} = \\sum^{\\infty}_{k=0} k^r p_k .\\] The variance is \\[\\mathrm{Var~} N = \\mathrm{E~} (N-\\mu)^2 = \\mathrm{E~} N^2 - \\mu^2\\] Also recall the moment generating function \\[M_N(t) = \\mathrm{E~}e^{tN} = \\sum^{\\infty}_{k=0} e^{tk} p_k .\\] 2.2.2 Probability Generating Function The probability generating function is \\[\\begin{aligned} \\mathrm{P}(z) &amp;= \\mathrm{E~}z^N = \\mathrm{E~}\\exp{(N \\ln z)} = M_N(\\ln{z})\\\\ &amp;= \\sum^{\\infty}_{k=0} z^k p_k .\\end{aligned}\\] By taking the \\(m\\)th derivative, we see that \\[\\begin{aligned} \\left. P^{(m)}(z)\\right|_{z=0} &amp;= \\frac{\\partial^m }{\\partial z^m} P(z)|_{z=0} = p_m m!\\end{aligned}\\] the pgf “generates” the probabilities. Further, the pgf can be used to generate moments \\[\\begin{aligned} P^{(1)}(1) &amp;= \\sum k p_k = \\mathrm{E~}N .\\end{aligned}\\] and \\[P^{(2)}(1) = \\mathrm{E~}N(N-1).\\] 2.2.3 Important Frequency Distributions The three important (in insurance) frequency distributions are: Poisson Negative binomial Binomial They are important because: They fit well many insurance data sets of interest They provide the basis for more complex distributions that even better approximate real situations of interest to us 2.2.3.1 Poisson Distribution This distribution has parameter \\(\\lambda\\), probability mass function \\[p_k = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\] and pgf \\[\\begin{aligned} P(z) &amp;= M_N (\\ln z) = \\exp(\\lambda(z-1))\\end{aligned}\\] The expectation is \\(\\mathrm{E~}N = \\lambda\\) which is the same as the variance, \\(\\mathrm{Var~}N = \\lambda\\). 2.2.3.2 Negative Binomial Distribution This distribution has parameters \\((r, \\beta)\\), probability mass function (pmf) \\[p_k = {k+r-1\\choose k} \\left(\\frac{1}{1+\\beta}\\right)^r \\left(\\frac{\\beta}{1+\\beta}\\right)^k\\] and probability generating function (pgf) \\[\\begin{aligned} P(z) &amp;= (1-\\beta(z-1))^{-r} \\end{aligned}\\] The expectation is \\(\\mathrm{E~}N = r\\beta\\) and the variance is \\(\\mathrm{Var~}N = r\\beta(1+\\beta)\\). When \\(\\beta&gt;0\\), we have \\(\\mathrm{Var~}N &gt;\\mathrm{E~}N\\). This distribution is said to be overdispersed (relative to the Poisson). 2.2.3.3 Binomial Distribution This distribution has parameters \\((m,q)\\), probability mass function \\[p_k = {m\\choose k} q^k (1-q)^{m-k}\\] and pgf \\[\\begin{aligned} P(z) &amp;= (1+q(z-1))^m\\end{aligned}\\] The mean is \\(\\mathrm{E~}N = mq\\) and the variance is \\(\\mathrm{Var~}N = mq(1-q)\\). 2.3 The (\\(a, b\\), 0) Class Recall the notation \\(p_k= \\Pr(N = k)\\). Definition. A count distribution is a member of the (\\(a, b\\), 0) class if the probabilities \\(p_k\\) satisfy \\[\\frac{p_k}{p_{k-1}}=a+\\frac{b}{k},\\] for constants \\(a,b\\) and for $k=1,2,3, $. There are only three distributions that are members of the (\\(a,b\\),0) class. They are the Poisson (\\(a=0\\)), binomial(\\(a&lt;0\\)), and negative binomial (\\(a&gt;0\\)). The recursive expression provides a computationally efficient way to generate probabilities. 2.3.0.1 The (\\(a, b\\), 0) Class - Special Cases Example: Poisson Distribution. Recall the pmf \\(p_k =\\frac{\\lambda^k}{k!}e^{-\\lambda}\\). Examining the ratio, \\[\\frac{p_k}{p_{k-1}} = \\frac{\\lambda^k/k!}{\\lambda^{k-1}/(k-1)!}\\frac{e^{-\\lambda}}{e^{-\\lambda}}= \\frac{\\lambda}{k}\\] Thus, the Poisson is a member of the (\\(a, b\\), 0) class with \\(a = 0\\), \\(b = \\lambda\\), and initial starting value \\(p_0 = e^{-\\lambda}\\). Other special cases (Please check) Example: Binomial Distribution. Use a similar technique to check that the binomial distribution is a member of the (\\(a, b\\), 0) class with \\(a = \\frac{-q}{1-q},\\) \\(b = \\frac{(m+1)q}{1-q},\\) and initial starting value \\(p_0 = (1-q)^m\\). Another special case of the (\\(a, b\\), 0) Class (Please check) Example: Negative Binomial Distribution. Check that the negative binomial distribution is a member of the (\\(a, b\\), 0) class with \\(a = \\frac{\\beta}{1+\\beta},\\) \\(b = \\frac{(r-1)\\beta}{1+\\beta},\\) and initial starting value \\(p_0 = (1+\\beta)^{-r}\\). Exercise. A discrete probability distribution has the following properties \\[\\begin{aligned} p_k&amp;=c\\left( 1+\\frac{2}{k}\\right) p_{k-1} \\:\\:\\: k=1,2,3,\\\\ p_1&amp;= \\frac{9}{256}\\end{aligned}\\] Determine the expected value of this discrete random variable (Ans: 9) 2.3.1 The (\\(a, b\\), 0) Class - Example Exercise. A discrete probability distribution has the following properties \\[\\begin{aligned} \\Pr(N=k) = \\left( \\frac{3k+9}{8k}\\right) \\Pr(N=k-1), ~~~k=1,2,3,\\ldots\\end{aligned}\\] Determine the value of \\(\\Pr(N=3)\\). (Ans: 0.1609) 2.4 Estimating Frequency Distributions 2.4.0.1 Parameter estimation The customary method of estimation is maximum likelihood. To provide intuition, we outline the ideas in the context of Bernoulli distribution. This is a special case of the binomial distribution with \\(m=1\\) For count distributions, either there is a claim \\(N=1\\) or not \\(N=0\\). The probability mass function is \\[p_k = \\Pr (N=k) = \\left\\{ \\begin{array}{ll} 1-q &amp; \\mathrm{if}\\ k=0 \\\\ q&amp; \\mathrm{if}\\ k=1 \\end{array} \\right. .\\] The Statistical Inference Problem Now suppose that we have a collection of independent random variables. The \\(i\\)th variable is denoted as \\(N_i\\). Further assume they have the same Bernoulli distribution with parameter \\(q\\). In statistical inference, we assume that we observe a sample of such random variables. The observed value of the \\(i\\)th random variable is \\(n_i\\). Assuming that the Bernoulli distribution is correct, we wish to say something about the probability parameter \\(q\\). 2.4.0.2 Bernoulli Likelihoods Definition. The likelihood is the observed value of the mass function. For a single observation, the likelihood is \\[\\left\\{ \\begin{array}{ll} 1-q &amp; \\mathrm{if}\\ n_i=0 \\\\ q &amp; \\mathrm{if}\\ n_i=1 \\end{array} \\right. .\\] The objective of maximum likelihood estimation (MLE) is to find the parameter values that produce the largest likelihood. Finding the maximum of the logarithmic function yields the same solution as finding the maximum of the corresponding function. Because it is generally computationally simpler, we consider the logarithmic (log-) likelihood, written as \\[\\left\\{ \\begin{array}{ll} \\ln \\left( 1-q\\right) &amp; \\mathrm{if}\\ n_i=0 \\\\ \\ln q &amp; \\mathrm{if}\\ n_i=1 \\end{array}\\right. .\\] 2.4.0.3 Bernoulli MLE More compactly, the log-likelihood of a single observation is \\[n_i \\ln q + (1-n_i)\\ln ( 1-q ) ,\\] Assuming independence, the log-likelihood of the data set is \\[L_{Bern}(q)=\\sum_i \\left\\{ n_i \\ln q + (1-n_i)\\ln ( 1-q ) \\right\\}\\] The (log) likelihood is viewed as a function of the parameters, with the data held fixed. In contrast, the joint probability mass function is viewed as a function of the realized data, with the parameters held fixed. The method of maximum likelihood means finding the values of \\(q\\) that maximize the log-likelihood. We began with the Bernoulli distribution in part because the log-likelihood is easy to maximize. Take a derivative of \\(L_{Bern}(q)\\) to get \\[\\frac{\\partial}{\\partial q} L_{Bern}(q)=\\sum_i \\left\\{ n_i \\frac{1}{q} - (1-n_i)\\frac{1}{1-q} \\right\\}\\] and solving the equation \\(\\frac{\\partial}{\\partial q} L_{Bern}(q) =0\\) yields \\[\\hat{q} = \\frac{\\sum_i n_i}{\\mathrm{sample ~size}}\\] or, in words, the \\(MLE\\) \\(\\hat{q}\\) is the fraction of one’s in the sample. Just to be complete, you should check, by taking derivatives, that when we solve \\(\\frac{\\partial}{\\partial q} L_{Bern}(q) =0\\) we are maximizing the function \\(L_{Bern}(q)\\), not minimizing it. 2.4.0.4 Frequency Distributions MLE We can readily extend this procedure to all frequency distributions For notation, suppose that \\(\\theta\\) (“theta”) is a parameter that describes a given frequency distribution \\(\\Pr(N=k; \\theta) = p_k(\\theta)\\) In later developments we will let \\(\\theta\\) be a vector but for the moment assume it to be a scalar. The log-likelihood of a a single observation is \\[\\left\\{ \\begin{array}{ll} \\ln p_0(\\theta) &amp; \\mathrm{if}\\ n_i=0 \\\\ \\ln p_1(\\theta) &amp; \\mathrm{if}\\ n_i=1 \\\\ \\vdots &amp; \\vdots \\end{array} \\right. .\\] that can be written more compactly as \\[\\sum_k I(n_i=k) \\ln p_k(\\theta).\\] this uses the notation \\(I(\\cdot)\\) to be the indicator of a set (it returns one if the event is true and 0 otherwise). Assuming independence, the log-likelihood of the data set is \\[L(\\theta)=\\sum_i \\left\\{ \\sum_k I(n_i=k) \\ln p_k(\\theta) \\right\\} = \\left\\{ \\sum_k m_k\\ln p_k(\\theta) \\right\\}\\] where we use the notation \\(m_k\\) to denote the number of observations that are observed having count \\(k\\). Using notation, \\(m_k = \\sum_i I(n_i=k)\\). Special Case. Poisson. A simple exercise in calculus yields \\[\\hat{\\lambda} = \\frac{\\mathrm{number ~of ~claims}}{\\mathrm{sample ~size}} = \\frac{\\sum_k k m_k}{\\sum_k m_k}\\] the average claim count. 2.5 Other Frequency Distributions Naturally, there are many other count distributions needed in practice For many insurance applications, one can work with one of our three basic distributions (binomial, Poisson, negative binomial) and allow the parameters to be a function of known explanatory variables. This allows us to explain claim probabilities in terms of known (to the insurer) variables such as age, sex, geographic location (territory), and so forth. This field of statistical study is known as regression analysis - it is an important topic that we will not pursue in this course. To extend our basic count distributions to alternatives needed in practice, we consider two approaches: Zero truncation or modification Mixing 2.5.1 Zero Truncation or Modification Why truncate or modify zero? If we work with a database of claims, then there are no zero! In personal lines (like auto), people may not want to report that first claim because they fear it will increase future insurance rates. Let’s modify zero probabilities in terms of the \\((a,b,0)\\) class Definition. A count distribution is a member of the (\\(a, b\\), 1) class if the probabilities \\(p_k\\) satisfy \\[\\frac{p_k}{p_{k-1}}=a+\\frac{b}{k},\\] for constants \\(a,b\\) and for \\(k=2,3, \\ldots\\). Note that this starts at \\(k=2\\), not \\(k=1\\). That is, the most important thing about this definition is that the recursion starts at \\(p_1\\), not \\(p_0\\). Thus, all distributions that are members of the (\\(a, b\\), 0) are members of the (\\(a, b\\), 1) class. Naturally, there are additional distributions that are members of this wider class. To see how this works, pick a specific distribution in the (\\(a, b\\), 0) class. Consider \\(p_k^0\\) to be a probability for this member of \\((a,b,0)\\). Let \\(p_k^M\\) be the corresponding probability for a member of \\((a,b,1)\\), where the \\(M\\) stands for “modified”. Pick a new probability of a zero claim, \\(p_0^M\\), and define \\[\\begin{aligned} c = \\frac{1-p_0^M}{1-p_0^0} .\\end{aligned}\\] We then calculate the rest of the modified distribution as \\[\\begin{aligned} p_k^M =c p_k^0\\end{aligned}\\] 2.5.1.1 Special Case: Poisson Truncated at Zero. For this case, we assume that \\(p_0^M=0\\), so that the probability of \\(N=0\\) is zero, hence the name “truncated at zero.” For this case, we use the letter \\(T\\) to denote probabilities instead of \\(M\\), so we use \\(p_k^T\\) for probabilities. Thus, \\[\\begin{aligned} p_k^T&amp;= \\left \\{ \\begin{array}{cc} 0 &amp; k=0\\\\ \\frac{1}{1-p_0^0}p_k^0 &amp; k \\ge 1\\\\ \\end{array} \\right.\\end{aligned}\\] 2.5.1.2 Modified Poisson Example Example: Zero Truncated/Modified Poisson. Consider a Poisson distribution with parameter \\(\\lambda=2\\). We show how to calculate \\(p_k, k=0,1,2,3\\), for the usual (unmodified), truncated and a modified version with \\((p_0^M=0.6)\\). Solution. For the Poisson distribution as a member of the (\\(a,b\\),0) class, we have \\(a=0\\) and \\(b=\\lambda=2\\). Thus, we may use the recursion \\(p_k = \\lambda p_{k-1}/k= 2 p_{k-1}/k\\) for each type, after determining starting probabilities. k \\(p_k\\) \\(p_k^T\\) \\(p_k^M\\) 0 \\(p_0=e^{-\\lambda}=0.135335\\) 0 0.6 1 \\(p_1=p_0(0+\\frac{\\lambda}{1})=0.27067\\) \\(\\frac{p_1}{1-p_0}=0.313035\\) \\(\\frac{1-p_0^M}{1-p_0}~p_1=0.125214\\) 2 \\(p_2=p_1\\left( \\frac{\\lambda}{2}\\right)=0.27067\\) \\(p_2^T=p_1^T\\left(\\frac{\\lambda}{2}\\right)=0.313035\\) \\(p_2^M=0.125214\\) 3 \\(p_3=p_2\\left(\\frac{\\lambda}{3}\\right)=0.180447\\) \\(p_3^T=p_2^T\\left(\\frac{\\lambda}{3}\\right)=0.208690\\) \\(p_3^M=p_2^M\\left(\\frac{\\lambda}{2}\\right)=0.083476\\) 2.5.1.3 Modified Poisson Exercise Exercise: Course 3, May 2000, Exercise 37. You are given: \\(p_k\\) denotes the probability that the number of claims equals \\(k\\) for \\(k=0,1,2,\\ldots\\) \\(\\frac{p_n}{p_m}=\\frac{m!}{n!}, m\\ge 0, n\\ge 0\\) Using the corresponding zero-modified claim count distribution with \\(p_0^M=0.1\\), calculate \\(p_1^M\\). 2.6 Mixture Distributions 2.6.1 Mixtures of Finite Populations Suppose that our population consists of several subgroups, each having their own distribution We randomly draw an observation from the population, without knowing which subgroup that we are drawing from For example, suppose that \\(N_1\\) represents claims form “good” drivers and \\(N_2\\) represents claims from “bad” drivers. We draw \\[N = \\begin{cases} N_1 &amp; \\text{with prob~}\\alpha\\\\ N_2 &amp; \\text{with prob~}(1-\\alpha) .\\\\ \\end{cases}\\] Here, \\(\\alpha\\) represents the probability of drawing a “good” driver. Our is said to be a “mixture” of two subgroups 2.6.1.1 Finite Population Mixture Example Exercise. Exam “C” 170. In a certain town the number of common colds an individual will get in a year follows a Poisson distribution that depends on the individual’s age and smoking status. The distribution of the population and the mean number of colds are as follows: Proportion of population Mean number of colds Children 0.3 3 Adult Non-Smokers 0.6 1 Adult Smokers 0.1 4 Calculate the probability that a randomly drawn person has 3 common colds in a year. Calculate the conditional probability that a person with exactly 3 common colds in a year is an adult smoker. 2.6.2 Mixtures of Infinitely Many Populations We can extend the mixture idea to an infinite number of populations. To illustrate, suppose we have a population of drivers. The \\(i\\)th person has their own (personal) expected number of claims, \\(\\lambda_i\\). For some driver’s, \\(\\lambda\\) is small (good drivers), for others it is high (not so good drivers). There is a distribution of \\(\\lambda\\). A convenient distribution is to use a gamma distribution with parameters \\((\\alpha, \\theta)\\). Then, one can check that \\[\\begin{aligned} N &amp;\\sim&amp; \\text{Negative Binomial} (r = \\alpha, \\beta = \\theta) .\\end{aligned}\\] See, for example, KPW, page 84. Mixture is very important in insurance applications, more on this later… 2.6.2.1 Negative Binomial as a Gamma Mixture of Poissons - Example Example. Suppose that \\(N|\\Lambda \\sim\\) Poisson\\((\\Lambda)\\) and that \\(\\Lambda \\sim\\) gamma with mean of 1 and variance of 2. Determine the probability that \\(N=1\\). Solution. For a gamma distribution with parameters \\((\\alpha, \\theta)\\), we have that mean is \\(\\alpha \\theta\\) and the variance is \\(\\alpha \\theta^2\\). Thus \\[\\begin{aligned} \\alpha &amp;= \\frac{1}{2} \\text{ and } \\theta =2.\\end{aligned}\\] Now, one can directly use the negative binomial approach to get \\(r = \\alpha = \\frac{1}{2}\\) and \\(\\beta= \\theta =2\\). Thus \\[\\begin{aligned} \\Pr(N=1) = p_1 &amp;= {1+r-1 \\choose 1}(\\frac{1}{(1+\\beta)^r})(\\frac{\\beta}{1+\\beta})^1 \\\\ &amp;= {1+\\frac{1}{2}-1 \\choose 1}{\\frac{1}{(1+2)^{1/2}}}(\\frac{2}{1+2})^1\\\\ &amp;= \\frac{1}{3^{3/2}} = 0.19245 .\\end{aligned}\\] 2.7 Goodness of Fit 2.7.0.1 Example: Singapore Automobile Data A 1993 portfolio of \\(n=7,483\\) automobile insurance policies from a major insurance company in Singapore. The count variable is the number of automobile accidents per policyholder. There were on average 0.06989 accidents per person. \\[ \\begin{matrix} \\hline \\textbf{Table. Comparison of Observed to Fitted Counts } \\\\ \\textbf{Based on Singapore Automobile Data} \\\\ \\begin{array}{crr} \\hline \\text{Count} &amp; \\text{Observed} &amp; \\text{Fitted Counts using the} \\\\ (k) &amp; (m_k) &amp; \\text{Poisson Distribution} (n\\widehat{p}_k) \\\\ \\hline 0 &amp; 6,996 &amp; 6,977.86 \\\\ 1 &amp; 455 &amp; 487.70 \\\\ 2 &amp; 28 &amp; 17.04 \\\\ 3 &amp; 4 &amp; 0.40 \\\\ 4 &amp; 0 &amp; 0.01 \\\\ \\hline Total &amp; 7,483 &amp; 7,483.00 \\\\ \\hline \\end{array} \\end{matrix}\\] The average is \\(\\bar{N} = \\frac{0\\cdot 6996 + 1 \\cdot 455 + 2 \\cdot 28 + 3 \\cdot 4 + 4 \\cdot 0}{7483} = 0.06989\\). 2.7.0.2 Singapore Data: Adequacy of the Poisson Model With the Poisson distribution The maximum likelihood estimator of \\(\\lambda\\) is \\(\\widehat{\\lambda}=\\overline{N}\\). Estimated probabilities, using \\(\\widehat{\\lambda}\\), are denoted as \\(\\widehat{p}_k\\). For goodness of fit, consider Pearson’s chi-square statistic \\[\\sum_k\\frac{\\left( m_k-n\\widehat{p}_k \\right) ^{2}}{n\\widehat{p}_k}.\\] Assuming that the Poisson distribution is a correct model; this statistic has an asymptotic chi-square distribution The degrees of freedom (\\(df\\)) equals the number of cells minus one minus the number of estimated parameters. For the Singapore data, this is \\(df=5-1-1=3\\). The statistic is 41.98; the basic Poisson model is inadequate. 2.7.0.3 Example. Course C/Exam 4. May 2001, 19. During a one-year period, the number of accidents per day was distributed as follows: Number of Accidents 0 1 2 3 4 5 Number of Days 209 111 33 7 5 2 You use a chi-square test to measure the fit of a Poisson distribution with mean 0.60. The minimum expected number of observations in any group should be 5. The maximum number of groups should be used. Determine the chi-square statistic. 2.8 Exercises Here are a set of exercises that guide the viewer through some of the theoretical foundations of Loss Data Analytics. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C. knitr::include_url(&quot;http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-problems/&quot;,height = &quot;600px&quot;) 2.9 Technical Supplement: Iterated Expectations 2.9.0.1 Iterated Expectations In some situations, we only observe a single outcome but can conceptualize an outcome as resulting from a two (or more) stage process. These are called two-stage, or “hierarchical,” type situations. Some special cases include: problems where the parameters of the distribution are random variables, mixture problems, where stage 1 represents the type of subpopulation and stage 2 represents a random variable with a distribution that depends on population type an aggregate distribution, where stage 1 represents the number of events and stage two represents the amount per event. In these situations, the law of iterated expectations can be useful. The law of total variation is a special case that is particularly helpful for variance calculations. To apply these rules, Identify the random variable that is being conditioned upon, typically a stage 1 outcome (that is not observed). Conditional on the stage 1 outcome, calculate summary measures such as a mean, variance, and the like. There are several results of the step (ii), one for each stage 1 outcome. Then, combine these results using the iterated expectations or total variation rules. 2.9.0.2 Iterated Expectations Consider two random variables, \\(X\\) and \\(Y\\), and a function \\(h(X,Y)\\). Assuming expectations exists and are finite, a rule/theorem from probability states that \\[\\mathrm{E~} h(X,Y)= \\mathrm{E~} \\left\\{ \\mathrm{E~} \\left( h(X,Y) | X \\right) \\right \\} .\\] This result is known as the law of iterated expectations. Here, the random variables may be discrete, continuous, or a hybrid combination of the two. Similarly, the law of total variation is \\[\\mathrm{Var~} h(X,Y)= \\mathrm{E~} \\left\\{ \\mathrm{Var~} \\left( h(X,Y) | X \\right) \\right \\} +\\mathrm{Var~} \\left\\{ \\mathrm{E~} \\left( h(X,Y) | X \\right) \\right \\},\\] the expectation of the conditional variance plus the variance of the conditional expectation. 2.9.0.3 Discrete Iterated Expectations To illustrate, suppose that \\(X\\) and \\(Y\\) are both discrete random variables with joint probability \\[p(x,y) = \\Pr(X=x, Y=y).\\] Further, let \\(p(y|x) = \\frac{p(x,y)}{\\Pr(X=x)}\\) be the conditional probability mass function. The conditional expectation is \\[\\mathrm{E~} \\left( h(X,Y) | X=x \\right) = \\sum_y h(x,y) p(y|x)\\] You can use the conditional expectation to get the unconditional expectation using \\[\\begin{aligned} \\mathrm{E~} \\left\\{ \\mathrm{E~} \\left( h(X,Y) | X \\right) \\right \\} &amp;= \\sum_x \\left\\{\\sum_y h(x,y) p(y|x) \\right \\} \\Pr(X=x) \\\\ &amp;= \\sum_x \\sum_y h(x,y) p(y|x) \\Pr(X=x) \\\\ &amp;= \\sum_x \\sum_y h(x,y) p(x,y) = \\mathrm{E~} h(X,Y)\\end{aligned}\\] The proofs of the law of iterated expectations for the continuous and hybrid cases are similar. 2.9.0.4 Law of Total Variation To see this rule, first note that we can calculate a conditional variance as \\[\\mathrm{Var~} \\left( h(X,Y) | X \\right) = \\mathrm{E~} \\left( h(X,Y)^2 | X \\right) -\\left\\{\\mathrm{E~} \\left( h(X,Y) | X \\right) \\right\\}^2.\\] From this, the expectation of the conditional variance is \\[\\begin{aligned} \\label{E:E1} \\mathrm{E~} \\mathrm{Var~} \\left( h(X,Y) | X \\right) = \\mathrm{E~} \\left( h(X,Y)^2\\right) - \\mathrm{E~}\\left\\{\\mathrm{E~} \\left( h(X,Y) | X \\right) \\right\\}^2.\\end{aligned}\\] Further, note that the conditional expectation, \\(\\mathrm{E~} \\left( h(X,Y) | X=x \\right)\\), is a function of \\(x\\), say, \\(g(x)\\). Now, \\(g(X)\\) is a random variable with mean \\(\\mathrm{E~} h(X,Y)\\) and variance \\[\\begin{aligned} \\label{E:E2} \\mathrm{Var~} \\left\\{ \\mathrm{E~} \\left( h(X,Y) | X \\right) \\right \\} &amp;=\\mathrm{Var~} g(X) \\nonumber \\\\ &amp;= \\mathrm{E~} g(X)^2\\ - \\left(\\mathrm{E~} h(X,Y)\\right)^2 \\nonumber\\\\ &amp;= \\mathrm{E~} \\left\\{\\mathrm{E~} \\left( h(X,Y) | X \\right) \\right\\}^2 - \\left(\\mathrm{E~} h(X,Y)\\right)^2\\end{aligned}\\] Adding the variance of the conditional expectation in equation to the expectation of conditional variance in equation gives the law of total variation. 2.9.0.5 Mixtures of Finite Populations: Example For example, suppose that \\(N_1\\) represents claims form “good” drivers and \\(N_2\\) represents claims from “bad” drivers. We draw \\[N = \\begin{cases} N_1 &amp; \\text{with prob~}\\alpha\\\\ N_2 &amp; \\text{with prob~}(1-\\alpha) .\\\\ \\end{cases}\\] Here, \\(\\alpha\\) represents the probability of drawing a “good” driver. Let \\(T\\) be the type, so \\(T=1\\) with prob \\(\\alpha\\) and \\(T=2\\) with prob \\(1-\\alpha\\). From the law of iterated expectations, we have \\[\\begin{aligned} \\mathrm{E~} N &amp;= \\mathrm{E~} \\left\\{ \\mathrm{E~} \\left( N | T \\right) \\right \\} \\\\ &amp;= \\mathrm{E~} N_1 \\times \\alpha + \\mathrm{E~} N_2 \\times (1-\\alpha).\\end{aligned}\\] From the law of total variation \\[\\mathrm{Var~} N= \\mathrm{E~} \\left\\{ \\mathrm{Var~} \\left( N | T \\right) \\right \\} +\\mathrm{Var~} \\left\\{ \\mathrm{E~} \\left( N | T \\right) \\right \\},\\] 2.9.0.6 Mixtures of Finite Populations: Example 2 To be more concrete, suppose that \\(N_j\\) is Poisson with parameter \\(\\lambda_j\\). Then \\[\\mathrm{Var~} N_j|T= \\mathrm{E~} N_j|T = \\begin{cases} \\lambda_1 &amp; T=1\\\\ \\lambda_2 &amp; T=2\\\\ \\end{cases}\\] Thus \\[\\mathrm{E~} \\left\\{ \\mathrm{Var~} \\left( N | T \\right) \\right \\} = \\alpha \\lambda_1+ (1-\\alpha) \\lambda_2\\] and \\[\\mathrm{Var~} \\left\\{ \\mathrm{E~} \\left( N | T \\right) \\right \\} = (\\lambda_1-\\lambda_2)^2 \\alpha (1-\\alpha)\\] (Recall: for a Bernoulli with outcomes \\(a\\) and \\(b\\) and prob \\(\\alpha\\), the variance is \\((b-a)^2\\alpha(1-\\alpha)\\)). Thus, \\[\\mathrm{Var~} N= \\alpha \\lambda_1+ (1-\\alpha) \\lambda_2 + (\\lambda_1-\\lambda_2)^2 \\alpha (1-\\alpha)\\] "],
["modeling-loss-severity.html", "Chapter 3 Modeling Loss Severity 3.1 Basic Distributional Quantities 3.2 Continuous Distributions for Modeling Loss Severity 3.3 Methods of Creating New Distributions 3.4 Coverage Modifications 3.5 Maximum Likelihood Estimation 3.6 Further Resources and Contributors 3.7 Exercises", " Chapter 3 Modeling Loss Severity October 27, 2016 Chapter Preview The traditional loss distribution approach to modeling aggregate losses starts by separately fitting a frequency distribution to the number of losses and a severity distribution to the size of losses. The estimated aggregate loss distribution combines the loss frequency distribution and the loss severity distribution by convolution. Discrete distributions often referred to as counting or frequency distributions were used in Chapter 2 to describe the number of events such as number of accidents to the driver or number of claims to the insurer. Lifetimes, asset values, losses and claim sizes are usually modeled as continuous random variables and as such are modeled using continuous distributions, often referred to as loss or severity distributions. Mixture distributions are used to model phenomenon investigated in a heterogeneous population, such as modelling more than one type of claims in liability insurance (small frequent claims and large relatively rare claims). In this chapter we explore the use of continuous as well as mixture distributions to model the random size of loss. We present key attributes that characterize continuous models and means of creating new distributions from existing ones. In this chapter we explore the effect of coverage modifications, which change the conditions that trigger a payment, such as applying deductibles, limits, or adjusting for inflation, on the distribution of individual loss amounts. 3.1 Basic Distributional Quantities In this section we calculate the basic distributional quantities: moments, percentiles and generating functions. 3.1.1 Moments Let \\(X\\) be a continuous random variable with probability density function \\(f_{X}\\left( x \\right)\\). The k-th raw moment of \\(X\\), denoted by \\(\\mu_{k}^{\\prime}\\), is the expected value of the k-th power of \\(X\\), provided it exists. The first raw moment \\(\\mu_{1}^{\\prime}\\) is the mean of \\(X\\) usually denoted by \\(\\mu\\). The formula for \\(\\mu_{k}^{\\prime}\\) is given as \\[\\mu_{k}^{\\prime} = E\\left( X^{k} \\right) = \\int_{0}^{\\infty}{x^{k}f_{X}\\left( x \\right)dx } .\\] The support of the random variable \\(X\\) is assumed to be nonnegative since actuarial phenomena are rarely negative. The k-th central moment of \\(X\\), denoted by \\(\\mu_{k}\\), is the expected value of the k-th power of the deviation of \\(X\\) from its mean \\(\\mu\\). The formula for \\(\\mu_{k}\\) is given as \\[\\mu_{k} = E\\left\\lbrack {(X - \\mu)}^{k} \\right\\rbrack = \\int_{0}^{\\infty}{\\left( x - \\mu \\right)^{k}f_{X}\\left( x \\right) dx }.\\] The second central moment \\(\\mu_{2}\\) defines the variance of \\(X\\), denoted by \\(\\sigma^{2}\\). The square root of the variance is the standard deviation \\(\\sigma\\). A further characterization of the shape of the distribution includes its degree of symmetry as well as its flatness compared to the normal distribution. The ratio of the third central moment to the cube of the standard deviation \\(\\left( \\mu_{3} / \\sigma^{3} \\right)\\) defines the coefficient of skewness which is a measure of symmetry. A positive coefficient of skewness indicates that the distribution is skewed to the right (positively skewed). The ratio of the fourth central moment to the fourth power of the standard deviation \\(\\left(\\mu_{4} / \\sigma^{4} \\right)\\) defines the coefficient of kurtosis. The normal distribution has a coefficient of kurtosis of 3. Distributions with a coefficient of kurtosis greater than 3 have heavier tails and higher peak than the normal, whereas distributions with a coefficient of kurtosis less than 3 have lighter tails and are flatter. Example 3.1 (SOA) \\(X\\) has a gamma distribution with mean 8 and skewness 1. Find the variance of \\(X\\). Solution The probability density function of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\frac{\\left( x / \\theta \\right)^{\\alpha}}{x\\Gamma\\left( \\alpha \\right)} e^{- x / \\theta} \\] for \\(x &gt; 0\\). For \\(\\alpha&gt;0\\), \\[\\mu_{k}^{\\prime} = E\\left( X^{k} \\right) = \\int_{0}^{\\infty}{\\frac{1}{\\left( \\alpha - 1 \\right)!\\theta^{\\alpha}}x^{k + \\alpha - 1}e^{- x / \\theta} dx} = \\frac{\\Gamma\\left( k + \\alpha \\right)}{\\Gamma\\left( \\alpha \\right)}\\theta^{k}\\] Given \\(\\Gamma\\left( r + 1 \\right) = r\\Gamma\\left( r \\right)\\), then \\(\\mu_{1}^{\\prime} = E\\left( X \\right) = \\alpha\\theta\\), \\(\\mu_{2}^{\\prime} = E\\left( X^{2} \\right) = \\left( \\alpha + 1 \\right)\\alpha\\theta^{2}\\), \\(\\mu_{3}^{\\prime} = E\\left( X^{3} \\right) = \\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3}\\), and \\(Var\\left( X \\right) = \\alpha\\theta^{2}\\). \\[\\text{Skewness} = \\frac{E\\left\\lbrack {(X - \\mu_{1}^{\\prime})}^{3} \\right\\rbrack}{{Var\\left( X \\right)}^{3/2}} = \\frac{\\mu_{3}^{\\prime} - 3\\mu_{2}^{\\prime}\\mu_{1}^{\\prime} + 2{\\mu_{1}^{\\prime}}^{3}}{{Var\\left( X \\right)}^{3/2}} \\\\ = \\frac{\\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3} - 3\\left( \\alpha + 1 \\right)\\alpha^{2}\\theta^{3} + 2\\alpha^{3}\\theta^{3}}{\\left( \\alpha\\theta^{2} \\right)^{3/2}} = \\frac{2}{\\alpha^{1/2}} = 1\\] Hence, \\(\\alpha = 4\\). Since, \\(E\\left( X \\right) = \\alpha\\theta = 8\\), then \\(\\theta = 2\\) and \\(Var\\left( X \\right) = \\alpha\\theta^{2} = 16\\). 3.1.2 Quantiles Percentiles can also be used to describe the characteristics of the distribution of \\(X\\). The 100pth percentile of the distribution of \\(X\\), denoted by \\(\\pi_{p}\\), is the value of \\(X\\) which satisfies \\[F_{X}\\left( {\\pi_{p}}^{-} \\right) \\leq p \\leq F\\left( \\pi_{p} \\right) ,\\] for \\(0 \\leq p \\leq 1\\). The 50-th percentile or the middle point of the distribution, \\(\\pi_{0.5}\\), is the median. Unlike discrete random variables, percentiles of continuous variables are distinct. Example 3.2 (SOA) Let \\(X\\) be a continuous random variable with density function \\(f_{X}\\left( x \\right) = \\theta e^{- \\theta x}\\), for \\(x &gt; 0\\) and 0 elsewhere. If the median of this distribution is \\(\\frac{1}{3}\\), find \\(\\theta\\). Solution \\(F_{X}\\left( x \\right) = 1 - e^{- \\theta x}\\). Then, \\(F_{X}\\left( \\pi_{0.5} \\right) = 1 - e^{- \\theta\\pi_{0.5}} = 0.5\\). Thus, \\(1 - e^{-\\theta / 3} = 0.5\\) and \\(\\theta = 3 \\ln 2\\). 3.1.3 The Moment Generating Function The moment generating function, denoted by \\(M_{X}\\left( t \\right)\\) uniquely characterizes the distribution of \\(X\\). While it is possible for two different distributions to have the same moments and yet still differ, this is not the case with the moment generating function. That is, if two random variables have the same moment generating function, then they have the same distribution. The moment generating is a real function whose k-th derivative at zero is equal to the k-th raw moment of \\(X\\). The moment generating function is given by \\[M_{X}\\left( t \\right) = E\\left( e^{\\text{tX}} \\right) = \\int_{0}^{\\infty}{e^{\\text{tx}}f_{X}\\left( x \\right) dx }\\] for all \\(t\\) for which the expected value exists. Example 3.3 (SOA) The random variable \\(X\\) has an exponential distribution with mean \\(\\frac{1}{b}\\). It is found that \\(M_{X}\\left( - b^{2} \\right) = 0.2\\). Find \\(b\\). Solution \\[M_{X}\\left( t \\right) = E\\left( e^{\\text{tX}} \\right) = \\int_{0}^{\\infty}{e^{\\text{tx}}be^{- bx} dx} = \\int_{0}^{\\infty}{be^{- x\\left( b - t \\right)} dx} = \\frac{b}{\\left( b - t \\right)}.\\] Then, \\[M_{X}\\left( - b^{2} \\right) = \\frac{b}{\\left( b + b^{2} \\right)} = \\frac{1}{\\left( 1 + b \\right)} = 0.2.\\] Thus, \\(b = 4\\). Example 3.4 Let \\(X_{1}\\), \\(X_{2}\\), ., \\(X_{n}\\) be independent \\(\\text{Ga}\\left( \\alpha_{i},\\theta \\right)\\) random variables. Find the distribution of \\(S = \\sum_{i = 1}^{n}X_{i}\\). Solution The moment generating function of \\(S\\) is \\[M_{S}\\left( t \\right) = \\text{E}\\left( e^{\\text{tS}} \\right) = E\\left( e^{t\\sum_{i = 1}^{n}X_{i}} \\right) \\\\ = E\\left( \\prod_{i = 1}^{n}e^{tX_{i}} \\right) = \\prod_{i = 1}^{n}{E\\left( e^{tX_{i}} \\right) = \\prod_{i = 1}^{n}{M_{X_{i}}\\left( t \\right)}} .\\] The moment generating function of \\(X_{i}\\) is \\(M_{X_{i}}\\left( t \\right) = \\left( 1 - \\theta t \\right)^{- \\alpha_{i}}\\). Then, \\[M_{S}\\left( t \\right) = \\prod_{i = 1}^{n}\\left( 1 - \\theta t \\right)^{- \\alpha_{i}} = \\left( 1 - \\theta t \\right)^{- \\sum_{i = 1}^{n}\\alpha_{i}}, \\] indicating that \\(S\\sim Ga\\left( \\sum_{i = 1}^{n}\\alpha_{i},\\theta \\right)\\). By finding the first and second derivatives of \\(M_{S}\\left( t \\right)\\) at zero, we can show that \\(E\\left( S \\right) = \\left. \\ \\frac{\\partial M_{S}\\left( t \\right)}{\\partial t} \\right|_{t = 0} = \\alpha\\theta\\) where \\(\\alpha = \\sum_{i = 1}^{n}\\alpha_{i}\\), and \\[E\\left( S^{2} \\right) = \\left. \\ \\frac{\\partial^{2}M_{S}\\left( t \\right)}{\\partial t^{2}} \\right|_{t = 0} = \\left( \\alpha + 1 \\right)\\alpha\\theta^{2}.\\] Hence, \\(Var\\left( S \\right) = \\alpha\\theta^{2}\\). 3.1.4 Probability Generating Function The probability generating function, denoted by \\(P_{X}\\left( z \\right)\\), also uniquely characterizes the distribution of \\(X\\). It is defined as \\[P_{X}\\left( z \\right) = E\\left( z^{X} \\right) = \\int_{0}^{\\infty}{z^{x}f_{X}\\left( x \\right) dx}\\] for all \\(z\\) for which the expected value exists. We can also use the probability generating function to generate moments of \\(X\\). By taking the k-th derivative of \\(P_{X}\\left( z \\right)\\) with respect to \\(z\\) and evaluate it at \\(z\\ = \\ 1\\), we get \\[E\\left\\lbrack X\\left( X - 1 \\right)\\ldots\\left( X - k + 1 \\right) \\right\\rbrack .\\] 3.2 Continuous Distributions for Modeling Loss Severity In this section we explain the characteristics of distributions suitable for modeling severity of losses, including gamma, Pareto, Weibull and generalized beta distribution of the second kind. Applications for which each distribution may be used are identified. 3.2.1 The Gamma Distribution The gamma distribution is commonly used in modeling claim severity. The traditional approach in modelling losses is to fit separate models for claim frequency and claim severity. When frequency and severity are modeled separately it is common for actuaries to use the Poisson distribution for claim count and the gamma distribution to model severity. An alternative approach for modelling losses that has recently gained popularity is to create a single model for pure premium (average claim cost) that will be described in Chapter 4. The continuous variable \\(X\\) is said to have the gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) if its probability density function is given by \\[f_{X}\\left( x \\right) = \\frac{\\left( x/ \\theta \\right)^{\\alpha}}{x\\Gamma\\left( \\alpha \\right)}\\exp \\left( -x/ \\theta \\right) \\ \\ \\ \\text{for } x &gt; 0 .\\] Note that \\(\\ \\alpha &gt; 0,\\ \\theta &gt; 0\\). Figures 3.1 and 3.2 demonstrate the effect of the scale and shape parameters on the gamma density function. Figure 3.1: Gamma Density, with shape=2 and Varying Scale Figure 3.2: Gamma Density, with scale=100 and Varying Shape R Code for Gamma Density Plots # Varying Scale Gamma Densities scaleparam &lt;- seq(100,250,by=50) shapeparam &lt;- 2:5 x = seq(0,1000,by=1) par(mar = c(4, 4, .1, .1)) fgamma &lt;- dgamma(x, shape = 2, scale = scaleparam[1]) plot(x, fgamma, type = &quot;l&quot;, ylab = &quot;Gamma Density&quot;) for(k in 2:length(scaleparam)){ fgamma &lt;- dgamma(x,shape = 2, scale = scaleparam[k]) lines(x,fgamma, col = k) } legend(&quot;topright&quot;, c(&quot;scale=100&quot;, &quot;scale=150&quot;, &quot;scale=200&quot;, &quot;scale=250&quot;), lty=1, col = 1:4) # Varying Shape Gamma Densities par(mar = c(4, 4, .1, .1)) fgamma &lt;- dgamma(x, shape = shapeparam[1], scale = 100) plot(x, fgamma, type = &quot;l&quot;, ylab = &quot;Gamma Density&quot;) for(k in 2:length(shapeparam)){ fgamma &lt;- dgamma(x,shape = shapeparam[k], scale = 100) lines(x,fgamma, col = k) } legend(&quot;topright&quot;, c(&quot;shape=2&quot;, &quot;shape=3&quot;, &quot;shape=4&quot;, &quot;shape=5&quot;), lty=1, col = 1:4) When \\(\\alpha = 1\\) the gamma reduces to an exponential distribution and when \\(\\alpha = \\frac{n}{2}\\) and \\(\\theta = 2\\) the gamma reduces to a chi-square distribution with \\(n\\) degrees of freedom. As we will see in Section 3.5.2, the chi-square distribution is used extensively in statistical hypothesis testing. The distribution function of the gamma model is the incomplete gamma function, denoted by \\(\\Gamma\\left( \\frac{\\alpha;x}{\\theta} \\right)\\), and defined as \\[F_{X}\\left( x \\right) = \\Gamma\\left( \\alpha; \\frac{x}{\\theta} \\right) = \\frac{1}{\\Gamma\\left( \\alpha \\right)}\\int_{0}^{x /\\theta}t^{\\alpha - 1}e^{- t}\\text{dt}\\] \\(\\alpha &gt; 0,\\ \\theta &gt; 0\\). The \\(k\\)-th moment of the gamma distributed random variable for any positive \\(k\\) is given by \\[E\\left( X^{k} \\right) = \\theta^{k} \\frac{\\Gamma\\left( \\alpha + k \\right)}{\\Gamma\\left( \\alpha \\right)} \\ \\ \\ \\text{for } k &gt; 0 .\\] The mean and variance are given by \\(E\\left( X \\right) = \\alpha\\theta\\) and \\(Var\\left( X \\right) = \\alpha\\theta^{2}\\), respectively. Since all moments exist for any positive \\(k\\), the gamma distribution is considered a light tailed distribution, which may not be suitable for modeling risky assets as it will not provide a realistic assessment of the likelihood of severe losses. 3.2.2 The Pareto Distribution The Pareto distribution, named after the Italian economist Vilfredo Pareto (1843-1923), has many economic and financial applications. It is a positively skewed and heavy-tailed distribution which makes it suitable for modeling income, high-risk insurance claims and severity of large casualty losses. The survival function of the Pareto distribution which decays slowly to zero was first used to describe the distribution of income where a small percentage of the population holds a large proportion of the total wealth. For extreme insurance claims, the tail of the severity distribution (losses in excess of a threshold) can be modelled using a Pareto distribution. The continuous variable \\(X\\) is said to have the Pareto distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) if its pdf is given by \\[f_{X}\\left( x \\right) = \\frac{\\alpha\\theta^{\\alpha}}{\\left( x + \\theta \\right)^{\\alpha + 1}} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] Figures 3.3 and 3.4 demonstrate the effect of the scale and shape parameters on the Pareto density function. Figure 3.3: Pareto Density, with shape=3 and Varying Scale Figure 3.4: Pareto Density, with scale=2000 and Varying Shape R Code for Pareto Density Plots # Varying Scale Pareto Densities #install.packages(&quot;VGAM&quot;) library(VGAM) scaleparam &lt;- seq(2000,3500,500) shapeparam &lt;- 1:4 z&lt;- seq(1,3000,by=1) fpareto &lt;- dpareto(z, shape = 3, scale = scaleparam[1]) plot(z, fpareto, ylim=c(0,0.002),type = &quot;l&quot;, ylab = &quot;Pareto Density&quot;) for(k in 2:length(shapeparam)){ fpareto &lt;- dpareto(z,shape = 3, scale = scaleparam[k]) lines(z,fpareto, col = k) } legend(&quot;topright&quot;, c(&quot;scale=2000&quot;, &quot;scale=2500&quot;, &quot;scale=3000&quot;, &quot;scale=3500&quot;), lty=1, col = 1:4) # Varying Shape Pareto Densities fpareto &lt;- dpareto(z, shape = shapeparam[1], scale = 2000) plot(z, fpareto, ylim=c(0,0.002),type = &quot;l&quot;, ylab = &quot;Pareto Density&quot;) for(k in 2:length(shapeparam)){ fpareto &lt;- dpareto(z,shape = shapeparam[k], scale = 2000) lines(z,fpareto, col = k)} legend(&quot;topright&quot;, c(&quot;shape=1&quot;, &quot;shape=2&quot;, &quot;shape=3&quot;, &quot;shape=4&quot;), lty=1, col = 1:4) The distribution function of the Pareto distribution is given by \\[F_{X}\\left( x \\right) = 1 - \\left( \\frac{\\theta}{x + \\theta} \\right)^{\\alpha} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] It can be easily seen that the hazard function of the Pareto distribution is a decreasing function in \\(x\\), another indication that the distribution is heavy tailed. The \\(k\\)-th moment of the Pareto distributed random variable exists, if and only if, \\(\\alpha &gt; k\\). If \\(k\\) is a positive integer then \\[E\\left( X^{k} \\right) = \\frac{k!\\theta^{k}}{\\left( \\alpha - 1 \\right)\\cdots\\left( \\alpha - k \\right)} \\ \\ \\ \\alpha &gt; k.\\] The mean and variance are given by \\[E\\left( X \\right) = \\frac{\\theta}{\\alpha - 1} \\ \\ \\ \\text{for } \\alpha &gt; 1\\] and \\[Var\\left( X \\right) = \\frac{\\alpha\\theta^{2}}{\\left( \\alpha - 1 \\right)^{2}\\left( \\alpha - 2 \\right)} \\ \\ \\ \\text{for } \\alpha &gt; 2,\\]respectively. Example 3.5 The claim size of an insurance portfolio follows the Pareto distribution with mean and variance of 40 and 1800 respectively. Find The shape and scale parameters. The 95-th percentile of this distribution. Solution \\(E\\left( X \\right) = \\frac{\\theta}{\\alpha - 1} = 40\\) and \\(Var\\left( X \\right) = \\frac{\\alpha\\theta^{2}}{\\left( \\alpha - 1 \\right)^{2}\\left( \\alpha - 2 \\right)} = 1800\\). By dividing the square of the first equation by the second we get \\(\\frac{\\alpha - 2}{\\alpha} = \\frac{40^{2}}{1800}\\). Thus, \\(\\alpha = 18.02\\) and \\(\\theta = 680.72\\). The 95-th percentile, \\(\\pi_{0.95}\\), satisfies the equation \\[F_{X}\\left( \\pi_{0.95} \\right) = 1 - \\left( \\frac{680.72}{\\pi_{0.95} + 680.72} \\right)^{18.02} = 0.95.\\] Thus, \\(\\pi_{0.95} = 122.96\\). 3.2.3 The Weibull Distribution The Weibull distribution, named after the Swedish physicist Waloddi Weibull (1887-1979) is widely used in reliability, life data analysis, weather forecasts and general insurance claims. Truncated data arise frequently in insurance studies. The Weibull distribution is particularly useful in modeling left-truncated claim severity distributions. Weibull was used to model excess of loss treaty over automobile insurance as well as earthquake inter-arrival times. The continuous variable \\(X\\) is said to have the Weibull distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) if its probability density function is given by \\[f_{X}\\left( x \\right) = \\frac{\\alpha}{\\theta}\\left( \\frac{x}{\\theta} \\right)^{\\alpha - 1} \\exp \\left(- \\left( \\frac{x}{\\theta} \\right)^{\\alpha}\\right) \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] Figures 3.5 and 3.6 demonstrate the effects of the scale and shape parameters on the Weibull density function. Figure 3.5: Weibull Density, with shape=3 and Varying Scale Figure 3.6: Weibull Density, with scale=100 and Varying Shape R Code for Weibull Density Plots # Varying Scale Weibull Densities z&lt;- seq(0,400,by=1) scaleparam &lt;- seq(50,200,50) shapeparam &lt;- seq(1.5,3,0.5) plot(z, dweibull(z, shape = 3, scale = scaleparam[1]), type = &quot;l&quot;, ylab = &quot;Weibull density&quot;) for(k in 2:length(scaleparam)){ lines(z,dweibull(z,shape = 3, scale = scaleparam[k]), col = k)} legend(&quot;topright&quot;, c(&quot;scale=50&quot;, &quot;scale=100&quot;, &quot;scale=150&quot;, &quot;scale=200&quot;), lty=1, col = 1:4) # Varying Shape Weibull Densities plot(z, dweibull(z, shape = shapeparam[1], scale = 100), ylim=c(0,0.012), type = &quot;l&quot;, ylab = &quot;Weibull density&quot;) for(k in 2:length(shapeparam)){ lines(z,dweibull(z,shape = shapeparam[k], scale = 100), col = k)} legend(&quot;topright&quot;, c(&quot;shape=1.5&quot;, &quot;shape=2&quot;, &quot;shape=2.5&quot;, &quot;shape=3&quot;), lty=1, col = 1:4) The distribution function of the Weibull distribution is given by \\[F_{X}\\left( x \\right) = 1 - e^{- \\left( x / \\theta \\right)^{\\alpha}} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] It can be easily seen that the shape parameter \\(\\alpha\\) describes the shape of the hazard function of the Weibull distribution. The hazard function is a decreasing function when \\(\\alpha &lt; 1\\), constant when \\(\\alpha = 1\\) and increasing when \\(\\alpha &gt; 1\\). This behavior of the hazard function makes the Weibull distribution a suitable model for a wide variety of phenomena such as weather forecasting, electrical and industrial engineering, insurance modeling and financial risk analysis. The \\(k\\)-th moment of the Weibull distributed random variable is given by \\[E\\left( X^{k} \\right) = \\theta^{k}\\Gamma\\left( 1 + \\frac{k}{\\alpha} \\right) .\\] The mean and variance are given by \\[E\\left( X \\right) = \\theta\\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right)\\] and \\[Var(X)= \\theta^{2}\\left( \\Gamma\\left( 1 + \\frac{2}{\\alpha} \\right) - \\left\\lbrack \\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right) \\right\\rbrack ^{2}\\right),\\] respectively. Example 3.6 Suppose that the probability distribution of the lifetime of AIDS patients (in months) from the time of diagnosis is described by the Weibull distribution with shape parameter 1.2 and scale parameter 33.33. Find the probability that a randomly selected person from this population survives at least 12 months, A random sample of 10 patients will be selected from this population. What is the probability that at most two will die within one year of diagnosis. Find the 99-th percentile of this distribution. Solution Let \\(X\\) be the lifetime of AIDS patients (in months) \\[{\\Pr\\left( X \\geq 12 \\right) = S}_{X}\\left( 12 \\right) = e^{- \\left( \\frac{12}{33.33} \\right)^{1.2}} = 0.746.\\] Let \\(Y\\) be the number of patients who die within one year of diagnosis. Then, \\(Y\\sim Bin\\left( 10,\\ 0.254 \\right)\\) and \\(\\Pr\\left( Y \\leq 2 \\right) = 0.514.\\) Let \\(\\pi_{0.99}\\) denote the 99-th percentile of this distribution. Then, \\[S_{X}\\left( \\pi_{0.99} \\right) = \\exp\\left\\{- \\left( \\frac{\\pi_{0.99}}{33.33} \\right)^{1.2}\\right\\} = 0.01\\] and \\(\\pi_{0.99} = 118.99\\). 3.2.4 The Generalized Beta Distribution of the Second Kind The Generalized Beta Distribution of the Second Kind (GB2) was introduced by Venter (1983) in the context of insurance loss modeling and by McDonald (1984) as an income and wealth distribution. It is a four-parameter very flexible distribution that can model positively as well as negatively skewed distributions. The continuous variable \\(X\\) is said to have the GB2 distribution with parameters \\(a\\), \\(b\\), \\(\\alpha\\) and \\(\\beta\\) if its probability density function is given by \\[f_{X}\\left( x \\right) = \\frac{ax^{a \\alpha - 1}}{b^{a \\alpha}B\\left( \\alpha,\\beta \\right)\\left\\lbrack 1 + \\left( x/b \\right)^{a} \\right\\rbrack^{\\alpha + \\beta}} \\ \\ \\ \\text{for } x &gt; 0,\\] \\(a,b,\\alpha,\\beta &gt; 0\\), and where the beta function \\(B\\left( \\alpha,\\beta \\right)\\) is defined as \\[B\\left( \\alpha,\\beta \\right) = \\int_{0}^{1}{t^{\\alpha - 1}\\left( 1 - t \\right)^{\\beta - 1}}\\text{dt}.\\] The GB2 provides a model for heavy as well as light tailed data. It includes the exponential, gamma, Weibull, Burr, Lomax, F, chi-square, Rayleigh, lognormal and log-logistic as special or limiting cases. For example, by setting the parameters \\(a = \\alpha = \\beta = 1\\), then the GB2 reduces to the log-logistic distribution. When \\(a = 1\\) and \\(\\beta \\rightarrow \\infty\\), it reduces to the gamma distribution and when \\(\\alpha = 1\\) and \\(\\beta \\rightarrow \\infty\\), it reduces to the Weibull distribution. The \\(k\\)-th moment of the GB2 distributed random variable is given by \\[E\\left( X^{k} \\right) = \\frac{b^{k}\\left( \\alpha + \\frac{k}{a},\\beta - \\frac{k}{a} \\right)}{\\left( \\alpha,\\beta \\right)}, \\ \\ \\ k &gt; 0.\\] Earlier applications of the GB2 were on income data and more recently have been used to model long-tailed claims data. GB2 was used to model different types of automobile insurance claims, severity of fire losses as well as medical insurance claim data. 3.3 Methods of Creating New Distributions In this section we understand connections among the distributions; give insights into when a distribution is preferred when compared to alternatives; provide foundations for creating new distributions. 3.3.1 Functions of Random Variables and their Distributions In Section 3.2 we discussed some elementary known distributions. In this section we discuss means of creating new parametric probability distributions from existing ones. Let \\(X\\) be a continuous random variable with a known probability density function \\(f_{X}(x)\\) and distribution function \\(F_{X}(x)\\). Consider the transformation \\(Y = g\\left( X \\right)\\), where \\(g(X)\\) is a one-to-one transformation defining a new random variable \\(Y\\). We can use the distribution function technique, the change-of-variable technique or the moment-generating function technique to find the probability density function of the variable of interest \\(Y\\). In this section we apply the following techniques for creating new families of distributions: (a) multiplication by a constant (b) raising to a power, (c) exponentiation and (d) mixing. 3.3.2 Multiplication by a Constant If claim data show change over time then such transformation can be useful to adjust for inflation. If the level of inflation is positive then claim costs are rising, and if it is negative then costs are falling. To adjust for inflation we multiply the cost \\(X\\) by 1+ inflation rate (negative inflation is deflation). To account for currency impact on claim costs we also use a transformation to apply currency conversion from a base to a counter currency. Consider the transformation \\(Y = cX\\), where \\(c &gt; 0\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( cX \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{c} \\right) = F_{X}\\left( \\frac{y}{c} \\right).\\] Hence, the probability density function of interest \\(f_{Y}(y)\\) can be written as \\[f_{Y}\\left( y \\right) = \\frac{1}{c}f_{X}\\left( \\frac{y}{c} \\right).\\] Suppose that \\(X\\) belongs to a certain set of parametric distributions and define a rescaled version \\(Y\\ = \\ cX\\), \\(c\\ &gt; \\ 0\\). If \\(Y\\) is in the same set of distributions then the distribution is said to be a scale distribution. When a member of a scale distribution is multiplied by a constant \\(c\\) (\\(c &gt; 0\\)), the scale parameter for this scale distribution meets two conditions: The parameter is changed by multiplying by \\(c\\); All other parameter remain unchanged. Example 3.7 (SOA) The aggregate losses of Eiffel Auto Insurance are denoted in Euro currency and follow a Lognormal distribution with \\(\\mu = 8\\) and \\(\\sigma = 2\\). Given that 1 euro \\(=\\) 1.3 dollars, find the set of lognormal parameters, which describe the distribution of Eiffel’s losses in dollars? Solution Let \\(X\\) and \\(Y\\) denote the aggregate losses of Eiffel Auto Insurance in euro currency and dollars respectively. Then, \\(Y = 1.3X\\). \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( 1.3X \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{1.3} \\right) = F_{X}\\left( \\frac{y}{1.3} \\right).\\] \\(X\\) follows a lognormal distribution with parameters \\(\\mu = 8\\) and \\(\\sigma = 2\\). The probability density function of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\ln x - \\mu}{\\sigma} \\right)^{2}\\right\\} \\ \\ \\ \\text{for } x &gt; 0.\\] Then, the probability density function of interest \\(f_{Y}(y)\\) is \\[f_{Y}\\left( y \\right) = \\frac{1}{1.3}f_{X}\\left( \\frac{y}{1.3} \\right) \\\\ = \\frac{1}{1.3}\\frac{1.3}{y \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\ln\\left( y/1.3 \\right) - \\mu}{\\sigma} \\right)^{2}\\right\\} \\\\ = \\frac{1}{y \\sigma\\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\ln y - \\left( \\ln 1.3 + \\mu \\right)}{\\sigma} \\right)^{2}\\right\\}.\\] Then \\(Y\\) follows a lognormal distribution with parameters \\(\\ln 1.3 + \\mu = 8.26\\) and \\(\\sigma = 2.00\\). If we let \\(\\mu = ln(m)\\) then it can be easily seen that \\(m\\)=\\(e^{\\mu}\\) is the scale parameter which was multiplied by 1.3 while \\(\\sigma\\) is the shape parameter that remained unchanged. Example 3.8 Demonstrate that the gamma distribution is a scale distribution. Solution Let \\(X\\sim Ga(\\alpha,\\theta)\\) and \\(Y = cX\\), then \\[f_{Y}\\left( y \\right) = \\frac{1}{c}f_{X}\\left( \\frac{y}{c} \\right) = \\frac{\\left( \\frac{y}{c\\theta} \\right)^{\\alpha}}{y\\Gamma\\left( \\alpha \\right)}\\exp \\left( - \\frac{y}{c\\theta} \\right) .\\] We can see that \\(Y\\sim Ga(\\alpha,c\\theta)\\) indicating that gamma is a scale distribution and \\(\\theta\\) is a scale parameter. 3.3.3 Raising to a Power In the previous section we have talked about the flexibility of the Weibull distribution in fitting reliability data. Looking to the origins of the Weibull distribution, we recognize that the Weibull is a power transformation of the exponential distribution. This is an application of another type of transformation which involves raising the random variable to a power. Consider the transformation \\(Y = X^{\\tau}\\), where \\(\\tau &gt; 0\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\leq y^{1/ \\tau} \\right) = F_{X}\\left( y^{1/ \\tau} \\right).\\] Hence, the probability density function of interest \\(f_{Y}(y)\\) can be written as \\[f_{Y}(y) = \\frac{1}{\\tau} y^{1/ \\tau - 1} f_{X}\\left( y^{1/ \\tau} \\right).\\] On the other hand, if \\(\\tau &lt; 0\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\geq y^{1/ \\tau} \\right) = 1 - F_{X}\\left( y^{1/ \\tau} \\right), \\] and \\[f_{Y}(y) = \\left| \\frac{1}{\\tau} \\right|{y^{1/ \\tau - 1}f}_{X}\\left( y^{1/ \\tau} \\right).\\] Example 3.9 We assume that \\(X\\) follows the exponential distribution with mean \\(\\theta\\) and consider the transformed variable \\(Y = X^{\\tau}\\). Show that \\(Y\\) follows the Weibull distribution when \\(\\tau\\) is positive and determine the parameters of the Weibull distribution. Solution \\[f_{X}(x) = \\frac{1}{\\theta}e^{- x/ \\theta} \\ \\ \\ \\, x &gt; 0.\\] \\[f_{Y}\\left( y \\right) = \\frac{1}{\\tau}{y^{\\frac{1}{\\tau} - 1}f}_{X}\\left( y^{\\frac{1}{\\tau}} \\right) \\\\ = \\frac{1}{\\tau \\theta }y^{\\frac{1}{\\tau} - 1}e^{- \\frac{y^{\\frac{1}{\\tau}}}{\\theta}} = \\frac{\\alpha}{\\beta}\\left( \\frac{y}{\\beta} \\right)^{\\alpha - 1}e^{- \\left( y/ \\beta \\right)^{\\alpha}}.\\] where \\(\\alpha = \\frac{1}{\\tau}\\) and \\(\\beta = \\theta^{\\tau}\\). Then, \\(Y\\) follows the Weibull distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\beta\\). 3.3.4 Exponentiation The normal distribution is a very popular model for a wide number of applications and when the sample size is large, it can serve as an approximate distribution for other models. If the random variable \\(X\\) has a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then \\(Y = e^{X}\\) has lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^{2}\\). The lognormal random variable has a lower bound of zero, is positively skewed and has a long right tail. A lognormal distribution is commonly used to describe distributions of financial assets such as stock prices. It is also used in fitting claim amounts for automobile as well as health insurance. This is an example of another type of transformation which involves exponentiation. Consider the transformation \\(Y = e^{X}\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\ln y \\right) = F_{X}\\left( \\ln y \\right).\\] Hence, the probability density function of interest \\(f_{Y}(y)\\) can be written as \\[f_{Y}(y) = \\frac{1}{y}f_{X}\\left( \\ln y \\right).\\] Example 3.10 (SOA) \\(X\\) has a uniform distribution on the interval \\((0,\\ c)\\). \\(Y = e^{X}\\). Find the distribution of \\(Y\\). Solution \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\ln y \\right) = F_{X}\\left( \\ln y \\right).\\] Then, \\[f_{Y}\\left( y \\right) = \\frac{1}{y}f_{X}\\left(\\ln y \\right) = \\frac{1}{\\text{cy}}. \\] Since \\(0 &lt; x &lt; c\\), then \\(1 &lt; y &lt; e^{c}\\). 3.3.5 Finite Mixtures Mixture distributions represent a useful way of modelling data that are drawn from a heterogeneous population. This parent population can be thought to be divided into multiple subpopulations with distinct distributions. 3.3.5.1 Two-point Mixture If the underlying phenomenon is diverse and can actually be described as two phenomena representing two subpopulations with different modes, we can construct the two point mixture random variable \\(X\\). Given random variables \\(X_{1}\\) and \\(X_{2}\\), with probability density functions \\(f_{X_{1}}\\left( x \\right)\\) and \\(f_{X_{2}}\\left( x \\right)\\) respectively, the probability density function of \\(X\\) is the weighted average of the component probability density function \\(f_{X_{1}}\\left( x \\right)\\) and \\(f_{X_{2}}\\left( x \\right)\\). The probability density function and distribution function of \\(X\\) are given by \\[f_{X}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right),\\] and \\[F_{X}\\left( x \\right) = aF_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)F_{X_{2}}\\left( x \\right),\\] for \\(0 &lt; a &lt;1\\), where the mixing parameters \\(a\\) and \\((1 - a)\\) represent the proportions of data points that fall under each of the two subpopulations respectively. This weighted average can be applied to a number of other distribution related quantities. The k-th moment and moment generating function of \\(X\\) are given by \\(E\\left( X^{k} \\right) = aE\\left( X_{1}^{K} \\right) + \\left( 1 - a \\right)E\\left( X_{2}^{k} \\right)\\), and \\[M_{X}\\left( t \\right) = aM_{X_{1}}\\left( t \\right) + \\left( 1 - a \\right)M_{X_{2}}\\left( t \\right),\\] respectively. Example 3.11 (SOA) The distribution of the random variable \\(X\\) is an equally weighted mixture of two Poisson distributions with parameters \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) respectively. The mean and variance of \\(X\\) are 4 and 13, respectively. Determine \\(\\Pr\\left( X &gt; 2 \\right)\\). Solution \\[E\\left( X \\right) = 0.5\\lambda_{1} + 0.5\\lambda_{2} = 4\\] \\[E\\left( X^{2} \\right) = 0.5\\left( \\lambda_{1} + \\lambda_{1}^{2} \\right) + 0.5\\left( \\lambda_{2} + \\lambda_{2}^{2} \\right) = 13 + 16\\] Simplifying the two equations we get \\(\\lambda_{1} + \\lambda_{2} = 8\\) and \\(\\lambda_{1}^{2} + \\lambda_{2}^{2} = 50\\). Then, the parameters of the two Poisson distributions are 1 and 7. \\[\\Pr\\left( X &gt; 2 \\right) = 0.5\\Pr\\left( X_{1} &gt; 2 \\right) + 0.5\\Pr\\left( X_{2} &gt; 2 \\right) = 0.05\\] 3.3.5.2 k-point Mixture In case of finite mixture distributions, the random variable of interest \\(X\\) has a probability \\(p_{i}\\) of being drawn from homogeneous subpopulation \\(i\\), where \\(i = 1,2,\\ldots,k\\) and \\(k\\) is the initially specified number of subpopulations in our mixture. The mixing parameter \\(p_{i}\\) represents the proportion of observations from subpopulation \\(i\\). Consider the random variable \\(X\\) generated from \\(k\\) distinct subpopulations, where subpopulation \\(i\\) is modeled by the continuous distribution \\(f_{X_{i}}\\left( x \\right)\\). The probability distribution of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}f_{X_{i}}\\left( x \\right)},\\] where \\(0 &lt; p_{i} &lt; 1\\) and \\(\\sum_{i = 1}^{k} p_{i} = 1\\). This model is often referred to as a finite mixture or a \\(k\\) point mixture. The distribution function, \\(r\\)-th moment and moment generating functions of the \\(k\\)-th point mixture are given as \\[F_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}F_{X_{i}}\\left( x \\right)},\\] \\[E\\left( X^{r} \\right) = \\sum_{i = 1}^{k}{p_{i}E\\left( X_{i}^{r} \\right)}, \\text{and}\\] \\[M_{X}\\left( t \\right) = \\sum_{i = 1}^{k}{p_{i}M_{X_{i}}\\left( t \\right)},\\] respectively. Example 3.12 (SOA) \\(Y_{1}\\) is a mixture of \\(X_{1}\\) and \\(X_{2}\\) with mixing weights \\(a\\) and \\((1 - a)\\). \\(Y_{2}\\) is a mixture of \\(X_{3}\\) and \\(X_{4}\\) with mixing weights \\(b\\) and \\((1 - b)\\). \\(Z\\) is a mixture of \\(Y_{1}\\) and \\(Y_{2}\\) with mixing weights \\(c\\) and \\((1 - c)\\). Show that \\(Z\\) is a mixture of \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\) and \\(X_{4}\\), and find the mixing weights. Solution \\[f_{Y_{1}}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right)\\] \\[f_{Y_{2}}\\left( x \\right) = bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right)\\] \\[f_{Z}\\left( x \\right) = cf_{Y_{1}}\\left( x \\right) + \\left( 1 - c \\right)f_{Y_{2}}\\left( x \\right)\\] \\[f_{Z}\\left( x \\right) = c\\left\\lbrack af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) \\right\\rbrack + \\left( 1 - c \\right)\\left\\lbrack bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\right\\rbrack\\] \\(= caf_{X_{1}}\\left( x \\right) + c\\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) + \\left( 1 - c \\right)bf_{X_{3}}\\left( x \\right) + (1 - c)\\left( 1 - b \\right)f_{X_{4}}\\left( x \\right)\\). Then, \\(Z\\) is a mixture of \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\) and \\(X_{4}\\), with mixing weights \\(\\text{ca}\\), \\(c\\left( 1 - a \\right)\\), \\(\\left( 1 - c \\right)b\\) and \\((1 - c)\\left( 1 - b \\right)\\). 3.3.6 Continuous Mixtures A mixture with a very large number of subpopulations (\\(k\\) goes to infinity) is often referred to as a continuous mixture. In a continuous mixture, subpopulations are not distinguished by a discrete mixing parameter but by a continuous variable \\(\\theta\\), where \\(\\theta\\) plays the role of \\(p_{i}\\) in the finite mixture. Consider the random variable \\(X\\) with a distribution depending on a parameter \\(\\theta\\), where \\(\\theta\\) itself is a continuous random variable. This description yields the following model for \\(X\\) \\[f_{X}\\left( x \\right) = \\int_{0}^{\\infty}{f_{X}\\left( x\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)} d \\theta ,\\] where \\(f_{X}\\left( x\\left| \\theta \\right.\\ \\right)\\) is the conditional distribution of \\(X\\) at a particular value of \\(\\theta\\) and \\(g\\left( \\theta \\right)\\) is the probability statement made about the unknown parameter \\(\\theta\\), known as the prior distribution of \\(\\theta\\) (the prior information or expert opinion to be used in the analysis). The distribution function, \\(k\\)-th moment and moment generating functions of the continuous mixture are given as \\[F_{X}\\left( x \\right) = \\int_{-\\infty}^{\\infty}{F_{X}\\left( x\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)} d \\theta,\\] \\[E\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{E\\left( X^{k}\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)}d \\theta,\\] \\[M_{X}\\left( t \\right) = E\\left( e^{t X} \\right) = \\int_{-\\infty}^{\\infty}{E\\left( e^{ tx}\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)}d \\theta, \\] respectively. The \\(k\\)-th moments of the mixture distribution can be rewritten as \\[E\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{E\\left( X^{k}\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)}d\\theta = E\\left\\lbrack E\\left( X^{k}\\left| \\theta \\right.\\ \\right) \\right\\rbrack .\\] In particular the mean and variance of \\(X\\) are given by \\[E\\left( X \\right) = E\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack\\] and \\[Var\\left( X \\right) = E\\left\\lbrack Var\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack + Var\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack .\\] Example 3.13 (SOA) \\(X\\) has a binomial distribution with a mean of \\(100q\\) and a variance of \\(100q\\left( 1 - q \\right)\\) and \\(q\\) has a beta distribution with parameters \\(a = 3\\) and \\(b = 2\\). Find the unconditional mean and variance of \\(X\\). Solution \\(E\\left( q \\right) = \\frac{a}{a + b} = \\frac{3}{5}\\) and \\(E\\left( q^{2} \\right) = \\frac{a\\left( a + 1 \\right)}{\\left( a + b \\right)\\left( a + b + 1 \\right)} = \\frac{2}{5}\\). \\(E\\left( X \\right) = E\\left\\lbrack E\\left( X\\left| q \\right.\\ \\right) \\right\\rbrack = E\\left( 100q \\right) = 100E\\left( q \\right) = 60\\), \\[Var\\left( X \\right) = E\\left\\lbrack Var\\left( X\\left| q \\right.\\ \\right) \\right\\rbrack + Var\\left\\lbrack E\\left( X\\left| q \\right.\\ \\right) \\right\\rbrack = E\\left\\lbrack 100q\\left( 1 - q \\right) \\right\\rbrack + Var\\left( 100q \\right)\\] \\(= 100E\\left( q \\right) - 100E\\left( q^{2} \\right) + 100^{2}V\\left( q \\right) = 420\\). Exercise 3.14 (SOA) Claim sizes, \\(X\\), are uniform on for each policyholder. varies by policyholder according to an exponential distribution with mean 5. Find the unconditional distribution, mean and variance of \\(X\\). Solution The conditional distribution of \\(X\\) is \\(f_{X}\\left( \\left. \\ x \\right|\\theta \\right) = \\frac{1}{10}\\) for \\(\\theta &lt; x &lt; \\theta + 10\\). The prior distribution of \\(\\theta\\) is \\(g\\left( \\theta \\right) = \\frac{1}{5}e^{- \\frac{\\theta}{5}}\\) for \\(0 &lt; \\theta &lt; \\infty\\). The conditional mean and variance of \\(X\\) are given by \\[E\\left( \\left. \\ X \\right|\\theta \\right) = \\frac{\\theta + \\theta + 10}{2} = \\theta + 5\\] and \\[Var\\left( \\left. \\ X \\right|\\theta \\right) = \\frac{\\left\\lbrack \\left( \\theta + 10 \\right) - \\theta \\right\\rbrack^{2}}{12} = \\frac{100}{12}, \\] respectively. Hence, the unconditional mean and variance of \\(X\\) are given by \\[E\\left( X \\right) = E\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack = E\\left( \\theta + 5 \\right) = E\\left( \\theta \\right) + 5 = 5 + 5 = 10,\\] and \\[Var\\left( X \\right) = E\\left\\lbrack V\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack + Var\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack \\\\ = E\\left( \\frac{100}{12} \\right) + Var\\left( \\theta + 5 \\right) = 8.33 + Var\\left( \\theta \\right) = 33.33. \\] The unconditional distribution of \\(X\\) is \\[f_{X}\\left( x \\right) = \\int_{}^{}{f_{X}\\left( x |\\theta \\right) ~g\\left( \\theta \\right) d \\theta} .\\] \\[f_{X}\\left( x \\right) = \\left\\{ \\begin{matrix} \\int_{0}^{x}{\\frac{1}{50}e^{- \\frac{\\theta}{5}}d\\theta = \\frac{1}{10}\\left( 1 - e^{- \\frac{x}{5}} \\right)} &amp; 0 \\leq x \\leq 10, \\\\ \\int_{x - 10}^{x}{\\frac{1}{50}e^{- \\frac{\\theta}{5}} d\\theta} = \\frac{1}{10}\\left( e^{- \\frac{\\left( x - 10 \\right)}{5}} - e^{- \\frac{x}{5}} \\right) &amp; 10 &lt; x &lt; \\infty. \\\\ \\end{matrix} \\right.\\ \\] 3.4 Coverage Modifications In this section we evaluate the impacts of coverage modifications: a) deductibles, b) policy limit, c) coinsurance and inflation on insurer’s costs. 3.4.1 Policy Deductibles Under an ordinary deductible policy, the insured (policyholder) agrees to cover a fixed amount of an insurance claim before the insurer starts to pay. This fixed expense paid out of pocket is called the deductible and often denoted by \\(d\\). The insurer is responsible for covering the loss \\(X\\) less the deductible \\(d\\). Depending on the agreement, the deductible may apply to each covered loss or to a defined benefit period (month, year, etc.) Deductibles eliminate a large number of small claims, reduce costs of handling and processing these claims, reduce premiums for the policyholders and reduce moral hazard. Moral hazard occurs when the insured takes more risks, increasing the chances of loss due to perils insured against, knowing that the insurer will incur the cost (e.g. a policyholder with collision insurance may be encouraged to drive recklessly). The larger the deductible, the less the insured pays in premiums for an insurance policy. Let \\(X\\) denote the loss incurred to the insured and \\(Y\\) denote the amount of paid claim by the insurer. Speaking of the benefit paid to the policyholder, we differentiate between two variables: The payment per loss and the payment per payment. The payment per loss variable, denoted by \\(Y^{L}\\), includes losses for which a payment is made as well as losses less than the deductible and hence is defined as \\[Y^{L} = \\left( X - d \\right)_{+} = \\left\\{ \\begin{array}{cc} 0 &amp; X &lt; d, \\\\ X - d &amp; X &gt; d \\end{array} \\right. .\\] \\(Y^{L}\\) is often referred to as left censored and shifted variable because the values below \\(d\\) are not ignored and all losses are shifted by a value \\(d\\). On the other hand, the payment per payment variable, denoted by \\(Y^{P}\\), is not defined when there is no payment and only includes losses for which a payment is made. The variable is defined as \\[Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d \\end{matrix} \\right. \\] \\(Y^{P}\\) is often referred to as left truncated and shifted variable or excess loss variable because the claims smaller than \\(d\\) are not reported and values above \\(d\\) are shifted by \\(d\\). Even when the distribution of \\(X\\) is continuous, the distribution of \\(Y^{L}\\) is partly discrete and partly continuous. The discrete part of the distribution is concentrated at \\(Y = 0\\) (when \\(X \\leq d\\)) and the continuous part is spread over the interval \\(Y &gt; 0\\) (when \\(X &gt; d\\)). For the discrete part, the probability that no payment is made is the probability that losses fall below the deductible; that is, \\[\\Pr\\left( Y^{L} = 0 \\right) = \\Pr\\left( X \\leq d \\right) = F_{X}\\left( d \\right).\\] Using the transformation \\(Y^{L} = X - d\\) for the continuous part of the distribution, we can find the probability density function of \\(Y^{L}\\) given by \\[f_{Y^{L}}\\left( y \\right) = \\left\\{ \\begin{matrix} F_{X}\\left( d \\right) &amp; y = 0, \\\\ f_{X}\\left( y + d \\right) &amp; y &gt; 0 \\end{matrix} \\right. \\] We can see that the payment per payment variable is the payment per loss variable conditioned on the loss exceeding the deductible; that is, \\(Y^{P} = \\left. \\ Y^{L} \\right|X &gt; d\\). Hence, the probability density function of \\(Y^{P}\\) is given by \\[f_{Y^{P}}\\left( y \\right) = \\frac{f_{X}\\left( y + d \\right)}{1 - F_{X}\\left( d \\right)},\\] for \\(y &gt; 0\\). Accordingly, the distribution functions of \\(Y^{L}\\)and \\(Y^{P}\\) are given by \\[F_{Y^{L}}\\left( y \\right) = \\left\\{ \\begin{matrix} F_{X}\\left( d \\right) &amp; y = 0, \\\\ F_{X}\\left( y + d \\right) &amp; y &gt; 0. \\\\ \\end{matrix} \\right.\\ \\] and \\[F_{Y^{P}}\\left( y \\right) = \\frac{F_{X}\\left( y + d \\right) - F_{X}\\left( d \\right)}{1 - F_{X}\\left( d \\right)},\\] for \\(y &gt; 0\\), respectively. The raw moments of \\(Y^{L}\\) and \\(Y^{P}\\) can be found directly using the probability density function of \\(X\\) as follows \\[E\\left\\lbrack \\left( Y^{L} \\right)^{k} \\right\\rbrack = \\int_{d}^{\\infty}\\left( x - d \\right)^{k}f_{X}\\left( x \\right)dx ,\\] and \\[E\\left\\lbrack \\left( Y^{P} \\right)^{k} \\right\\rbrack = \\frac{\\int_{d}^{\\infty}\\left( x - d \\right)^{k}f_{X}\\left( x \\right) dx }{{1 - F}_{X}\\left( d \\right)} = \\frac{E\\left\\lbrack \\left( Y^{L} \\right)^{k} \\right\\rbrack}{{1 - F}_{X}\\left( d \\right)},\\] respectively. We have seen that the deductible \\(d\\) imposed on an insurance policy is the amount of loss that has to be paid out of pocket before the insurer makes any payment. The deductible \\(d\\) imposed on an insurance policy reduces the insurer’s payment. The loss elimination ratio (LER) is the percentage decrease in the expected payment of the insurer as a result of imposing the deductible. LER is defined as \\[LER = \\frac{E\\left( X \\right) - E\\left( Y^{L} \\right)}{E\\left( X \\right)}.\\] A little less common type of policy deductible is the franchise deductible. The Franchise deductible will apply to the policy in the same way as ordinary deductible except that when the loss exceeds the deductible \\(d\\), the full loss is covered by the insurer. The payment per loss and payment per payment variables are defined as \\[Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d, \\\\ X &amp; X &gt; d, \\\\ \\end{matrix} \\right.\\ \\] and \\[Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\leq d, \\\\ X &amp; X &gt; d, \\\\ \\end{matrix} \\right.\\ \\] respectively. Example 3.15 (SOA) A claim severity distribution is exponential with mean 1000. An insurance company will pay the amount of each claim in excess of a deductible of 100. Calculate the variance of the amount paid by the insurance company for one claim, including the possibility that the amount paid is 0. Solution Let \\(Y^{L}\\) denote the amount paid by the insurance company for one claim. \\[Y^{L} = \\left( X - 100 \\right)_{+} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq 100, \\\\ X - 100 &amp; X &gt; 100. \\\\ \\end{matrix} \\right.\\ \\] The first and second moments of \\(Y^{L}\\) are \\[E\\left( Y^{L} \\right) = \\int_{100}^{\\infty}\\left( x - 100 \\right)f_{X}\\left( x \\right)dx \\\\ = {\\int_{100}^{\\infty}{S_{X}\\left( x \\right)}dx = 1000e}^{- \\frac{100}{1000}},\\] and \\[E\\left\\lbrack \\left( Y^{L} \\right)^{2} \\right\\rbrack = \\int_{100}^{\\infty}\\left( x - 100 \\right)^{2}f_{X}\\left( x \\right)dx \\\\ = 2 \\times 1000^{2}e^{- \\frac{100}{1000}}.\\] \\[Var\\left( Y^{L} \\right) = \\left( 2 \\times 1000^{2}e^{- \\frac{100}{1000}} \\right) - \\left( {1000e}^{- \\frac{100}{1000}} \\right)^{2} = 990,944.\\] The solution can be simplified if we make use of the relationship between \\(X\\) and \\(Y^{P}\\). If \\(X\\) is exponentially distributed with mean 1000, then \\(Y^{P}\\) is also exponentially distributed with the same mean. Hence, \\(E\\left( Y^{P} \\right)\\)=1000 and \\[E\\left\\lbrack \\left( Y^{P} \\right)^{2} \\right\\rbrack = 2 \\times 1000^{2}.\\] Using the relationship between \\(Y^{L}\\) and \\(Y^{P}\\) we find \\[E\\left( Y^{L} \\right) = \\ E\\left( Y^{P} \\right)S_{X}\\left( 100 \\right){= 1000e}^{- \\frac{100}{1000}}\\] \\[E\\left\\lbrack \\left( Y^{L} \\right)^{2} \\right\\rbrack = E\\left\\lbrack \\left( Y^{P} \\right)^{2} \\right\\rbrack S_{X}\\left( 100 \\right) = 2 \\times 1000^{2}e^{- \\frac{100}{1000}}.\\] Example 3.16 (SOA) For an insurance: Losses have a density function \\[f_{X}\\left( x \\right) = \\left\\{ \\begin{matrix} 0.02x &amp; 0 &lt; x &lt; 10, \\\\ 0 &amp; \\text{elsewhere.} \\\\ \\end{matrix} \\right. \\] The insurance has an ordinary deductible of 4 per loss. \\(Y^{P}\\) is the claim payment per payment random variable. Calculate \\(E\\left( Y^{P} \\right)\\). Solution \\[Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\leq 4, \\\\ X - 4 &amp; X &gt; 4. \\\\ \\end{matrix} \\right.\\ \\] \\(E\\left( Y^{P} \\right) = \\frac{\\int_{4}^{10}\\left( x - 4 \\right)0.02xdx}{{1 - F}_{X}\\left( 4 \\right)} = \\frac{2.88}{0.84} = 3.43\\). Example 3.17 (SOA) You are given: Losses follow an exponential distribution with the same mean in all years. The loss elimination ratio this year is 70%. The ordinary deductible for the coming year is 4/3 of the current deductible. Compute the loss elimination ratio for the coming year. Solution The LER for the current year is \\[\\frac{E\\left( X \\right) - E\\left( Y^{L} \\right)}{E\\left( X \\right)} = \\frac{\\theta - \\theta e^{- d / \\theta}}{\\theta} = 1 - e^{- d / \\theta} = 0.7.\\] Then, \\(e^{- d / \\theta} = 0.3\\). The LER for the coming year is \\[ \\frac{\\theta - \\theta e^{- \\frac{\\left( \\frac{4}{3}d \\right)}{\\theta}}}{\\theta} = 1 - e^{- \\frac{\\left( \\frac{4}{3} d \\right)}{\\theta}} = 1 - \\left( e^{-d /\\theta} \\right)^{4/3} = 1 - {0.3}^{4/3} = 0.8 .\\] 3.4.2 Policy Limits Under a limited policy, the insurer is responsible for covering the actual loss \\(X\\) up to the limit of its coverage. This fixed limit of coverage is called the policy limit and often denoted by \\(u\\). If the loss exceeds the policy limit, the difference \\(X - u\\) has to be paid by the policyholder. While a higher policy limit means a higher payout to the insured, it is associated with a higher premium. Let \\(X\\) denote the loss incurred to the insured and \\(Y\\) denote the amount of paid claim by the insurer. Then \\(Y\\) is defined as \\[Y = X \\land u = \\left\\{ \\begin{matrix} X &amp; X \\leq u, \\\\ u &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] It can be seen that the distinction between \\(Y^{L}\\) and \\(Y^{P}\\) is not needed under limited policy as the insurer will always make a payment. Even when the distribution of \\(X\\) is continuous, the distribution of \\(Y\\) is partly discrete and partly continuous. The discrete part of the distribution is concentrated at \\(Y = u\\) (when \\(X &gt; u\\)), while the continuous part is spread over the interval \\(Y &lt; u\\) (when \\(X \\leq u\\)). For the discrete part, the probability that the benefit paid is \\(u\\), is the probability that the loss exceeds the policy limit \\(u\\); that is, \\[\\Pr \\left( Y = u \\right) = \\Pr \\left( X &gt; u \\right) = {1 - F}_{X}\\left( u \\right).\\] For the continuous part of the distribution \\(Y = X\\), hence the probability density function of \\(Y\\) is given by \\[f_{Y}\\left( y \\right) = \\left\\{ \\begin{matrix} f_{X}\\left( y \\right) &amp; 0 &lt; y &lt; u, \\\\ 1 - F_{X}\\left( u \\right) &amp; y = u. \\\\ \\end{matrix} \\right.\\ \\] Accordingly, the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\left\\{ \\begin{matrix} F_{X}\\left( x \\right) &amp; 0 &lt; y &lt; u, \\\\ 1 &amp; y \\geq u. \\\\ \\end{matrix} \\right.\\ \\] The raw moments of \\(Y\\) can be found directly using the probability density function of \\(X\\) as follows \\[E\\left( Y^{k} \\right) = E\\left\\lbrack \\left( X \\land u \\right)^{k} \\right\\rbrack = \\int_{0}^{u}x^{k}f_{X}\\left( x \\right)dx + \\int_{u}^{\\infty}{u^{k}f_{X}\\left( x \\right)} dx \\\\ \\int_{0}^{u}x^{k}f_{X}\\left( x \\right)dx + u^{k}\\left\\lbrack {1 - F}_{X}\\left( u \\right) \\right\\rbrack dx.\\] Example 3.18 (SOA) Under a group insurance policy, an insurer agrees to pay 100% of the medical bills incurred during the year by employees of a small company, up to a maximum total of one million dollars. The total amount of bills incurred, \\(X\\), has probability density function \\[f_{X}\\left( x \\right) = \\left\\{ \\begin{matrix} \\frac{x\\left( 4 - x \\right)}{9} &amp; 0 &lt; x &lt; 3, \\\\ 0 &amp; \\text{elsewhere.} \\\\ \\end{matrix} \\right.\\ \\] where \\(x\\) is measured in millions. Calculate the total amount, in millions of dollars, the insurer would expect to pay under this policy. Solution \\[Y = X \\land 1 = \\left\\{ \\begin{matrix} X &amp; X \\leq 1, \\\\ 1 &amp; X &gt; 1. \\\\ \\end{matrix} \\right.\\ \\] \\(E\\left( Y \\right) = E\\left( X \\land 1 \\right) = \\int_{0}^{1}\\frac{x^{2}(4 - x)}{9}dx + \\int_{1}^{3}\\frac{x\\left( 4 - x \\right)}{9}dx = 0.935\\). 3.4.3 Coinsurance As we have seen in Section 3.4.1, the amount of loss retained by the policyholder can be losses up to the deductible \\(d\\). The retained loss can also be a percentage of the claim. The percentage \\(\\alpha\\), often referred to as the coinsurance factor, is the percentage of claim the insurance company is required to cover. If the policy is subject to an ordinary deductible and policy limit, coinsurance refers to the percentage of claim the insurer is required to cover, after imposing the ordinary deductible and policy limit. The payment per loss variable, \\(Y^{L}\\), is defined as \\[Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d, \\\\ \\alpha\\left( X - d \\right) &amp; d &lt; X \\leq u, \\\\ \\alpha\\left( u - d \\right) &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] The policy limit (the maximum amount paid by the insurer) in this case is \\(\\alpha\\left( u - d \\right)\\), while \\(u\\) is the maximum covered loss. The \\(k\\)-th moment of \\(Y^{L}\\) is given by \\[E\\left\\lbrack \\left( Y^{L} \\right)^{k} \\right\\rbrack = \\int_{d}^{u}\\left\\lbrack \\alpha\\left( x - d \\right) \\right\\rbrack^{k}f_{X}\\left( x \\right)dx + \\int_{u}^{\\infty}\\left\\lbrack \\alpha\\left( u - d \\right) \\right\\rbrack^{k}f_{X}\\left( x \\right) dx .\\] A growth factor \\(\\left( 1 + r \\right)\\) may be applied to \\(X\\) resulting in an inflated loss random variable \\(\\left( 1 + r \\right)X\\) (the prespecified d and u remain unchanged). The resulting per loss variable can be written as \\[Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq \\frac{d}{1 + r}, \\\\ \\alpha\\left\\lbrack \\left( 1 + r \\right)X - d \\right\\rbrack &amp; \\frac{d}{1 + r} &lt; X \\leq \\frac{u}{1 + r}, \\\\ \\alpha\\left( u - d \\right) &amp; X &gt; \\frac{u}{1 + r}. \\\\ \\end{matrix} \\right.\\ \\] The first and second moments of \\(Y^{L}\\) can be expressed as \\[E\\left( Y^{L} \\right) = \\alpha\\left( 1 + r \\right)\\left\\lbrack E\\left( X \\land \\frac{u}{1 + r} \\right) - E\\left( X \\land \\frac{d}{1 + r} \\right) \\right\\rbrack,\\] and \\[E\\left\\lbrack \\left( Y^{L} \\right)^{2} \\right\\rbrack = \\alpha^{2}\\left( 1 + r \\right)^{2} \\left\\{ E\\left\\lbrack \\left( X \\land \\frac{u}{1 + r} \\right)^{2} \\right\\rbrack - E\\left\\lbrack \\left( X \\land \\frac{d}{1 + r} \\right)^{2} \\right\\rbrack \\right. \\\\ \\left. \\ \\ \\ \\ \\ - 2\\left( \\frac{d}{1 + r} \\right)\\left\\lbrack E\\left( X \\land \\frac{u}{1 + r} \\right) - E\\left( X \\land \\frac{d}{1 + r} \\right) \\right\\rbrack \\right\\} ,\\] respectively. The formulae given for the first and second moments of \\(Y^{L}\\) are general. Under full coverage, \\(\\alpha = 1\\), \\(r = 0\\), \\(u = \\infty\\), \\(d = 0\\) and \\(E\\left( Y^{L} \\right)\\) reduces to \\(E\\left( X \\right)\\). If only an ordinary deductible is imposed, \\(\\alpha = 1\\), \\(r = 0\\), \\(u = \\infty\\) and \\(E\\left( Y^{L} \\right)\\) reduces to \\(E\\left( X \\right) - E\\left( X \\land d \\right)\\). If only a policy limit is imposed \\(\\alpha = 1\\), \\(r = 0\\), \\(d = 0\\) and \\(E\\left( Y^{L} \\right)\\) reduces to \\(E\\left( X \\land u \\right)\\). Example 3.19 (SOA) The ground up loss random variable for a health insurance policy in 2006 is modeled with X, an exponential distribution with mean 1000. An insurance policy pays the loss above an ordinary deductible of 100, with a maximum annual payment of 500. The ground up loss random variable is expected to be 5% larger in 2007, but the insurance in 2007 has the same deductible and maximum payment as in 2006. Find the percentage increase in the expected cost per payment from 2006 to 2007. Solution \\[Y_{2006}^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq 100, \\\\ X - 100 &amp; 100 &lt; X \\leq 600, \\\\ 500 &amp; X &gt; 600. \\\\ \\end{matrix} \\right.\\ \\] \\[Y_{2007}^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq 95.24, \\\\ 1.05X - 100 &amp; 95.24 &lt; X \\leq 571.43, \\\\ 500 &amp; X &gt; 571.43. \\\\ \\end{matrix} \\right.\\ \\] \\[E\\left( Y_{2006}^{L} \\right) = E\\left( X \\land 600 \\right) - E\\left( X \\land 100 \\right) = 1000\\left( {1 - e}^{- \\frac{600}{1000}} \\right) - 1000\\left( {1 - e}^{- \\frac{100}{1000}} \\right)\\] \\(= 356.026\\). \\[E\\left( Y_{2007}^{L} \\right) = 1.05\\left\\lbrack E\\left( X \\land 571.43 \\right) - E\\left( X \\land 95.24 \\right) \\right\\rbrack\\] \\(= 1.05\\left\\lbrack 1000\\left( {1 - e}^{- \\frac{571.43}{1000}} \\right) - 1000\\left( {1 - e}^{- \\frac{95.24}{1000}} \\right) \\right\\rbrack\\) \\(\\mathbf{=}361.659\\). \\(E\\left( Y_{2006}^{P} \\right) = \\frac{356.026}{e^{- \\frac{100}{1000}} = 393.469}\\). \\(E\\left( Y_{2007}^{P} \\right) = \\frac{361.659}{e^{- \\frac{95.24}{1000}} = 397.797}\\). There is an increase of 1.1% from 2006 to 2007. 3.4.4 Reinsurance In Section 3.4.1 we introduced the policy deductible, which is a contractual arrangement under which an insured transfers part of the risk by securing coverage from an insurer in return for an insurance premium. Under that policy, when the loss exceeds the deductible, the insurer is not required to pay until the insured has paid the fixed deductible. We now introduce reinsurance, a mechanism of insurance for insurance companies. Reinsurance is a contractual arrangement under which an insurer transfers part of the underlying insured risk by securing coverage from another insurer (referred to as a reinsurer) in return for a reinsurance premium. Although reinsurance involves a relationship between three parties: the original insured, the insurer (often referred to as cedent or cedant) and the reinsurer, the parties of the reinsurance agreement are only the primary insurer and the reinsurer. There is no contractual agreement between the original insured and the reinsurer. The reinsurer is not required to pay under the reinsurance contract until the insurer has paid a loss to its original insured. The amount retained by the primary insurer in the reinsurance agreement (the reinsurance deductible) is called retention. Reinsurance arrangements allow insurers with limited financial resources to increase the capacity to write insurance and meet client requests for larger insurance coverage while reducing the impact of potential losses and protecting the insurance company against catastrophic losses. Reinsurance also allows the primary insurer to benefit from underwriting skills, expertize and proficient complex claim file handling of the larger reinsurance companies. Example 3.20 (SOA) In 2005 a risk has a two-parameter Pareto distribution with \\(\\alpha = 2\\) and \\(\\theta = 3000\\). In 2006 losses inflate by 20%. Insurance on the risk has a deductible of 600 in each year. \\(P_{i}\\), the premium in year \\(i\\), equals 1.2 times expected claims. The risk is reinsured with a deductible that stays the same in each year. \\(R_{i}\\), the reinsurance premium in year \\(i\\), equals 1.1 times the expected reinsured claims. \\(\\frac{R_{2005}}{P_{2005} = 0.55}\\). Calculate \\(\\frac{R_{2006}}{P_{2006}}\\). Solution Let us use the following notation: \\(X_{i}:\\) The risk in year \\(i\\) \\(Y_{i}:\\) The insured claim in year \\(i\\) \\(P_{i}:\\) The insurance premium in year \\(i\\) \\(Y_{i}^{R}:\\) The reinsured claim in year \\(i\\) \\(R_{i}:\\) The reinsurance premium in year \\(i\\) \\(d:\\) The insurance deductible in year \\(i\\) (the insurance deductible is fixed each year, equal to 600) \\(d^{R}:\\) The reinsurance deductible or retention in year \\(i\\) (the reinsurance deductible is fixed each year, but unknown) where \\(i = 2005,\\ 2006\\) \\[Y_{i} = \\left\\{ \\begin{matrix} 0 &amp; X_{i} \\leq 600 \\\\ X_{i} - 600 &amp; X_{i} &gt; 600 \\\\ \\end{matrix} \\right.\\ \\] where \\(i = 2005,\\ 2006\\) \\[X_{2005}\\sim Pa\\left( 2,3000 \\right)\\] \\[E\\left( Y_{2005} \\right) = E\\left( X_{2005} - 600 \\right)_{+} = E\\left( X_{2005} \\right) - E\\left( X_{2005} \\land 600 \\right)\\] \\(= 3000 - 3000\\left( 1 - \\frac{3000}{3600} \\right) = 2500\\) \\[P_{2005} = 1.2E\\left( Y_{2005} \\right) = 3000\\] Since \\(X_{2006} = 1.2X_{2005}\\) and Pareto is a scale distribution with scale parameter \\(\\theta\\), then \\(X_{2006}\\sim Pa\\left( 2,3600 \\right)\\) \\[E\\left( Y_{2006} \\right) = E\\left( X_{2006} - 600 \\right)_{+} = E\\left( X_{2006} \\right) - E\\left( X_{2006} \\land 600 \\right)\\] \\(= 3600 - 3600\\left( 1 - \\frac{3600}{4200} \\right) = 3085.714\\) \\[P_{2006} = 1.2E\\left( Y_{2006} \\right) = 3702.857\\] \\[Y_{i}^{R} = \\left\\{ \\begin{matrix} 0 &amp; X_{i} - 600 \\leq d^{R} \\\\ X_{i} - 600 - d^{R} &amp; X_{i} - 600 &gt; d^{R} \\\\ \\end{matrix} \\right.\\ \\] Since \\(\\frac{R_{2005}}{P_{2005}} = 0.55\\), then \\(R_{2005} = 3000 \\times 0.55 = 1650\\) Since \\(R_{2005} = 1.1E\\left( Y_{2005}^{R} \\right)\\), then \\(E\\left( Y_{2005}^{R} \\right) = \\frac{1650}{1.1} = 1500\\) \\[E\\left( Y_{2005}^{R} \\right) = E\\left( X_{2005} - 600 - d^{R} \\right)_{+} = E\\left( X_{2005} \\right) - E\\left( X_{2005} \\land \\left( 600 + d^{R} \\right) \\right)\\] \\(= 3000 - 3000\\left( 1 - \\frac{3000}{3600 + d^{R}} \\right) = 1500 \\Rightarrow d^{R} = 2400\\) \\[E\\left( Y_{2006}^{R} \\right) = E\\left( X_{2006} - 600 - d^{R} \\right)_{+} = E\\left( X_{2006} - 3000 \\right)_{+} = E\\left( X_{2006} \\right) - E\\left( X_{2006} \\land 3000 \\right)\\] \\(= 3600 - 3600\\left( 1 - \\frac{3600}{6600} \\right) = 1963.636\\) \\[R_{2006} = 1.1E\\left( Y_{2006}^{R} \\right) = 1.1 \\times 1963.636 = 2160\\] Therefore \\(\\frac{R_{2006}}{P_{2006}} = \\frac{2160}{3702.857} = 0.583\\) 3.5 Maximum Likelihood Estimation In this section we estimate statistical parameters using the method of maximum likelihood. Maximum likelihood estimates in the presence of grouping, truncation or censoring are calculated. 3.5.1 Maximum Likelihood Estimators for Complete Data Pricing of insurance premiums and estimation of claim reserving are among many actuarial problems that involve modeling the severity of loss (claim size). The principles for using maximum likelihood to estimate model parameters were introduced in Chapter xxx. In this section, we present a few examples to illustrate how actuaries fit a parametric distribution model to a set of claim data using maximum likelihood. In these examples we derive the asymptotic variance of maximum-likelihood estimators of the model parameters. We use the delta method to derive the asymptotic variances of functions of these parameters. Example 3.21 Consider a random sample of claim amounts: 8,000 10,000 12,000 15,000. You assume that claim amounts follow an inverse exponential distribution, with parameter \\(\\theta\\). Calculate the maximum likelihood estimator for \\(\\theta\\). Approximate the variance of the maximum likelihood estimator. Determine an approximate 95% confidence interval for \\(\\theta\\). Determine an approximate 95% confidence interval for \\(\\Pr \\left( X \\leq 9,000 \\right).\\) Solution The probability density function is \\[f_{X}\\left( x \\right) = \\frac{\\theta e^{- \\frac{\\theta}{x}}}{x^{2}}, \\] where \\(x &gt; 0\\). The likelihood function, \\(L\\left( \\theta \\right)\\), can be viewed as the probability of the observed data, written as a function of the model’s parameter \\(\\theta\\) \\[L\\left( \\theta \\right) = \\prod_{i = 1}^{4}{f_{X_{i}}\\left( x_{i} \\right)} = \\frac{\\theta^{4}e^{- \\theta\\sum_{i = 1}^{4}\\frac{1}{x_{i}}}}{\\prod_{i = 1}^{4}x_{i}^{2}}.\\] The loglikelihood function, \\(\\ln L \\left( \\theta \\right)\\), is the sum of the individual logarithms. \\[\\ln L \\left( \\theta \\right) = 4ln\\theta - \\theta\\sum_{i = 1}^{4}\\frac{1}{x_{i}} - 2\\sum_{i = 1}^{4}\\ln x_{i} .\\] \\[\\frac{d \\ln L \\left( \\theta \\right)}{d \\theta} = \\frac{4}{\\theta} - \\sum_{i = 1}^{4}\\frac{1}{x_{i}}.\\] The maximum likelihood estimator of \\(\\theta\\), denoted by \\(\\hat{\\theta}\\), is the solution to the equation \\[\\frac{4}{\\hat{\\theta}} - \\sum_{i = 1}^{4}{\\frac{1}{x_{i}} = 0}.\\] Thus, \\(\\hat{\\theta} = \\frac{4}{\\sum_{i = 1}^{4}\\frac{1}{x_{i}}} = 10,667\\) The second derivative of \\(\\ln L \\left( \\theta \\right)\\) is given by \\[\\frac{d^{2}\\ln L\\left( \\theta \\right)}{d\\theta^{2}} = \\frac{- 4}{\\theta^{2}}.\\] Evaluating the second derivative of the loglikelihood function at \\(\\hat{\\theta} = 10,667\\) gives a negative value, indicating \\(\\hat{\\theta}\\) as the value that maximizes the loglikelihood function. Taking reciprocal of negative expectation of the second derivative of \\(\\ln L \\left( \\theta \\right)\\), we obtain an estimate of the variance of \\(\\hat{\\theta}\\) \\(\\widehat{Var}\\left( \\hat{\\theta} \\right) = \\left. \\ \\left\\lbrack E\\left( \\frac{d^{2}\\ln L \\left( \\theta \\right)}{d\\theta^{2}} \\right) \\right\\rbrack^{- 1} \\right|_{\\theta = \\hat{\\theta}} = \\frac{{\\hat{\\theta}}^{2}}{4} = 28,446,222\\). It should be noted that as the sample size \\(n \\rightarrow \\infty\\), the distribution of the maximum likelihood estimator \\(\\hat{\\theta}\\) converges to a normal distribution with mean \\(\\theta\\) and variance \\(\\hat{V}\\left( \\hat{\\theta} \\right)\\). The approximate confidence interval in this example is based on the assumption of normality, despite the small sample size, only for the purpose of illustration. The 95% confidence interval for \\(\\theta\\) is given by \\[10,667 \\pm 1.96\\sqrt{28,446,222} = \\left( 213.34,\\ 21,120.66 \\right).\\] The distribution function of \\(X\\) is \\(F\\left( x \\right) = 1 - e^{- \\frac{x}{\\theta}}\\). Then, the maximum likelihood estimate of \\(g\\left( \\theta \\right) = F\\left( 9,000 \\right)\\) is \\[g\\left( \\hat{\\theta} \\right) = 1 - e^{- \\frac{9,000}{10,667}} = 0.57.\\] We use the delta method to approximate the variance of \\(g\\left( \\hat{\\theta} \\right)\\). \\[\\frac{\\text{dg}\\left( \\theta \\right)}{d \\theta} = {- \\frac{9,000}{\\theta^{2}}e}^{- \\frac{9,000}{\\theta}}.\\] \\(\\widehat{Var}\\left\\lbrack g\\left( \\hat{\\theta} \\right) \\right\\rbrack = \\left( - {\\frac{9,000}{{\\hat{\\theta}}^{2}}e}^{- \\frac{9,000}{\\hat{\\theta}}} \\right)^{2}\\hat{V}\\left( \\hat{\\theta} \\right) = 0.0329\\). The 95% confidence interval for \\(F\\left( 9,000 \\right)\\) is given by \\[0.57 \\pm 1.96\\sqrt{0.0329} = \\left( 0.214,\\ 0.926 \\right).\\] Example 3.22 A random sample of size 6 is from a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). The sample values are 200, 3,000, 8,000, 60,000, 60,000, 160,000. Calculate the maximum likelihood estimator for \\(\\mu\\) and \\(\\sigma\\). Estimate the covariance matrix of the maximum likelihood estimator. Determine approximate 95% confidence intervals for \\(\\mu\\) and \\(\\sigma\\). Determine an approximate 95% confidence interval for the mean of the lognormal distribution. Solution The probability density function is \\[f_{X}\\left( x \\right) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp - \\frac{1}{2}\\left( \\frac{\\ln x - \\mu}{\\sigma} \\right)^{2},\\] where \\(x &gt; 0\\). The likelihood function, \\(L\\left( \\mu,\\sigma \\right)\\), is the product of the pdf for each data point. \\[L\\left( \\mu,\\sigma \\right) = \\prod_{i = 1}^{6}{f_{X_{i}}\\left( x_{i} \\right)} = \\frac{1}{\\sigma^{6}\\left( 2\\pi \\right)^{3}\\prod_{i = 1}^{6}x_{i}}exp - \\frac{1}{2}\\sum_{i = 1}^{6}\\left( \\frac{\\ln x_{i} - \\mu}{\\sigma} \\right)^{2}.\\] The loglikelihood function, \\(\\ln L \\left( \\mu,\\sigma \\right)\\), is the sum of the individual logarithms. \\[\\ln \\left( \\mu,\\sigma \\right) = - 6ln\\sigma - 3ln\\left( 2\\pi \\right) - \\sum_{i = 1}^{6}\\ln x_{i} - \\frac{1}{2}\\sum_{i = 1}^{6}\\left( \\frac{\\ln x_{i} - \\mu}{\\sigma} \\right)^{2}.\\] The first partial derivatives are \\[\\frac{\\partial lnL\\left( \\mu,\\sigma \\right)}{\\partial\\mu} = \\frac{1}{\\sigma^{2}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right).\\] \\[\\frac{\\partial lnL\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} = \\frac{- 6}{\\sigma} + \\frac{1}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right)^{2}.\\] The maximum likelihood estimators of \\(\\mu\\) and \\(\\sigma\\), denoted by \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\), are the solutions to the equations \\[\\frac{1}{{\\hat{\\sigma}}^{2}}\\sum_{i = 1}^{6}\\left( lnx_{i} - \\hat{\\mu} \\right) = 0.\\] \\[\\frac{- 6}{\\hat{\\sigma}} + \\frac{1}{{\\hat{\\sigma}}^{3}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\hat{\\mu} \\right)^{2} = 0.\\] These yield the estimates \\(\\hat{\\mu} = \\frac{\\sum_{i = 1}^{6}{\\ln x_{i}}}{6} = 9.38\\) and \\({\\hat{\\sigma}}^{2} = \\frac{\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\hat{\\mu} \\right)^{2}}{6} = 5.12\\). The second partial derivatives are \\(\\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu^{2}} = \\frac{- 6}{\\sigma^{2}}\\), \\(\\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu\\partial\\sigma} = \\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right)\\) and \\(\\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\sigma^{2}} = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right)^{2}\\). To derive the covariance matrix of the mle we need to find the expectations of the second derivatives. Since the random variable \\(X\\) is from a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), then \\(\\text{lnX}\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). \\(E\\left( \\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu^{2}} \\right) = E\\left( \\frac{- 6}{\\sigma^{2}} \\right) = \\frac{- 6}{\\sigma^{2}}\\), \\(E\\left( \\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu\\partial\\sigma} \\right) = \\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}{E\\left( \\ln x_{i} - \\mu \\right)} = \\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left\\lbrack E\\left( \\ln x_{i} \\right) - \\mu \\right\\rbrack\\)=\\(\\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left( \\mu - \\mu \\right) = 0\\), and \\(E\\left( \\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\sigma^{2}} \\right) = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}{E\\left( \\ln x_{i} - \\mu \\right)}^{2} = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}{V\\left( \\ln x_{i} \\right) = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}{\\sigma^{2} = \\frac{- 12}{\\sigma^{2}}}}\\). Using the negatives of these expectations we obtain the Fisher information matrix \\[\\begin{bmatrix} \\frac{6}{\\sigma^{2}} &amp; 0 \\\\ 0 &amp; \\frac{12}{\\sigma^{2}} \\\\ \\end{bmatrix}\\]. The covariance matrix, \\(\\Sigma\\), is the inverse of the Fisher information matrix \\[\\Sigma = \\begin{bmatrix} \\frac{\\sigma^{2}}{6} &amp; 0 \\\\ 0 &amp; \\frac{\\sigma^{2}}{12} \\\\ \\end{bmatrix}\\]. The estimated matrix is given by \\[\\hat{\\Sigma} = \\begin{bmatrix} 0.8533 &amp; 0 \\\\ 0 &amp; 0.4267 \\\\ \\end{bmatrix}\\]. The 95% confidence interval for \\(\\mu\\) is given by \\(9.38 \\pm 1.96\\sqrt{0.8533} = \\left( 7.57,\\ 11.19 \\right)\\). The 95% confidence interval for \\(\\sigma^{2}\\) is given by \\(5.12 \\pm 1.96\\sqrt{0.4267} = \\left( 3.84,\\ 6.40 \\right)\\). The mean of X is \\(\\exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\). Then, the maximum likelihood estimate of \\[g\\left( \\mu,\\sigma \\right) = \\exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\] is \\[g\\left( \\hat{\\mu},\\hat{\\sigma} \\right) = \\exp\\left( \\hat{\\mu} + \\frac{{\\hat{\\sigma}}^{2}}{2} \\right) = 153,277.\\] We use the delta method to approximate the variance of the mle \\(g\\left( \\hat{\\mu},\\hat{\\sigma} \\right)\\). \\(\\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\mu} = exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\) and \\(\\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} = \\sigma exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\). Using the delta method, the approximate variance of \\(g\\left( \\hat{\\mu},\\hat{\\sigma} \\right)\\) is given by \\[\\left. \\ \\hat{V}\\left( g\\left( \\hat{\\mu},\\hat{\\sigma} \\right) \\right) = \\begin{bmatrix} \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\mu} &amp; \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} \\\\ \\end{bmatrix}\\Sigma\\begin{bmatrix} \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\mu} \\\\ \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} \\\\ \\end{bmatrix} \\right|_{\\mu = \\hat{\\mu},\\sigma = \\hat{\\sigma}}\\] \\[= \\begin{bmatrix} 153,277 &amp; 346,826 \\\\ \\end{bmatrix}\\begin{bmatrix} 0.8533 &amp; 0 \\\\ 0 &amp; 0.4267 \\\\ \\end{bmatrix}\\begin{bmatrix} 153,277 \\\\ 346,826 \\\\ \\end{bmatrix} =\\]71,374,380,000 The 95% confidence interval for \\(\\exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\) is given by \\(153,277 \\pm 1.96\\sqrt{71,374,380,000} = \\left( - 370,356,\\ 676,910 \\right)\\). Since the mean of the lognormal distribution cannot be negative, we should replace the negative lower limit in the previous interval by a zero. 3.5.2 Maximum Likelihood Estimators for Grouped Data In the previous section we considered the maximum likelihood estimation of continuous models from complete (individual) data. Each individual observation is recorded, and its contribution to the likelihood function is the density at that value. In this section we consider the problem of obtaining maximum likelihood estimates of parameters from grouped data. The observations are only available in grouped form, and the contribution of each observation to the likelihood function is the probability of falling in a specific group (interval). Let \\(n_{j}\\) represent the number of observations in the interval \\(\\left( \\left. \\ c_{j - 1},c_{j} \\right\\rbrack \\right.\\ \\) The grouped data likelihood function is thus given by \\[L\\left( \\theta \\right) = \\prod_{j = 1}^{k}\\left\\lbrack F\\left( \\left. \\ c_{j} \\right|\\theta \\right) - F\\left( \\left. \\ c_{j - 1} \\right|\\theta \\right) \\right\\rbrack^{n_{j}},\\] where \\(c_{0}\\) is the smallest possible observation (often set to zero) and \\(c_{k}\\) is the largest possible observation (often set to infinity). Example 3.23 (SOA) For a group of policies, you are given that losses follow the distribution function \\(F\\left( x \\right) = 1 - \\frac{\\theta}{x}\\), for \\(\\theta &lt; x &lt; \\infty.\\) Further, a sample of 20 losses resulted in the following: \\[ {\\small \\begin{matrix}\\hline \\text{Interval} &amp; \\text{Number of Losses} \\\\ \\hline (\\theta, 10] &amp; 9 \\\\ (10, 25] &amp; 6 \\\\ (25, \\infty) &amp; 5 \\\\ \\hline \\end{matrix} } \\] Calculate the maximum likelihood estimate of \\(\\theta\\). Solution The contribution of each of the 9 observations in the first interval to the likelihood function is the probability of \\(X \\leq 10\\); that is, \\(\\Pr\\left( X \\leq 10 \\right) = F\\left( 10 \\right)\\). Similarly, the contributions of each of 6 and 5 observations in the second and third intervals are \\(\\Pr\\left( 10 &lt; X \\leq 25 \\right) = F\\left( 25 \\right) - F(10)\\) and \\(P\\left( X &gt; 25 \\right) = 1 - F(25)\\), respectively. The likelihood function is thus given by \\[L\\left( \\theta \\right) = \\left\\lbrack F\\left( 10 \\right) \\right\\rbrack^{9}\\left\\lbrack F\\left( 25 \\right) - F(10) \\right\\rbrack^{6}\\left\\lbrack 1 - F(25) \\right\\rbrack^{5}\\] \\[{= \\left( 1 - \\frac{\\theta}{10} \\right)}^{9}\\left( \\frac{\\theta}{10} - \\frac{\\theta}{25} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5}\\] \\[{= \\left( \\frac{10 - \\theta}{10} \\right)}^{9}\\left( \\frac{15\\theta}{250} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5}.\\] Then, \\(\\ln L \\left( \\theta \\right) = 9ln\\left( 10 - \\theta \\right) + 6ln\\theta + 5ln\\theta - 9ln10 + 6ln15 - 6ln250 - 5ln25\\). \\[\\frac{d \\ln L \\left( \\theta \\right)}{d \\theta} = \\frac{- 9}{\\left( 10 - \\theta \\right)} + \\frac{6}{\\theta} + \\frac{5}{\\theta}.\\] The maximum likelihood estimator, \\(\\hat{\\theta}\\), is the solution to the equation \\[\\frac{- 9}{\\left( 10 - \\hat{\\theta} \\right)} + \\frac{11}{\\hat{\\theta}} = 0\\] and \\(\\hat{\\theta} = 5.5\\). 3.5.3 Maximum Likelihood Estimators for Censored Data Another distinguishing feature of data gathering mechanism is censoring. While for some event of interest (losses, claims, lifetimes, etc.) the complete data maybe available, for others only partial information is available; information that the observation exceeds a specific value. The limited policy introduced in Section 3.4.2 is an example of right censoring. Any loss greater than or equal to the policy limit is recorded at the limit. The contribution of the censored observation to the likelihood function is the probability of the random variable exceeding this specific limit. Note that contributions of both complete and censored data share the survivor function, for a complete point this survivor function is multiplied by the hazard function, but for a censored observation it is not. Example 3.24 (SOA) The random variable has survival function: \\[S_{X}\\left( x \\right) = \\frac{\\theta^{4}}{\\left( \\theta^{2} + x^{2} \\right)^{2}}.\\] Two values of \\(X\\) are observed to be 2 and 4. One other value exceeds 4. Calculate the maximum likelihood estimate of \\(\\theta\\). Solution The contributions of the two observations 2 and 4 are \\(f_{X}\\left( 2 \\right)\\) and \\(f_{X}\\left( 4 \\right)\\) respectively. The contribution of the third observation, which is only known to exceed 4 is \\(S_{X}\\left( 4 \\right)\\). The likelihood function is thus given by \\[L\\left( \\theta \\right) = f_{X}\\left( 2 \\right)f_{X}\\left( 4 \\right)S_{X}\\left( 4 \\right).\\] The probability density function of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\frac{4x\\theta^{4}}{\\left( \\theta^{2} + x^{2} \\right)^{3}}.\\] Thus, \\[L\\left( \\theta \\right) = \\frac{8\\theta^{4}}{\\left( \\theta^{2} + 4 \\right)^{3}}\\frac{16\\theta^{4}}{\\left( \\theta^{2} + 16 \\right)^{3}}\\frac{\\theta^{4}}{\\left( \\theta^{2} + 16 \\right)^{2}} = \\\\ \\frac{128\\theta^{12}}{\\left( \\theta^{2} + 4 \\right)^{3}\\left( \\theta^{2} + 16 \\right)^{5}},\\] \\(\\ln L\\left( \\theta \\right) = ln128 + 12ln\\theta - 3ln\\left( \\theta^{2} + 4 \\right) - 5ln\\left( \\theta^{2} + 16 \\right)\\), and \\(\\frac{\\text{dlnL}\\left( \\theta \\right)}{d \\theta} = \\frac{12}{\\theta} - \\frac{6\\theta}{\\left( \\theta^{2} + 4 \\right)} - \\frac{10\\theta}{\\left( \\theta^{2} + 16 \\right)}\\). The maximum likelihood estimator, \\(\\hat{\\theta}\\), is the solution to the equation \\[\\frac{12}{\\hat{\\theta}} - \\frac{6\\hat{\\theta}}{\\left( {\\hat{\\theta}}^{2} + 4 \\right)} - \\frac{10\\hat{\\theta}}{\\left( {\\hat{\\theta}}^{2} + 16 \\right)} = 0\\] or \\[12\\left( {\\hat{\\theta}}^{2} + 4 \\right)\\left( {\\hat{\\theta}}^{2} + 16 \\right) - 6{\\hat{\\theta}}^{2}\\left( {\\hat{\\theta}}^{2} + 16 \\right) - 10{\\hat{\\theta}}^{2}\\left( {\\hat{\\theta}}^{2} + 4 \\right) = \\\\ - 4{\\hat{\\theta}}^{4} + 104{\\hat{\\theta}}^{2} + 768 = 0,\\] which yields \\({\\hat{\\theta}}^{2} = 32\\) and \\(\\hat{\\theta} = 5.7\\). 3.5.4 Maximum Likelihood Estimators for Truncated Data This section is concerned with the maximum likelihood estimation of the continuous distribution of the random variable \\(X\\) when the data is incomplete due to truncation. If the values of \\(X\\) are truncated at \\(d\\), then it should be noted that we would not have been aware of the existence of these values had they not exceeded \\(d\\). The policy deductible introduced in Section 3.4.1 is an example of left truncation. Any loss less than or equal to the deductible is not recorded. The contribution to the likelihood function of an observation \\(x\\) truncated at \\(d\\) will be a conditional probability and the \\(f_{X}\\left( x \\right)\\) will be replaced by \\(\\frac{f_{X}\\left( x \\right)}{S_{X}\\left( d \\right)}\\). Example 3.25 (SOA) For the single parameter Pareto distribution with \\(\\theta = 2\\), maximum likelihood estimation is applied to estimate the parameter \\(\\alpha\\). Find the estimated mean of the ground up loss distribution based on the maximum likelihood estimate of \\(\\alpha\\) for the following data set: Ordinary policy deductible of 5, maximum covered loss of 25 (policy limit 20) 8 insurance payment amounts: 2, 4, 5, 5, 8, 10, 12, 15 2 limit payments: 20, 20. Solution The contributions of the different observations can be summarized as follows: For the exact loss: \\(f_{X}\\left( x \\right)\\) For censored observations: \\(S_{X}\\left( 25 \\right)\\). For truncated observations: \\(\\frac{f_{X}\\left( x \\right)}{S_{X}\\left( 5 \\right)}\\). Given that ground up losses smaller than 5 are omitted from the data set, the contribution of all observations should be conditional on exceeding 5. The likelihood function becomes \\[L\\left( \\alpha \\right) = \\frac{\\prod_{i = 1}^{8}{f_{X}\\left( x_{i} \\right)}}{\\left\\lbrack S_{X}\\left( 5 \\right) \\right\\rbrack^{8}}\\left\\lbrack \\frac{S_{X}\\left( 25 \\right)}{S_{X}\\left( 5 \\right)} \\right\\rbrack^{2}.\\] For the single parameter Pareto the probability density and distribution functions are given by \\[f_{X}\\left( x \\right) = \\frac{\\alpha\\theta^{\\alpha}}{x^{\\alpha + 1}} \\ \\ \\text{and} \\ \\ F_{X}\\left( x \\right) = 1 - \\left( \\frac{\\theta}{x} \\right)^{\\alpha},\\] for \\(x &gt; \\theta\\), respectively. Then, the likelihood and loglikelihood functions are given by \\[L\\left( \\alpha \\right) = \\frac{\\alpha^{8}}{\\prod_{i = 1}^{8}x_{i}^{\\alpha + 1}}\\frac{5^{10\\alpha}}{25^{2\\alpha}},\\] \\[\\ln L \\left( \\alpha \\right) = 8ln\\alpha - \\left( \\alpha + 1 \\right)\\sum_{i = 1}^{8}{\\ln x_{i}} + 10\\alpha ln5 - 2\\alpha ln25.\\] \\(\\frac{\\text{dlnL}\\left( \\alpha \\right)}{d \\theta} = \\frac{8}{\\alpha} - \\sum_{i = 1}^{8}{\\ln x_{i}} + 10ln5 - 2ln25\\). The maximum likelihood estimator, \\(\\hat{\\alpha}\\), is the solution to the equation \\[\\frac{8}{\\hat{\\alpha}} - \\sum_{i = 1}^{8}{\\ln x_{i}} + 10ln5 - 2ln25 = 0,\\]which yields \\[\\hat{\\alpha} = \\frac{8}{\\sum_{i = 1}^{8}{\\ln x_{i}} - 10ln5 + 2ln25} = \\frac{8}{(ln7 + ln9 + \\ldots + ln20) - 10ln5 + 2ln25} = 0.785.\\] The mean of the Pareto only exists for \\(\\alpha &gt; 1\\). Since \\(\\hat{\\alpha} = 0.785 &lt; 1\\). Then, the mean does not exist. 3.6 Further Resources and Contributors In describing losses, actuaries fit appropriate parametric distribution models for the frequency and severity of loss. This involves finding appropriate statistical distributions that could efficiently model the data in hand. After fitting a distribution model to a data set, the model should be validated. Model validation is a crucial step in the model building sequence. It assesses how well these statistical distributions fit the data in hand and how well can we expect this model to perform in the future. If the selected model does not fit the data, another distribution is to be chosen. If more than one model seems to be a good fit for the data, we then have to make the choice on which model to use. It should be noted though that the same data should not serve for both purposes (fitting and validating the model). Additional data should be used to assess the performance of the model. There are many statistical tools for model validation. Alternative goodness of fit tests used to determine whether sample data are consistent with the candidate model, will be presented in a separate chapter. Further Readings and References Cummins, J. D. and Derrig, R. A. 1991. Managing the Insolvency Risk of Insurance Companies, Springer Science+ Business Media, LLC. Frees, E. W. and Valdez, E. A. 2008. Hierarchical insurance claims modeling, Journal of the American Statistical Association, 103, 1457-1469. Klugman, S. A., Panjer, H. H. and Willmot, G. E. 2008. Loss Models from Data to Decisions, Wiley. Kreer, M., Kizilers, A., Thomas, A. W. and Eg?dio dos Reis, A. D. 2015. Goodness-of-fit tests and applications for left-truncated Weibull distributions to non-life insurance, European Actuarial Journal, 5, 139-163. McDonald, J. B. 1984. Some generalized functions for the size distribution of income, Econometrica 52, 647-663. McDonald, J. B. and Xu, Y. J. 1995. A generalization of the beta distribution with applications, Journal of Econometrics 66, 133-52. Tevet, D. 2016. Applying generalized linear models to insurance data: Frequency/severity versus premium modeling in: Frees, E. W., Derrig, A. R. and Meyers G. (Eds.) Predictive Modeling Applications in Actuarial Science Vol. II Case Studies in Insurance. Cambridge University Press. Venter, G. 1983. Transformed beta and gamma distributions and aggregate losses. Proceedings of the Casualty Actuarial Society 70: 156-193. Contributors Zeinab Amin, The American University in Cairo, is the principal author of this chapter. Date: October 27, 2016. Email: zeinabha@aucegypt.edu for chapter comments and suggested improvements. Many helpful comments have been provided by Hirokazu (Iwahiro) Iwasawa, iwahiro@bb.mbn.or.jp . 3.7 Exercises Here are a set of exercises that guide the viewer through some of the theoretical foundations of Loss Data Analytics. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C. knitr::include_url(&quot;http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/chapter-3-modeling-loss-severity/loss-data-analytics-severity-problems/&quot;,height = &quot;600px&quot;) Bibliography "],
["model-selection-and-inference.html", "Chapter 4 Model Selection and Inference 4.1 Nonparametric Estimation Tools 4.2 Nonparametric Estimation Tools For Model Selection 4.3 Nonparametric Estimation using Modified Data 4.4 Topics in Parametric Estimation 4.5 Bayesian Inference 4.6 Exercises", " Chapter 4 Model Selection and Inference 4.1 Nonparametric Estimation Tools 4.1.1 Moments 4.1.1.1 Moment Estimators \\(X_1, \\ldots, X_n\\) is a random sample (with replacement) from F(.) Sometimes we say that \\(X_1, \\ldots, X_n\\) are identically and independently distributed (\\(iid\\)) We will not assume a parametric form for the distribution function F() and so proceed with a nonparametric analysis. The \\(k\\)th (raw) moment is \\(\\mathrm{E~} X^k = \\mu^{\\prime}_k\\) . It is estimated by the corresponding statistic \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k .\\] The \\(k\\)th (central) moment is \\(\\mathrm{E~} (X-\\mu)^k = \\mu_k\\). It is estimated by \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i-\\bar{X}\\right)^k .\\] 4.1.1.2 Empirical Distribution Function Define the empirical distribution function to be \\[\\begin{aligned} F_n(x) &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right).\\end{aligned}\\] Here, the notation \\(I(\\cdot)\\) is the indicator function, it returns 1 if the event \\((\\cdot)\\) is true and 0 otherwise. Example – Toy. Consider \\(n=10\\) observations as in Figure 4.1 \\[\\begin{equation*} \\begin{array}{l|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i &amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array}\\end{equation*}\\] \\(\\bar{x} = 19.7\\) and that the estimate of the second central moment, the sample variance, is 34.45556. ## [1] 10 15 15 15 20 23 23 23 23 30 Figure 4.1: Empirical Distribution Function of a Toy Example R Code for Toy Example CDF (xExample = c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.2 Quantiles Special Cases The median is that point so that approximately half of a data set is below (or above) it. The first quartile is that number so that approximately 25% of the data is below it. A \\(100p\\) percentile is that number so that \\(100 \\times p\\) percent of the data is below it. In general, for a given \\(0&lt;q&lt;1\\), define the \\(q\\)th quantile \\(q_F\\) to be any number that satisfies \\[\\begin{aligned} \\label{E:Quantile} F(q_F-) \\le q \\le F(q_F).\\end{aligned}\\] Here, the notation \\(F(x-)\\) means to evaluate the function \\(F(\\cdot)\\) as a left-hand limit. If \\(F(\\cdot)\\) is continuous at \\(q_F\\), then \\(F(q_F-) = F(q_F)\\) Quantiles for a Continuous Distribution Function 4.1.2.1 Quantiles If F is smooth or there is a jump at \\(q\\), the definition of the quantile \\(q_F\\) is unique if F is flat at \\(q\\), then there a many definitions of \\(q_F\\) Three Quantile Cases 4.1.2.2 Quantiles Example – Toy. Consider \\(n=10\\) observations: The median might be defined to be any number between 20 and 23. Many software packages use the average 21.5. KPW defines the smoothed empirical percentile to be \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] where \\(j=[(n+1)q]\\) and, \\(h=(n+1)q-j\\), and \\(X_{(1)}, \\ldots, X_{(n)}\\) are the ordered values (the order statistics) corresponding to \\(X_1, \\ldots, X_n\\). Example. Take \\(n=10\\) and \\(q=0.5\\). Then, \\(j=[(11)0.5]=[5.5]=5\\) and, \\(h=(11)(0.5)-5=0.5\\). With this \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Take \\(n=10\\) and \\(q=0.2\\). Then, \\(j=[(11)0.2]=[2.2]=2\\) and, \\(h=(11)(0.2)-2=0.2\\). With this \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.2 (15) + (0.8)(15) = 15.\\] 4.1.3 Density Estimators When the random variable is discrete, estimate the probability mass function \\(f(x) = \\Pr(X=x)\\) is using \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x).\\] Observations may be grouped in the sense that they fall into intervals of the form \\([c_{j-1}, c_j)\\), for \\(j=1, \\ldots, k\\). The constants \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) form some partition of the domain of F(.). Then, use \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] where \\(n_j\\) is the number of observations (\\(X_i\\)) that fall into the interval \\([c_{j-1}, c_j)\\). Another way to write this is \\[f_n(x) = \\frac{1}{n(c_j-c_{j-1})} \\sum_{i=1}^n I(c_{j-1} &lt; X_i \\le c_j).\\] 4.1.3.1 Uniform Kernel Density Estimator Let \\(b&gt;0\\), known as a bandwidth, \\[\\begin{aligned} \\label{E:KDF} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b).\\end{aligned}\\] The estimator is the average over \\(n\\) \\(iid\\) realizations of a random variable with mean \\[\\begin{aligned} \\mathrm{E~ } \\frac{1}{2b} I(x-b &lt; X \\le x + b) &amp;= \\frac{1}{2b}\\left(F(x+b)-F(x-b)\\right) \\\\ &amp;= \\frac{1}{2b} \\left( \\left\\{ F(x) + b F^{\\prime}(x) + b^2 C_1\\right\\} \\right.\\\\ &amp; ~ ~ ~ - \\left. \\left\\{ F(x) - b F^{\\prime}(x) + b^2 C_2\\right\\} \\right) \\\\ &amp;= F^{\\prime}(x) + b \\frac{C_1-C_2}{2} \\rightarrow F^{\\prime}(x) = f(x),\\end{aligned}\\] as \\(b\\rightarrow 0\\). That is, \\(f_n(x)\\) is an asymptotically unbiased estimator of \\(f(x)\\). 4.1.3.2 Kernel Density Estimator More generally, define the kernel density estimator \\[\\begin{aligned} \\label{E:KDF2} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n k\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] where \\(k\\) is a probability density function centered about 0. Special Cases uniform kernel, \\(k(y) = \\frac{1}{2}I(-1 &lt; y \\leq 1)\\) . triangular kernel, \\(k(y) = (1-|y|)\\times I(|y| \\le 1)\\) Epanechnikov kernel, \\(k(y) = \\frac{3}{4}(1-y^2) \\times I(|y| \\le 1)\\), and Gaussian kernel \\(k(y) = \\phi(y)\\), where \\(\\phi(\\cdot)\\) is the standard normal density function. 4.1.3.3 Kernel Density Estimator of a Distribution Function The kernel density estimator of a distribution function is \\[\\begin{aligned} \\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n K\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] where \\(K\\) is a probability distribution function associated with the kernel density \\(k\\). To illustrate, for the uniform kernel, we have \\(k(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\) so \\[\\begin{aligned} K(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned}\\] 4.2 Nonparametric Estimation Tools For Model Selection 4.2.1 Graphical Comparisions 4.2.1.1 Comparing Distribution and Density Functions The left-hand panel compares distribution functions, with the dots corresponding to the empirical distribution, the thick blue curve corresponding to the fitted gamma and the light purple curve corresponding to the fitted Pareto. The right hand panel compares these three distributions summarized using probability density functions. Nonparametric Versus Fitted Parametric Distribution and Density Functions 4.2.1.2 PP Plot The horizontal axes gives the empirical distribution function at each observation. In the left-hand panel, the corresponding distribution function for the gamma is shown in the vertical axis. The right-hand panel shows the fitted Pareto distribution. Lines of \\(y=x\\) are superimposed. Probability-Probability (pp) Plots. KPW also recommends plotting the difference \\(D(x) = F_n(x) - F^*(x)\\) versus \\(x\\). Here, \\(F^*(x)\\) is the fitted model distribution function. 4.2.1.3 QQ Plot The horizontal axes gives the empirical quantiles at each observation. The right-hand panels they are graphed on a logarithmic basis. The vertical axis gives the quantiles from the fitted distributions; Gamma quantiles are in the upper panels, Pareto quantiles are in the lower panels. The lower-right hand panel suggests that the Pareto distribution does a good job with large observations but provides a poorer fit for small observations. Quantile-Quantile (\\(qq\\)) Plots 4.2.2 Statistical Comparisions 4.2.2.1 Three Goodness of Fit Statistics \\[ \\begin{matrix} \\begin{array}{ccc} \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline Kolmogorov &amp; sup_x |F_n(x) - F(x) | &amp; max(D^+ - D^-) \\\\ -Smirnov &amp;&amp;D^+ = \\max_{i=1, \\ldots, n} \\left(\\frac{i}{n} - F_i\\right) \\\\ &amp; &amp;D^- = \\max_{i=1, \\ldots, n} \\left(F_i - \\frac{i-1}{n} \\right) \\\\ \\hline Cramer&amp; n \\int (F_n(x) - F(x))^2 dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i - (2i-1)/n\\right)^2 \\\\ -von Mises &amp; &amp; \\\\ \\hline Anderson&amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} dx &amp;\\\\ -Darling &amp; &amp; -\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\end{matrix} \\] 4.3 Nonparametric Estimation using Modified Data 4.3.1 Grouped Data 4.3.1.1 Grouped Data Observations may be grouped in the sense that they fall into intervals of the form \\([c_{j-1}, c_j)\\), for \\(j=1, \\ldots, k\\). The constants \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) form some partition of the domain of F(.). Define the empirical distribution function at the boundaries is defined in the usual way: \\[F_n(c_j) = \\frac{\\text{number of observations } \\le c_j}{n}\\] For other values of \\(x\\), one could use the Ogive: connect values of the boundaries with a straight line. For another way of smoothing, recall the kernel density estimator of the distribution function \\[\\begin{aligned} \\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n K\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] For densities, use \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] 4.3.2 Censored Data 4.3.2.1 Censored Data Censoring occurs when we observe only a limited value of an observation. Suppose that \\(X\\) represents a loss due to an insured event and that \\(u\\) is a known censoring point. If observations are censored from the right (or from above), then we observe \\[Y = \\min(X,u).\\] In this case, \\(u\\) may represent the upper limit of coverage for an insurer. The loss exceeds the amount \\(u\\) but the insurer does not have in its records the amount of the actual loss. If observations are censored from the left (or from below), then we observe \\[Y = \\max(X,u).\\] Let \\(u\\) represents the upper limit of coverage but now \\(Y - u\\) represents the amount that a reinsurer is responsible for. If the loss \\(X &lt; u\\), then \\(Y=0\\), no loss for the reinsurer. If the loss \\(X \\ge u\\), then \\(Y= X-u\\) represents the reinsurer’s retained claims. 4.3.2.2 Kaplan-Meier Product Limit Estimator Let \\(t_{1} &lt;\\cdots&lt; t_{c}\\) be distinct points at which an event of interest occurs, or non-censored losses, and let \\(s_j\\) be the number of events at time point \\(t_{j}\\) . Further, the corresponding risk set is the number of observations that are active at an instant just prior to \\(t_{j}\\) . Using notation, the risk set is \\(R_{j}=\\sum_{i=1}^{n}I(x_{i}\\geq t_{j})\\). With this notation, the product-limit estimator of the distribution function is \\[\\hat{F}(x)= \\left\\lbrace \\begin{array}{llll} 0 &amp; x &lt; t_{1} \\\\ 1-\\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1} .\\\\ \\end{array} \\right .\\] Greenwood (1926) derived the formula for the estimated variance \\[\\widehat{Var}(\\hat{F}(x)) = (1-\\hat{F}(x))^{2} \\sum _{j:t_{j} \\leq x} \\dfrac{s_j}{R_{j}(R_{j}-s_j)}.\\] 4.3.3 Truncated Data 4.3.3.1 Truncated Data An outcome is potentially truncated when the availability of an observation depends on the outcome. In insurance, it is common for observations to be truncated from the left (or below) at \\(d\\) when the amount observed is \\[Y = \\begin{cases} \\text{we do not observe X} &amp; X &lt; d\\\\ X-d &amp; X \\ge d. \\end{cases}\\] In this case, \\(d\\) may represent the deductible associated with an insurance coverage. If the insured loss is less than the deductible, then the insurer does not observe the loss. If the loss exceeds the deductible, then the excess \\(X-d\\) is the claim that the insurer covers. Observations may also truncated from the right (or above) at \\(d\\) when the amount observed is \\[Y = \\begin{cases} X &amp; X &lt; d \\\\ \\text{we do not observe X} &amp; X \\ge d\\\\ \\end{cases}\\] Classic examples of truncation from the right include \\(X\\) as a measure of distance of a star. When the distance exceeds a certain level \\(d\\), the star is no longer observable. 4.3.3.2 Right-Censored, Left-Truncated Empirical Distribution Functions Procedure from KPW. Notation: For each observation \\(i\\), let \\(d_i\\) be the lower truncation limit (0 if no truncation) Let \\(u_i\\) be the upper censoring limit (=\\(\\infty\\) if no censoring) The recorded value is \\(x_i\\) in the case of no censoring, \\(u_i\\) if there is censoring. For notation, let \\(t_1 &lt; \\cdots &lt; t_k\\) be \\(k\\) unique observations of \\(x_i\\) that are uncensored. Define \\(s_j\\) to be the number of \\(x_i\\)’s at \\(t_j\\). Define the risk set \\[R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j}) - \\sum_{i=1}^n I(d_i \\geq t_{j})\\] The product-limit estimator of the distribution function is \\[\\hat{F}(x)= \\left\\lbrace \\begin{array}{llll} 0 &amp; x &lt; t_{1} \\\\ 1- \\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1}\\\\ \\end{array} \\right .\\] The Nelson-Aalen estimator of the distribution function is \\[\\hat{F}(x)= \\left\\lbrace \\begin{array}{llll} 0 &amp; x &lt; t_{1} \\\\ 1- \\exp \\left(-\\sum_{j:t_{j} \\leq x}\\frac{s_j}{R_j} \\right) &amp; x \\geq t_{1}\\\\ \\end{array} \\right.\\] 4.4 Topics in Parametric Estimation 4.4.1 Starting Values Maximum likelihood is a desirable estimation technique because It employs data efficiently (enjoys certain optimality properties) It can be used in a variety of data sampling schemes (e.g.,iid, grouped, censored, regression, and so forth) However, maximum likelihood is a recursive estimation procedure that requires starting values to begin the recursion Two alternative estimation techniques are: Method of moments Percentile matching These are non-recursive techniques. Easy to implement and explain. Although less efficient than maximum likelihood, they can be employed to provide starting values for maximum likelihood. 4.4.1.1 Method of Moments Idea: Approximate the moments using a parametric distribution to the empirical (nonparametric) moments Example - Property Fund. For the 2010 property fund, there are \\(n=1,377\\) individual claims (in thousands of dollars) with \\[\\begin{aligned} m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\end{aligned}\\] Gamma Distribution From theory, \\(\\mu_1 = \\alpha \\theta\\) and \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\). Equating the two yields the method of moments estimators, easy algebra shows that \\[\\begin{aligned} \\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-2\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\end{aligned}\\] The method of moment estimators are \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809\\\\ %\\text{and} \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629.\\end{aligned}\\] In contrast, the maximum likelihood values turn out to be \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) and \\(\\hat{\\theta}_{MLE} = 91.61378\\) Big discrepancies between the two estimation procedures, suggesting that the gamma model fits poorly. Example - Property Fund. Recall the nonparametric estimates \\[\\begin{aligned} m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\end{aligned}\\] Pareto Distribution From theory, \\(\\mu_1 = \\theta/(\\alpha -1)\\) and \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\). Easy algebra shows \\[\\begin{aligned} \\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-2\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\end{aligned}\\] The method of moment estimators are \\[\\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-2*26,62259^2} = 2.01052\\\\ %\\text{and} \\\\ \\hat{\\theta} &amp;= (2.01052-1) \\cdot 26.62259 = 26.9027\\end{aligned}\\] The maximum likelihood values turn out to be \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) and \\(\\hat{\\theta}_{MLE} = 2.2821147\\). Interesting that \\(\\hat{\\alpha}_{MLE}&lt;1\\); for the Pareto distribution; recall that \\(\\alpha &lt;1\\) means that the mean is infinite. Indicates that the property claims data set is a long tail distribution. 4.4.1.2 Percentile Matching Under percentile matching, one approximates the parametric distribution using the empirical (nonparametric) quantiles, or percentiles. Example - Property Fund. The 25th percentile (the first quartile) turns out to be 0.78853 and the 95th percentile is 50.98293 (both in thousands of dollars). Pareto Distribution The Pareto distribution is particularly intuitively pleasing because of the closed-form solution for the quantiles. The distribution function is \\(F(x) = 1 - \\left(\\theta/(x+\\theta )\\right)^{\\alpha}\\). Easy algebra shows that we can express the quantile as \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right)\\] for a fraction \\(q\\), \\(0&lt;q&lt;1\\). With two equations \\[0.78853 = \\theta \\left( (1-.25)^{-1/\\alpha} -1 \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( (1-.95)^{-1/\\alpha} -1\\right)\\] and two unknowns, the solution is \\[ \\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] A numerical routine was required for these solutions - no analytic solution available. Recall that the maximum likelihood values are \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) and \\(\\hat{\\theta}_{MLE} = 2.2821147\\). The percentile matching provides a better approximation for the Pareto distribution than did the method of moments. 4.4.2 Grouped Data 4.4.2.1 Parametric Estimation Using Grouped Data Observations may be grouped in the sense that they fall into intervals of the form \\((c_{j-1}, c_j]\\), for \\(j=1, \\ldots, k\\). The constants \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) form some partition of the domain of F(.). Define \\(n_j\\) to be the number of observations that fall in the \\(j\\)th interval, \\((c_{j-1}, c_j]\\). The probability of an observation \\(X\\) falling in the \\(j\\)th interval is \\[\\Pr\\left(X \\in c_{j-1}, c_j]\\right) = F(c_j) - F(c_{j-1}).\\] The probability of an observation \\(X\\) falling in the \\(j\\)th interval is \\[\\Pr\\left(X \\in c_{j-1}, c_j]\\right) = F(c_j) - F(c_{j-1}).\\] The corresponding mass function is \\[\\begin{aligned} f(x) &amp;= \\begin{cases} F(c_1) - F(c_{0}) &amp; \\textrm{if~} x \\in (c_{0}, c_1]\\\\ \\vdots &amp; \\vdots \\\\ F(c_k) - F(c_{k-1}) &amp; \\textrm{if~} x \\in (c_{k-1}, c_k]\\\\ \\end{cases} \\\\ &amp;= \\prod_{j=1}^k \\left\\{F(c_j) - F(c_{j-1})\\right\\}^{I(x \\in (c_{j-1}, c_j])}\\end{aligned}\\] The likelihood is \\[\\begin{aligned} \\prod_{j=1}^n f(x_i) = \\prod_{j=1}^k \\left\\{F(c_j) - F(c_{j-1})\\right\\}^{n_j}\\end{aligned}\\] The log-likelihood is \\[\\begin{aligned} L(\\theta) = \\ln \\prod_{j=1}^n f(x_i) = \\sum_{j=1}^k n_j \\ln \\left\\{F(c_j) - F(c_{j-1})\\right\\}\\end{aligned}\\] 4.4.3 Parametric Estimation Using Censored Data 4.4.3.1 Censored Data Likelihood Censoring occurs when we observe only a limited value of an observation. Suppose that \\(X\\) represents a loss due to an insured event and that \\(u\\) is a known censoring point. If observations are censored from the right (or from above), then we observe we observe \\(Y= \\min(X, u)\\) and \\(\\delta_u= \\mathrm{I}(X \\geq u)\\). If censoring occurs so that \\(\\delta_u=1\\), then \\(X \\geq u\\) and the likelihood is \\(\\Pr(X \\ge u) = 1-\\mathrm{F}(u)\\). If censoring does not occur so that \\(\\delta_u=0\\), then \\(X &lt; C_U\\) and the likelihood is \\(\\mathrm{f}(y)\\). Summarizing, we have \\[\\begin{aligned} Likelihood &amp;= \\left\\{ \\begin{array}{cl} \\mathrm{f}(y) &amp; \\textrm{if~}\\delta=0 \\\\ 1-\\mathrm{F}(u) &amp; \\textrm{if~}\\delta=1 \\end{array}\\right. \\\\ &amp;= \\left( \\mathrm{f}(y)\\right)^{1-\\delta} \\left(1-\\mathrm{F}(u)\\right)^{\\delta} .\\end{aligned}\\] The second right-hand expression allows us to present the likelihood more compactly. For a single observation, we have \\[\\begin{aligned} Likelihood &amp;= \\left\\{ \\begin{array}{cl} \\mathrm{f}(y) &amp; \\textrm{if~}\\delta=0 \\\\ 1-\\mathrm{F}(u) &amp; \\textrm{if~}\\delta=1 \\end{array}\\right. \\\\ &amp;= \\left( \\mathrm{f}(y)\\right)^{1-\\delta} \\left(1-\\mathrm{F}(u)\\right)^{\\delta} .\\end{aligned}\\] Consider a random sample of size \\(n\\), \\[\\{ (y_1,\\delta_1), \\ldots,(y_n, \\delta_n) \\} \\] with potential censoring times ${ u_1, , u_n } $. The likelihood is \\[\\prod_{i=1}^n \\left( \\mathrm{f}(y_i)\\right)^{1-\\delta_i} \\left(1-\\mathrm{F}(u_i)\\right)^{\\delta_i} = \\prod_{\\delta_i=0}\\mathrm{f}(y_i) \\prod_{\\delta_i=1} \\{1-\\mathrm{F}(u_i)\\},\\] Here, the notation \\(\\prod_{\\delta_i=0}\\) means take the product over uncensored observations, and similarly for \\(\\prod_{\\delta_i=1}\\). The log-likelihood is \\[L(\\theta) = \\sum_{i=1}^n \\left\\{(1-\\delta_i) \\ln \\mathrm{f}(y_i) + \\delta_i \\ln \\left(1-\\mathrm{F}(u_i)\\right) \\right\\}\\] 4.4.4 Censored and Truncated Data Let \\(X\\) denote the outcome and let \\(C_L\\) and \\(C_U\\) be two constants. Type Limited Variable Censoring Information right censoring \\(X_U^{\\ast}= \\min(X, C_U)\\) \\(\\delta_U= \\mathrm{I}(X \\geq C_U)\\) left censoring \\(X_L^{\\ast}= \\max(y, C_L)\\) \\(\\delta_L= \\mathrm{I}(X \\leq C_L)\\) interval censoring right truncation \\(X\\) observe \\(X\\) if \\(X &lt; C_U\\) left truncation \\(X\\) observe \\(X\\) if \\(X &lt; C_L\\) Figure 4.2: Censoring and Truncation 4.4.4.1 Example: Mortality Study Suppose that you are conducting a two-year study of mortality of high-risk subjects, beginning January 1, 2010 and finishing January 1, 2012. For each subject, the beginning of the arrow represents that the the subject was recruited and the arrow end represents the event time. Thus, the arrow represents exposure time. Figure 4.3: Subjects on Test in a Mortality Study Type A - right-censored. This subject is alive at the beginning and the end of the study. Because the time of death is not known by the end of the study, it is right-censored. Most subjects are Type A. Type B. Complete information is available for a type B subject. The subject is alive at the beginning of the study and the death occurs within the observation period. Type C - right-censored and left-truncated. A type C subject is right-censored, in that death occurs after the observation period. However, the subject entered after the start of the study and is said to have a delayed entry time. Because the subject would not have been observed had death occurred before entry, it is left-truncated. Type D - left-truncated. A type D subject also has delayed entry. Because death occurs within the observation period, this subject is not right censored. Type E - left-truncated. A type E subject is not included in the study because death occurs prior to the observation period. Type F - right-truncated. Similarly, a type F subject is not included because the entry time occurs after the observation period. 4.4.5 Parametric Estimation Using Censored and Truncated Data Truncated data are handled in likelihood inference via conditional probabilities. Adjust the likelihood contribution by dividing by the probability that the variable was observed. Summarizing, we have the following contributions to the likelihood for six types of outcomes. \\[\\begin{array}{lc} \\hline Outcome &amp; Likelihood~Contribution \\\\\\hline \\text{exact value } &amp; f(x) \\\\ \\text{right-censoring } &amp; 1-F(C_U) \\\\ \\text{left-censoring } &amp; F(C_L) \\\\ \\text{right-truncation } &amp; f(x)/F(C_U) \\\\ \\text{left-truncation }&amp; f(x)/(1-F(C_L)) \\\\ \\text{interval-censoring} &amp; F(C_U)-F(C_L) \\\\ \\hline \\end{array}\\] For known outcomes and censored data, the likelihood is \\[\\prod_{E} \\mathrm{f}(x_i) \\prod_{R} \\{1-\\mathrm{F}(C_{Ui})\\} \\prod_{L} \\mathrm{F}(C_{Li}) \\prod_{I} (\\mathrm{F}(C_{Ui})-\\mathrm{F}(C_{Li})),\\] where \\(\\prod_{E}\\) is the product over observations with Exact values, and similarly for Right-, Left- and Interval-censoring. For right-censored and left-truncated data, the likelihood is \\[\\prod_{E} \\frac{\\mathrm{f}(x_i)}{1-\\mathrm{F}(C_{Li})} \\prod_{R} \\frac{1-\\mathrm{F}(C_{Ui})}{1-\\mathrm{F}(C_{Li})} ,\\] Similarly for other combinations. 4.4.5.1 Special Case: Exponential Distribution Consider data that are right-censored and left-truncated, with random variables \\(X_i\\) that are exponentially distributed with mean \\(\\theta\\). With these specifications, recall that \\(\\mathrm{f}(x) = \\theta^{-1} \\exp(-x/\\theta)\\) and \\(\\mathrm{F}(x) = 1-\\exp(-x/\\theta)\\). For this special case, the logarithmic likelihood is \\[\\begin{aligned} \\ln Likelihood &amp;= \\sum_{E} \\left( \\ln \\mathrm{f}(x_i) - \\ln (1-\\mathrm{F}(C_{Li})) \\right) -\\sum_{R}\\left( \\ln (1-\\mathrm{F}(C_{Ui}))- \\ln (1-\\mathrm{F}(C_{Li})) \\right) \\\\ &amp;= \\sum_{E} (-\\ln \\theta -(x_i-C_{Li})/\\theta ) -\\sum_{R} (C_{Ui}-C_{Li})/\\theta .\\end{aligned}\\] To simplify the notation, define \\(\\delta_i = \\mathrm{I}(X_i \\geq C_{Ui})\\) to be a binary variable that indicates right-censoring. Let \\(X_i^{\\ast \\ast} = \\min(X_i, C_{Ui}) - C_{Li}\\) be the amount that the observed variable exceeds the lower truncation limit. With this, the logarithmic likelihood is \\[\\ln Likelihood = - \\sum_{i=1}^n \\left((1-\\delta_i) \\ln \\theta + \\frac{x_i^{\\ast \\ast}}{\\theta} \\right).\\] Taking derivatives with respect to the parameter \\(\\theta\\) and setting it equal to zero yields the maximum likelihood estimator \\[\\begin{aligned} \\widehat{\\theta} &amp;= \\frac{1}{n_u} \\sum_{i=1}^n x_i^{\\ast \\ast},\\end{aligned}\\] where \\(n_u = \\sum_i (1-\\delta_i)\\) is the number of uncensored observations. 4.5 Bayesian Inference 4.5.1 Bayesian Model In the frequentist interpretation, one treats the vector of parameters \\(\\boldsymbol \\theta\\) as fixed yet unknown, whereas the outcomes \\(X\\) are realizations of random variables. With Bayesian statistical models, one views both the model parameters and the data as random variables. Use probability tools to reflect this uncertainty about the parameters \\(\\boldsymbol \\theta\\). For notation, we will think about \\(\\boldsymbol \\theta\\) as a random vector and let \\(\\pi(\\boldsymbol \\theta)\\) denote the distribution of possible outcomes. 4.5.1.1 Bayesian Inference Strengths There are several advantages of the Bayesian approach. One can describe the entire distribution of parameters conditional on the data. This allows one, for example, to provide probability statements regarding the likelihood of parameters. This approach allows analysts to blend information known from other sources with the data in a coherent manner. This topic is developed in detail in the credibility chapter. The Bayesian approach provides for a unified approach for estimating parameters. Some non-Bayesian methods, such as least squares, required a approach to estimating variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. Convenient for explaining results to consumers of the data analysis. Bayesian analysis is particularly useful for forecasting future responses. 4.5.1.2 Bayesian Model Prior Distribution. \\(\\pi(\\boldsymbol \\theta)\\) is called the prior distribution. Typically, it is a regular distribution and so integrates to one. We may be very uncertain (or have no clue) about the distribution of \\(\\boldsymbol \\theta\\); the Bayesian machinery allows this situation \\[\\int \\pi(\\theta) d\\theta = \\infty\\] in which case \\(\\pi(\\cdot)\\) is called an improper prior. Model Distribution. The distribution of outcomes given an assumed value of \\(\\boldsymbol \\theta\\) is known as the model distribution and denoted as \\(f(x | \\boldsymbol \\theta) = f_{X|\\boldsymbol \\theta} (x|\\boldsymbol \\theta )\\). This is the (usual frequentist) mass or density function. Joint Distribution. The distribution of outcomes and model parameters is, not surprisingly, known as the joint distribution and denoted as \\(f(x , \\boldsymbol \\theta) = f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)\\). Marginal Outcome Distribution. The distribution of outcomes can be expressed as \\[f(x) =\\int f(x | \\boldsymbol \\theta)\\pi(\\boldsymbol \\theta) d\\boldsymbol \\theta.\\] This is analogous to a frequentist mixture distribution. Posterior Distribution of Parameters. After outcomes have been observed (hence the terminology posterior), one can use Bayes theorem to write the distribution as \\[\\pi(\\boldsymbol \\theta | x) =\\frac{f(x , \\boldsymbol \\theta)}{f(x)} =\\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)}\\] The idea is to update your knowledge of the distribution of \\(\\boldsymbol \\theta\\) (\\(\\pi(\\boldsymbol \\theta)\\)) with the data \\(x\\). We can summarize the distribution using a confidence interval type statement. Definition. \\([a,b]\\) is said to be a \\(100(1-\\alpha)\\%\\) credibility interval for \\(\\boldsymbol \\theta\\) if \\[\\Pr (a \\le \\theta \\le b | \\mathbf{x}) \\ge 1- \\alpha.\\] 4.5.1.3 Two Examples Exam C Question 157. You are given: (i) In a portfolio of risks, each policyholder can have at most one claim per year. (ii) The probability of a claim for a policyholder during a year is \\(q\\). (iii) The prior density is \\[\\pi(q) = q^3/0.07, \\ \\ \\ 0.6 &lt; q &lt; 0.8\\] A randomly selected policyholder has one claim in Year 1 and zero claims in Year 2. For this policyholder, calculate the posterior probability that \\(0.7 &lt; q &lt; 0.8\\). Exam C Question 43. You are given: (i) The prior distribution of the parameter \\(\\Theta\\) has probability density function: \\[\\pi(\\theta) = 1/\\theta^2, \\ \\ \\ \\ 1 &lt; \\theta &lt; \\infty\\] (ii) Given \\(\\Theta = \\theta\\), claim sizes follow a Pareto distribution with parameters \\(\\alpha=2\\) and \\(\\theta\\). A claim of 3 is observed. Calculate the posterior probability that \\(\\Theta\\) exceeds 2. 4.5.2 Bayesian Inference - Decision Analysis In classical decision analysis, the loss function \\(l(\\hat{\\theta}, \\theta)\\) determines the penalty paid for using the estimate \\(\\hat{\\theta}\\) instead of the true \\(\\theta\\). The Bayes estimate is that value that minimizes the expected loss \\(\\mathrm{E~}l(\\hat{\\theta}, \\theta)\\). Some important special cases include: \\[\\begin{array}{ccc} \\hline \\text{ Loss function } l(\\hat{\\theta}, \\theta) &amp; \\text{Descriptor} &amp; \\text{Bayes Estimate}\\\\ \\hline (\\hat{\\theta}- \\theta)^2 &amp; \\text{squared error loss} &amp; \\mathrm{E}(\\theta|X) \\\\ |\\hat{\\theta}- \\theta| &amp; \\text{absolute deviation loss} &amp; median of \\pi(\\theta|x)\\\\ I(\\hat{\\theta} =\\theta) &amp; \\text{zero-one loss (for discrete probabilities)}&amp; mode of \\pi(\\theta|x) \\\\ \\hline \\end{array}\\] For new data \\(y\\), the predictive distribution is \\[f(y|x) = \\int f(y|\\theta) \\pi(\\theta|x) d\\theta .\\] With this, the Bayesian prediction of \\(y\\) is \\[\\begin{aligned} \\mathrm{E}(y|x) &amp;= \\int y f(y|x) dy = \\int y \\left(\\int f(y|\\theta) \\pi(\\theta|x) d\\theta \\right) dy \\\\ &amp;= \\int \\mathrm{E}(y|\\theta) \\pi(\\theta|x) d\\theta .\\end{aligned}\\] 4.5.2.1 Posterior Distribution How to calculate the posterior distribution? By hand - can do this in special cases Simulation - uses modern computational techniques. KPW (Section 12.4.4) mentions Markov Chain Monte Carlo (MCMC) simulation Normal Approximation. Theorem 12.39 of KPW provides a justification Conjugate distributions. Classical approach. Although this approach is available only for a limited number of distributions, it has the appeal that it provides closed-form expressions for the distributions, allowing for easy interpretations of results. We focus on this approach. To relate the prior and posterior distributions of the parameters, we have \\[\\begin{aligned} \\pi(\\boldsymbol \\theta | x)&amp;=\\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)} \\\\ &amp; \\propto f(x|\\boldsymbol \\theta ) \\pi(\\boldsymbol \\theta) \\\\ \\text{Posterior} &amp;\\text{is proportional to} \\text{likelihood} \\times \\text{prior} \\end{aligned}\\] For conjugate distributions, the posterior and the prior come from the same family of distributions. 4.5.2.2 Special Case: Poisson Gamma Conjugate Family Assume a Poisson(\\(\\lambda\\)) model distribution so that \\[f(\\mathbf{x} | \\lambda) = \\prod_{i=1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\] Assume \\(\\lambda\\) follows a gamma(\\(\\alpha, \\theta\\)) prior distribution so that \\[\\pi(\\lambda) = \\frac{\\left(\\lambda/\\theta\\right)^{\\alpha} \\exp(-\\lambda/\\theta)}{\\lambda \\Gamma(\\alpha)}.\\] The posterior distribution is proportional to \\[\\begin{aligned} \\pi(\\lambda | \\mathbf{x}) &amp; \\propto f(\\mathbf{x}|\\theta ) \\pi(\\lambda) \\\\ &amp;= C \\lambda^{\\sum_i x_i + \\alpha -1} \\exp(-\\lambda (n+1/\\theta))\\end{aligned}\\] where \\(C\\) is a constant. We recognize this to be a gamma distribution with new parameters \\(\\alpha_{new} = \\sum_i x_i + \\alpha\\) and \\(\\theta_{new} = 1/(n + 1/\\theta)\\). 4.6 Exercises Here are a set of exercises that guide the viewer through some of the theoretical foundations of Loss Data Analytics. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C. knitr::include_url(&quot;http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-model-selection/&quot;,height = &quot;600px&quot;) "],
["simulation.html", "Chapter 5 Simulation 5.1 Generating Independent Uniform Observations 5.2 Inverse Transform 5.3 How Many Simulated Values?", " Chapter 5 Simulation Simulation is a computer-based, computationally intensive, method of solving difficult problems, such as analyzing business processes. Instead of creating physical processes and experimenting with them in order to understand their operational characteristics, a simulation study is based on a computer representation - it considers various hypothetical conditions as inputs and summarizes the results. Through simulation, a vast number of hypothetical conditions can be quickly and inexpensively examined. Performing the same analysis with a physical system is not only expensive and time-consuming but, in many cases, impossible. A drawback of simulation is that computer models are not perfect representations of business processes. There are three basic steps for producing a simulation study: Generating approximately independent realizations that are uniformly distributed Transforming the uniformly distributed realizations to observations from a probability distribution of interest With the generated observations as inputs, designing a structure to produce interesting and reliable results. Designing the structure can be a difficult step, where the degree of difficulty depends on the problem being studied. There are many resources, including this tutorial, to help the actuary with the first two steps. 5.1 Generating Independent Uniform Observations We begin with a historically prominent method. Linear Congruential Generator. To generate a sequence of random numbers, start with \\(B_0\\), a starting value that is known as a “seed.” Update it using the recursive relationship \\[B_{n+1} = a B_n + c \\text{ modulo }m, ~~ n=0, 1, 2, \\ldots .\\] This algorithm is called a linear congruential generator. The case of \\(c=0\\) is called a multiplicative congruential generator; it is particularly useful for really fast computations. For illustrative values of \\(a\\) and \\(m\\), Microsoft’s Visual Basic uses \\(m=2^{24}\\), \\(a=1,140,671,485\\), and \\(c = 12,820,163\\) (see http://support.microsoft.com/kb/231847). This is the engine underlying the random number generation in Microsoft’s Excel program. The sequence used by the analyst is defined as \\(U_n=B_n/m.\\) The analyst may interpret the sequence {\\(U_{i}\\)} to be (approximately) identically and independently uniformly distributed on the interval (0,1). To illustrate the algorithm, consider the following. Example. Take \\(m=15\\), \\(a=3\\), \\(c=2\\) and \\(B_0=1\\). Then we have: step \\(n\\) \\(B_n\\) \\(U_n\\) 0 \\(B_0=1\\) 1 \\(B_1 =\\mod(3 \\times 1 +2) = 5\\) \\(U_1 = \\frac{5}{15}\\) 2 \\(B_2 =\\mod(3 \\times 5 +2) = 2\\) \\(U_2 = \\frac{2}{15}\\) 3 \\(B_3 =\\mod(3 \\times 2 +2) = 8\\) \\(U_3 = \\frac{8}{15}\\) 4 \\(B_4 =\\mod(3 \\times 8 +2) = 11\\) \\(U_4 = \\frac{11}{15}\\) Sometimes computer generated random results are known as pseudo-random numbers to reflect the fact that they are machine generated and can be replicated. That is, despite the fact that {\\(U_{i}\\)} appears to be i.i.d, it can be reproduced by using the same seed number (and the same algorithm). The ability to replicate results can be a tremendous tool as you use simulation while trying to uncover patterns in a business process. The linear congruential generator is just one method of producing pseudo-random outcomes. It is easy to understand and is (still) widely used. The linear congruential generator does have limitations, including the fact that it is possible to detect long-run patterns over time in the sequences generated (recall that we can interpret “independence” to mean a total lack of functional patterns). Not surprisingly, advanced techniques have been developed that address some of this method’s drawbacks. 5.2 Inverse Transform With the sequence of uniform random numbers, we next transform them to a distribution of interest. Let \\(F\\) represent a distribution function of interest. Then, use the inverse transform \\[X_i=F^{-1}\\left( U_i \\right) .\\] The result is that the sequence {\\(X_{i}\\)} is approximately i.i.d. with distribution function \\(F\\). To interpret the result, recall that a distribution function, \\(F\\), is monotonically increasing and so the inverse function, \\(F^{-1}\\), is well-defined. The inverse distribution function (also known as the quantile function), is defined as \\[\\begin{aligned} F^{-1}(y) = \\inf_x \\{ F(x) \\ge y \\} ,\\end{aligned}\\] where “\\(\\inf\\)” stands for “infimum”, or the greatest lower bound. Inverse Transform Visualization. Here is a graph to help you visualize the inverse transform. When the random variable is continuous, the distribution function is strictly increasing and we can readily identify a unique inverse at each point of the distribution. Figure 5.1: Inverse of a Distribution Function The inverse transform result is available when the underlying random variable is continuous, discrete or a mixture. Here is a series of examples to illustrate its scope of applications. Exponential Distribution Example. Suppose that we would like to generate observations from an exponential distribution with scale parameter \\(\\theta\\) so that \\(F(x) = 1 - e^{-x/\\theta}\\). To compute the inverse transform, we can use the following steps: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow y = 1-e^{-x/\\theta} \\\\ &amp;\\Leftrightarrow -\\theta \\ln(1-y) = x = F^{-1}(y) .\\end{aligned}\\] Thus, if \\(U\\) has a uniform (0,1) distribution, then \\(X = -\\theta \\ln(1-U)\\) has an exponential distribution with parameter \\(\\theta\\). Some Numbers. Take \\(\\theta = 10\\) and generate three random numbers to get \\(U\\) 0.26321364 0.196884752 0.897884218 \\(X = -10\\ln(1-U)\\) 1.32658423 0.952221285 9.909071325 Pareto Distribution Example. Suppose that we would like to generate observations from a Pareto distribution with parameters \\(\\alpha\\) and \\(\\theta\\) so that \\(F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha}\\). To compute the inverse transform, we can use the following steps: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow 1-y = \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha} \\\\ &amp;\\Leftrightarrow \\left(1-y\\right)^{-1/\\alpha} = \\frac{x+\\theta}{\\theta} = \\frac{x}{\\theta} +1 \\\\ &amp;\\Leftrightarrow \\theta \\left((1-y)^{-1/\\alpha} - 1\\right) = x = F^{-1}(y) .\\end{aligned}\\] Thus, \\(X = \\theta \\left((1-U)^{-1/\\alpha} - 1\\right)\\) has a Pareto distribution with parameters \\(\\alpha\\) and \\(\\theta\\). Inverse Transform Justification. Why does the random variable \\(X = F^{-1}(U)\\) have a distribution function “\\(F\\)”? This is easy to establish in the continuous case. Because \\(U\\) is a Uniform random variable on (0,1), we know that \\(\\Pr(U \\le y) = y\\), for \\(0 \\le y \\le 1\\). Thus, \\[\\begin{aligned} \\Pr(X \\le x) &amp;= \\Pr(F^{-1}(U) \\le x) \\\\ &amp;= \\Pr(F(F^{-1}(U)) \\le F(x)) \\\\ &amp;= \\Pr(U \\le F(x)) = F(x)\\end{aligned}\\] as required. The key step is that $ F(F^{-1}(u)) = u$ for each \\(u\\), which is clearly true when \\(F\\) is strictly increasing. Bernoulli Distribution Example. Suppose that we wish to simulate random variables from a Bernoulli distribution with parameter \\(p=0.85\\). A graph of the cumulative distribution function shows that the quantile function can be written as Figure 5.2: Distribution Function of a Binary Random Variable \\[\\begin{aligned} F^{-1}(y) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;y \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; y \\leq 1.0 . \\end{array} \\right.\\end{aligned}\\] Thus, with the inverse transform we may define \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;U \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; U \\leq 1.0 \\end{array} \\right.\\end{aligned}\\] Some Numbers. Generate three random numbers to get \\(U\\) 0.26321364 0.196884752 0.897884218 \\(X =F^{-1}(U)\\) 0 0 1 Discrete Distribution Example. Consider the time of a machine failure in the first five years. The distribution of failure times is given as: Time (\\(x\\)) 1 2 3 4 5 probability 0.1 0.2 0.1 0.4 0.2 \\(F(x)\\) 0.1 0.3 0.4 0.8 1.0 Figure 5.3: Distribution Function of a Discrete Random Variable Using the graph of the distribution function, with the inverse transform we may define \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 1 &amp; 0&lt;U \\leq 0.1 \\\\ 2 &amp; 0.1 &lt; U \\leq 0.3\\\\ 3 &amp; 0.3 &lt; U \\leq 0.4\\\\ 4 &amp; 0.4 &lt; U \\leq 0.8 \\\\ 5 &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right.\\end{aligned}\\] For general discrete random variables there may not be an ordering of outcomes. For example, a person could own one of five types of life insurance products and we might use the following algorithm to generate random outcomes: \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0&lt;U \\leq 0.1 \\\\ \\textrm{endowment} &amp; 0.1 &lt; U \\leq 0.3\\\\ \\textrm{term life} &amp; 0.3 &lt; U \\leq 0.4\\\\ \\textrm{universal life} &amp; 0.4 &lt; U \\leq 0.8 \\\\ \\textrm{variable life} &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right.\\end{aligned}\\] Another analyst may use an alternative procedure such as: \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0.9&lt;U&lt;1.0 \\\\ \\textrm{endowment} &amp; 0.7 \\leq U &lt; 0.9\\\\ \\textrm{term life} &amp; 0.6 \\leq U &lt; 0.7\\\\ \\textrm{universal life} &amp; 0.2 \\leq U &lt; 0.6 \\\\ \\textrm{variable life} &amp; 0 \\leq U &lt; 0.2 . \\end{array} \\right.\\end{aligned}\\] Both algorithms produce (in the long-run) the same probabilities, e.g., \\(\\Pr(\\textrm{whole life})=0.1\\), and so forth. So, neither is incorrect. You should be aware that there is “more than one way to skin a cat.” (What an old expression!) Similarly, you could use an alterative algorithm for ordered outcomes (such as failure times 1, 2, 3, 4, or 5, above). Mixed Distribution Example. Consider a random variable that is 0 with probability 70% and is exponentially distributed with parameter \\(\\theta= 10,000\\) with probability 30%. In practice, this might correspond to a 70% chance of having no insurance claims and a 30% chance of a claim - if a claim occurs, then it is exponentially distributed. The distribution function is given as \\[\\begin{aligned} F(y) = \\left\\{ \\begin{array}{cc} 0 &amp; x&lt;0 \\\\ 1 - 0.3 \\exp(-x/10000) &amp; x \\ge 0 . \\end{array} \\right.\\end{aligned}\\] Figure 5.4: Distribution Function of a Hybrid Random Variable From the graph, we can see that the inverse transform for generating random variables with this distribution function is \\[\\begin{aligned} X = F^{-1}(U) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt; U \\leq 0.7 \\\\ -1000 \\ln (\\frac{1-U}{0.3}) &amp; 0.7 &lt; U &lt; 1 . \\end{array} \\right.\\end{aligned}\\] As you have seen, for the discrete and mixed random variables, the key is to draw a graph of the distribution function that allows you to visualize potential values of the inverse function. 5.3 How Many Simulated Values? There are many topics to be described in the study of simulation (and fortunately many good sources to help you). The best way to appreciate simulation is to experience it. One topic that inevitably comes up is the number of simulated trials needed to rid yourself of sampling variability so that you may focus on patterns of interest. How many simulated values are recommended? 100? 1,000,000? We can use the central limit theorem to respond to this question. Suppose that we wish to use simulation to calculate \\(\\mathrm{E~}h(X)\\), where \\(h(\\cdot)\\) is some known function. Then, based on \\(R\\) simulations (replications), we get $ X_1,,X_R$. From this simulated sample, we calculate a sample average \\[\\overline{h}_R=\\frac{1}{R}\\sum_{i=1}^{R} h(X_i)\\] and a sample standard deviation \\[s_{h,R}^2 = \\frac{1}{R} \\sum_{i=1}^{R}\\left( h(X_i) -\\overline{h}_R \\right) ^2.\\] So, \\(\\overline{h}_R\\) is your best estimate of \\(\\mathrm{E~}h(X)\\) and \\(s_{h,R}^2\\) provides an indication of the uncertainty of your estimate. As one criterion for your confidence in the result, suppose that you wish to be within 1% of the mean with 95% certainty. According to the central limit theorem, your estimate should be approximately normally distributed. Thus, you should continue your simulation until \\[\\frac{.01\\overline{h}_R}{s_{h,R}/\\sqrt{R}}\\geq 1.96\\] or equivalently \\[R \\geq 38,416\\frac{s_{h,R}^2}{\\overline{h}_R^2}.\\] This criterion is a direct application of the approximate normality (recall that 1.96 is the 97.5th percentile of the standard normal curve). Note that \\(\\overline{h}_R\\) and \\(s_{h,R}\\) are not known in advance, so you will have to come up with estimates as you go (sequentially), either by doing a little pilot study in advance or by interrupting your procedure intermittently to see if the criterion is satisfied. "],
["portfolio-management-including-reinsurance.html", "Chapter 6 Portfolio Management including Reinsurance 6.1 Tails of Distributions 6.2 Measures of Risk 6.3 Reinsurance", " Chapter 6 Portfolio Management including Reinsurance 6.0.1 Overview: Define \\(S\\) to be (random) obligations that arise from a collection (portfolio) of insurance contracts We are particularly interested in probabilities of large outcomes and so formalize the notion of a heavy-tail distribution How much in assets does an insurer need to retain to meet obligations arising from the random \\(S\\)? A study of risk measures helps to address this question As with policyholders, insurers also seek mechanisms in order to spread risks. A company that sells insurance to an insurance company is known as a reinsurer 6.1 Tails of Distributions The tail of a distribution (more specifically: the right tail) is the portion of the distribution corresponding to large values of the r.v. Understanding large possible loss values is important because they have the greatest impact on the total of losses. R.v.’s that tend to assign higher probabilities to larger values are said to be heavier tailed. When choosing models, tail weight can help narrow choices or can confirm a choice for a model. 6.1.0.1 Classification Based on Moments One way of classifying distributions: are all moments finite, or not? The finiteness of all positive moments indicates a (relatively) light right tail. The finiteness of only positive moments up to a certain value indicates a heavy right tail. KPW Example 3.9: demonstrate that for the gamma distribution all positive moments are finite but for the Pareto distribution they are not. For the gamma distribution \\[\\begin{aligned} \\mu_k^{&#39;} &amp;= \\int_0^{\\infty} x^k \\frac{x^{\\alpha-1} e^{-x/\\theta}}{\\Gamma(\\alpha) \\theta^{\\alpha}} dx \\\\ &amp;= \\int_0^{\\infty} (y\\theta)^k \\frac{(y\\theta)^{\\alpha-1} e^{-y}}{\\Gamma(\\alpha) \\theta^{\\alpha}} \\theta dy \\\\ &amp;= \\frac{\\theta^k}{\\Gamma(\\alpha)} \\Gamma(\\alpha+k) &lt; \\infty \\ \\ \\ \\text{for\\ all}\\ k&gt;0.\\end{aligned}\\] KPW Example 3.9: demonstrate that for the gamma distribution all positive moments exist but for the Pareto distribution they do not. For the Pareto distribution \\[\\begin{aligned} \\mu_k^{&#39;} &amp;= \\int_0^{\\infty} x^k \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} dx \\\\ &amp;= \\int_{\\theta}^{\\infty} (y-\\theta)^k \\frac{\\alpha \\theta^{\\alpha}}{y^{\\alpha+1}} dy \\\\ &amp;= \\alpha \\cdot \\theta^{\\alpha} \\int_{\\theta}^{\\infty} \\sum_{j=0}^k \\left(\\begin{array}{c} k \\\\ j \\end{array} \\right) y^{j-\\alpha-1} (-\\theta)^{k-j} dy,\\end{aligned}\\] for integer values of \\(k\\). This integral is finite only if \\(\\int_{\\theta}^{\\infty} y^{j-\\alpha-1} dy = \\frac{y^{j-\\alpha}}{j-\\alpha}\\big{|}_{\\theta}^{\\infty}\\) is finite. Finiteness occurs when \\(j-\\alpha &lt; 0\\) for \\(j=1, \\ldots,k\\). Or, equivalently, \\(k&lt; \\alpha\\). Pareto is said to have a heavy tail, and gamma has a light tail. 6.1.0.2 Comparison Based on Limiting Tail Behavior Consider two distributions with the same mean. If ratio of \\(S_1(.)\\) and \\(S_2(.)\\) diverges to infinity, then distribution 1 has a heavier tail than distribution 2. Thus, we examine \\[\\begin{aligned} \\lim_{x\\to \\infty} \\frac{S_1(x)}{S_2(x)} &amp;= \\lim_{x \\to \\infty} \\frac{S_1^{&#39;}(x)}{S_2^{&#39;}(x)} \\\\ &amp;= \\lim_{x \\to \\infty} \\frac{-f_1(x)}{-f_2(x)} = \\lim_{x\\to \\infty} \\frac{f_1(x)}{f_2(x)}.\\end{aligned}\\] KPW Example 3.10: demonstrate that Pareto distribution has a heavier tail than the gamma distribution using the limit of the ratio of their density functions. We consider \\[\\begin{aligned} \\lim_{x\\to \\infty} \\frac{f_{\\text{Pareto}}(x)}{f_{\\text{gamma}}(x)} &amp;= \\lim_{x \\to \\infty} \\frac{\\alpha \\theta^{\\alpha} (x+ \\theta)^{-\\alpha-1}}{x^{\\tau-1} e^{-x/\\lambda} \\lambda^{-\\tau} \\Gamma(\\tau)^{-1}} \\\\ &amp;= c \\lim_{x\\to \\infty} \\frac{e^{x/\\lambda}}{(x+\\theta)^{\\alpha+1} x^{\\tau-1}} \\\\ &amp;= \\infty\\end{aligned}\\] Exponentials go to infinity faster than polynomials, thus the limit is infinity. 6.2 Measures of Risk A risk measure is a mapping from the r.v. representing the loss associated with the risks to the real line. A risk measure gives a single number that is intended to quantify the risk. For example, the standard deviation is a risk measure. Notation: \\(\\rho(X)\\). We briefly mention: VaR: Value at Risk; TVaR: Tail Value at Risk. 6.2.0.1 Value at Risk Say \\(F_X(x)\\) represents the cdf of outcomes over a fixed period of time, e.g. one year, of a portfolio of risks. We consider positive values of \\(X\\) as losses. Definition 3.11: let \\(X\\) denote a loss r.v., then the Value-at-Risk of \\(X\\) at the \\(100p\\%\\) level, denoted \\(VaR_p(X)\\) or \\(\\pi_p\\), is the \\(100p\\) percentile (or quantile) of the distribution of \\(X\\). E.g. for continuous distributions we have \\[\\begin{aligned} P(X&gt; \\pi_p) &amp;= 1-p.\\end{aligned}\\] VaR has become the standard risk measure used to evaluate exposure to risk. VaR is the amount of capital required to ensure, with a high degree of certainty, that the enterprise does not become technically insolvent. Which degree of certainty? 95\\(\\%\\)? in Solvency II \\(99.5\\%\\) (or: ruin probability of 1 in 200). VaR is not subadditive. Subadditivity of a risk measure \\(\\rho(.)\\) requires \\[\\begin{aligned} \\rho(X+Y) \\leq \\rho(X)+\\rho(Y).\\end{aligned}\\] Intuition behind subadditivity: combining risks is less riskier than holding them separately. Example: let \\(X\\) and \\(Y\\) be i.i.d. r.v.’s which are \\(\\text{Bern}(0.02)\\) distributed. Then, \\(P(X\\leq 0) = 0.98\\) and \\(P(Y\\leq 0)=0.98\\). Thus, \\(F_X^{-1}(0.975)=F_Y^{-1}(0.975)=0\\). For the sum, \\(X+Y\\), we have \\(P[X+Y=0]=0.98 \\cdot 0.98=0.9604\\). Thus, \\(F_{X+Y}^{-1}(0.975)&gt;0\\). VaR is not subadditive, since \\(\\text{VaR}(X+Y)\\) in this case is larger than \\(\\text{VaR}(X)+\\text{VaR}(Y)\\). Another drawback of VaR: it is a single quantile risk measure of a predetermined level \\(p\\); no information about the thickness of the upper tail of the distribution function from \\(\\text{VaR}_p\\) on; whereas stakeholders are interested in both frequency and severity of default. Therefore: study other risk measures, e.g. Tail Value at Risk (TVaR). 6.2.0.2 Tail Value at Risk Definition 3.12: let \\(X\\) denote a loss r.v., then the Tail Value at Risk of \\(X\\) at the \\(100p\\%\\) security level, \\(\\text{TVaR}(p)\\), is the expected loss given that the loss exceeds the \\(100p\\) percentile (or: quantile) of the distribution of \\(X\\). We have (assume continuous distribution) \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= E(X|X&gt;\\pi_p) \\\\ &amp;= \\frac{\\int_{\\pi_p}^{\\infty} x\\cdot f(x) dx}{1-F(\\pi_p)}.\\end{aligned}\\] We can rewrite this as the usual definition of TVaR \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= \\frac{\\int_{\\pi_p}^{\\infty} x dF_X(x)}{1-p} \\\\ &amp;= \\frac{\\int_p^1 \\text{VaR}_u(X) du}{1-p},\\end{aligned}\\] using the substitution \\(F_X(x) = u\\) and thus \\(x=F_X^{-1}(u)\\). From the definition \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= \\frac{\\int_p^1 \\text{VaR}_u(X) du}{1-p},\\end{aligned}\\] we understand TVaR is the arithmetic average of the quantiles of \\(X\\), from level \\(p\\) on; TVaR is averaging high level VaR; TVaR tells us much more about the tail of the distribution than does VaR alone. Finally, TVaR can also be written as \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= E(X|X&gt;\\pi_p) \\\\ &amp;= \\frac{\\int_{\\pi_p}^{\\infty} x f(x)dx}{1-p} \\\\ &amp;= \\pi_p + \\frac{\\int_{\\pi_p}^{\\infty} (x-\\pi_p) f(x) dx}{1-p} \\\\ &amp;= \\text{VaR}_p(X) + e(\\pi_p),\\end{aligned}\\] with \\(e(\\pi_p)\\) the mean excess loss function evaluated at the \\(100p\\)th percentile. We can understand these connections as follows. (Assume continuous r.v.’s) The relation \\[\\begin{aligned} \\text{CTE}_p(X) &amp;= \\text{TVaR}_{F_X(\\pi_p)}(X),\\end{aligned}\\] then follows immediately by combining the other two expressions. TVaR is a coherent risk measure, see e.g. Foundations of Risk Measurement course. Thus, \\(\\text{TVaR}(X+Y) \\leq \\text{TVaR}(X)+\\text{TVaR}(Y)\\). When using this risk measure, we never encounter a situation where combining risks is viewed as being riskier than keeping them separate. KPW Example 3.18 (Tail comparisons) Consider three loss distributions for an insurance company. Losses for the next year are estimated to be on average 100 million with standard deviation 223.607 million. You are interested in finding high quantiles of the distribution of losses. Using the normal, Pareto, and Weibull distributions, obtain the VaR at the 90%, 99%, and 99.99% security levels. Solution Normal distribution has a lighter tail than the others, and thus smaller quantiles. Pareto and Weibull with \\(\\tau&lt;1\\) have heavy tails, and thus relatively larger extreme quantiles. Example 3.18 (Tail comparisons) Consider three loss distributions for an insurance company. Losses for the next year are estimated to be on average 100 million with standard deviation 223.607 million. You are interested in finding high quantiles of the distribution of losses. Using the normal, Pareto, and Weibull distributions, obtain the VaR at the 99%, 99.9%, and 99.99% security levels. &gt; qnorm(c(0.9,0.99,0.999),mu,sigma) [1] 386.5639 620.1877 790.9976 &gt; qpareto(c(0.9,0.99,0.999),alpha,s) [1] 226.7830 796.4362 2227.3411 &gt; qweibull(c(0.9,0.99,0.999),tau,theta) [1] 265.0949 1060.3796 2385.8541 We learn from Example 3.18 that results vary widely depending on the choice of distribution. Thus, the selection of an appropriate loss model is highly important. To obtain numerical values of VaR or TVaR: estimate from the data directly; or use distributional formulas, and plug in parameter estimates. When estimating VaR directly from the data: use R to get quantile from the empirical distribution; R has 9 ways to estimate a VaR at level \\(p\\) from a sample of size \\(n\\), differing in the way the interpolation between order statistics close to \\(np\\) . When estimating TVaR directly from the data: take average of all observations that exceed the threshold (i.e.\\(\\pi_p\\)); Caution: we need a large number of observations (and a large number of observations \\(&gt; \\pi_p\\)) in order to get reliable estimates. When not may observations in excess of the threshold are available: construct a loss model; calculate values of VaR and TVaR directly from the fitted distribution. For example \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= E(X|X&gt;\\pi_p) \\\\ &amp;= \\pi_p + \\frac{\\int_{\\pi_p}^{\\infty} (x-\\pi_p) f(x) dx}{1-p} \\\\ &amp;= \\pi_p + \\frac{\\int_{-\\infty}^{\\infty} (x-\\pi_p) f(x) dx -\\int_{-\\infty}^{\\pi_p} (x-\\pi_p) f(x) dx }{1-p} \\\\ &amp;= \\pi_p + \\frac{E(X)-\\int_{-\\infty}^{\\pi_p} xf(x) dx -\\pi_p (1-F(\\pi_p))}{1-p} \\\\ &amp;= \\pi_p + \\frac{E(X) - E[\\min{(X,\\pi_p)}]}{1-p} = \\pi_p + \\frac{E(X)-E(X \\wedge \\pi_p)}{1-p},\\end{aligned}\\] see Appendix A for those expressions. 6.3 Reinsurance A major difference between reinsurance and primary insurance is that a reinsurance program is generally tailored more closely to the buyer There are two major types of reinsurance Proportional Excess of Loss A proportional treaty is an agreement between a reinsurer and a ceding company (the reinsured) in which the reinsurer assumes a given percent of losses and premium. 6.3.1 Proportional Reinsurance A proportional treaty is an agreement between a reinsurer and a ceding company (the reinsured) in which the reinsurer assumes a given percent of losses and premium. The simplest example of a proportional treaty is called Quota Share. In a quota share treaty, the reinsurer receives a flat percent, say 50%, of the premium for the book of business reinsured. In exchange, the reinsurer pays 50% of losses, including allocated loss adjustment expenses The reinsurer also pays the ceding company a ceding commission which is designed to reflect the differences in underwriting expenses incurred. The amounts paid by the direct insurer and the reinsurer are defined as follows: \\[\\begin{equation*} Y_{insurer} = \\begin{cases} X &amp; X \\le M\\\\ M &amp; X &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\min(X,M) = X \\wedge M \\end{equation*}\\] \\[\\begin{equation*} Y_{reinsurer} = \\begin{cases} 0 &amp; X \\le M\\\\ X- M &amp; X &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\max(0,X-M) \\end{equation*}\\] Note that \\(Y_{insurer}+Y_{reinsurer}=X\\). 6.3.2 Surplus Share Proportional Treaty Another proportional treaty is known as Surplus Share; these are common in property business. A surplus share treaty allows the reinsured to limit its exposure on any one risk to a given amount (the retained line). The reinsurer assumes a part of the risk in proportion to the amount that the insured value exceeds the retained line, up to a given limit (expressed as a multiple of the retained line, or number of lines). For example, let the retained Line be $100,000 and let the given limit be 4 lines ($400,000). Then, if \\(X\\) is the loss, the reinsurer’s portion is \\(\\min(400000, (X-100000)_+)\\). 6.3.3 Excess of Loss Reinsurance Under this arrangement, the direct insurer sets a retention level \\(M (&gt;0)\\) and pays in full any claim for which \\(X \\le M\\). The direct insurer retains an amount \\(M\\) of the risk. For claims for which \\(X &gt; M\\), the direct insurer pays \\(M\\) and the reinsurer pays the remaining amount \\(X-M\\). The amounts paid by the direct insurer and the reinsurer are defined as follows. \\[\\begin{equation*} Y_{insurer} = c X \\ \\ \\ \\ \\ Y_{reinsurer} = (1-c) X \\end{equation*}\\] Note that \\(Y_{insurer}+Y_{reinsurer}=X\\). 6.3.4 Relations with Personal Insurance We have already seen the needed tools to handle reinsurance in the context of personal insurance For a proportional reinsurance, the transformation $Y_{insurer} = c X $ is the same as a coinsurance adjustment in personal insurance For excess of loss reinsurance, the transformation \\(Y_{reinsurer} = \\max(0,X-M)\\) is the same as an insurer’s payment with a deductible \\(M\\) and \\(Y_{insurer} = \\min(X,M) = X \\wedge M\\) is equivalent to what a policyholder pays with deductible \\(M\\). Reinsurance applications suggest introducing layers of coverage, a (small) mathematical extension. 6.3.5 Layers of Coverage Instead of simply an insurer and reinsurer or an insurer and a policyholder, think about the situation with all three parties, a policyholder, insurer, and reinsurer, who agree on how to share a risk. In general, we consider \\(k\\) parties. If \\(k=4\\), it could be an insurer and three different reinsurers. Consider a simple example: Suppose that there are \\(k=3\\) parties. The first party is responsible for the first 100 of claims, the second responsible for claims from 100 to 3000, and the third responsible for claims above 3000. If there are four claims in the amounts 50, 600, 1800 and 4000, they would be allocated to the parties as follows: Layer Claim 1 Claim 2 Claim 3 Claim 4 Total (0, 100] 50 100 100 100 350 (100, 3000] 0 500 1700 2900 5100 (3000, \\(\\infty\\)) 0 0 0 1000 1000 Total 50 600 1800 4000 6450 Mathematically, partition the positive real line into \\(k\\) intervals using the cut-points \\(0 = c_0 &lt; c_1 &lt; \\cdots &lt; c_{k-1} &lt; c_k = \\infty\\). The \\(j\\)th interval is \\((c_{j-1}, c_j]\\). Let \\(Y_j\\) be the amount of risk shared by the \\(j\\)th party To illustrate, if a loss \\(x\\) is such that \\(c_{j-1} &lt;x \\le c_j\\), then \\[\\left(\\begin{array}{c} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_j \\\\Y_{j+1} \\\\ \\vdots \\\\Y_k \\end{array}\\right) =\\left(\\begin{array}{c} c_1-c_0 \\\\ c_2-c_1 \\\\ \\vdots \\\\ x-c_{j-1} \\\\ 0 \\\\ \\vdots \\\\0 \\end{array}\\right)\\] More succinctly, we can write \\[Y_j = \\min(X,c_j) - \\min(X,c_{j-1})\\] With the expression \\(Y_j = min(X,c_j) - min(X,c_{j-1})\\), we see that the \\(j\\)th party is responsible for claims in the interval \\((c_{j-1}, c_j]\\) Note that \\(X = Y_1 + Y_2 + \\cdots + Y_k\\) The parties need not be different. For example, suppose that a policyholder is responsible for the first 500 of claims and all claims in excess of 100,000. The insurer takes claims between 100 and 100,000. Then, we would use \\(c_1 = 100\\), \\(c_2 =100000\\). The policyholder is responsible for \\(Y_1 =\\min(X,100)\\) and \\(Y_3 = X - \\min(X,100000) = \\max(0, X-100000)\\). See the Wisconsin Property Fund site for more info on layers of reinsurance, https://sites.google.com/a/wisc.edu/local-government-property-insurance-fund/home/reinsurance "],
["technical-supplement-statistical-inference.html", "Chapter 7 Technical Supplement: Statistical Inference 7.1 Overview of Statistical Inference 7.2 Estimation and Prediction 7.3 Maximum Likelihood Theory", " Chapter 7 Technical Supplement: Statistical Inference 7.1 Overview of Statistical Inference A set of data (a sample) has been collected that is considered representative of a larger set (the population). This relationship is known as the sampling frame. Often, we can describe the distribution of the population in terms of a limited (finite) number of terms called parameters. These are referred to as parametric distributions. With nonparametric analysis, we do not limit ourselves to only a few parameters. The statistical inference goal is to say something about the (larger) population based on the observed sample (we “infer,” not “deduce”). There are three types of statements: Estimation Hypothesis Testing Prediction 7.1.0.1 Wisconsin Property Fund Discuss ideas of statistical inference in the context of a sample from the Wisconsin Property Fund Specifically, consider 1,377 individual claims from 2010 experience (slightly different from the analysis of 403 average claims in Chapter 1) First Third Standard Minimum Quartile Median Mean Quartile Maximum Deviation Claims 1 788 2,250 26,620 6,171 12,920,000 368,030 Logarithmic Claims 0 6.670 7.719 7.804 8.728 16.370 1.683 ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE) ClaimLevBC10&lt;-subset(ClaimLev,Year==2010); nrow(ClaimLevBC10) ## [1] 1377 par(mfrow=c(1, 2)) hist(ClaimLevBC10$Claim, main=&quot;&quot;, xlab=&quot;Claims&quot;) hist(log(ClaimLevBC10$Claim), main=&quot;&quot;, xlab=&quot;Logarithmic Claims&quot;) Figure 7.1: Distribution of Claims 7.1.0.2 Sampling Frame In statistics, a sampling frame error occurs when the sampling frame, the list from which the sample is drawn, is not an adequate approximation of the population of interest. For the property fund example, the sample consists of all 2010 claims The population might be all claims that could have potentially occurred in 2010. Or, it might be all claims that could potentially occur, such as in 2010, 2011, and so forth A sample must be a representative subset of a population, or “universe,” of interest. If the sample is not representative, taking a larger sample does not eliminate bias; you simply repeat the same mistake over again and again. A sample should be a representative subset of a population, or “universe,” of interest. Formally We assume that the random variable \\(X\\) represents a draw from a population with distribution function F(.) We make several such draws (\\(n\\)), each unrelated to one another (statistically independent) Sometimes we say that \\(X_1, \\ldots, X_n\\) is a random sample (with replacement) from F(.) Sometimes we say that \\(X_1, \\ldots, X_n\\) are identically and independently distributed (\\(iid\\)) 7.1.0.3 Describing the Population We think of the random variable \\(X\\) as a draw from the population with distribution function F(.) There are several ways to summarize F(.). We might consider the mean, standard deviation, 95th percentile, and so on. Because these summary stats do not depend on a specific parametric reference, they are nonparametric summary measures. In contrast, we can think of logarithmic claims as normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), that is, claims have a lognormal distribution We will also look at the gamma distribution, with parameters \\(\\alpha\\) and \\(\\theta\\), as a claims model The normal, lognormal, and gamma are examples of parametric distributions. The quantities \\(\\mu\\), \\(\\sigma\\), \\(\\alpha\\), and \\(\\theta\\) are known as parameters. When we know the parameters of a distribution family, then we have knowledge of the entire distribution. 7.2 Estimation and Prediction 7.2.0.1 Estimation Use \\(\\theta\\) to denote a summary of the population. Parametric - It can be a parameter from a distribution such as \\(\\mu\\) or \\(\\sigma\\). Nonparametric - It can also be a nonparametric summary such as the mean or standard deviation. Let \\(\\hat{\\theta} =\\hat{\\theta}(X_1, \\ldots, X_n)\\) be a function of the sample that provides proxy, or estimate, of \\(\\theta\\). It is a function of the sample \\(X_1, \\ldots, X_n\\). In our property fund case, 7.804 is a (nonparametric) estimate of the population expected logarithmic claim and 1.683 is an estimate of the corresponding standard deviation. These are (parametric) estimates of the normal distribution for logarithmic claims The estimate of the expected claim using the lognormal distribution is 10,106.8 (=\\(\\exp(7.804+1.683^2/2))\\). 7.2.0.2 Lognormal Distribution and Estimation Assume that claims follow a lognormal distribution, so that logarithmic claims follow the familiar normal distribution. Specifically, assume \\(\\ln X\\) has a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), sometimes denoted as \\(X \\sim N(\\mu, \\sigma^2)\\). For the property data, estimates are \\(\\hat{\\mu} =7.804\\) and \\(\\hat{\\sigma} = 1.683\\). The “hat” notation is common. These are said to be point estimates, a single approximation of the corresponding parameter. Under general maximum likelihood theory (that we will do in a little bit), these estimates typically have a normal distribution for large samples. Using notation, \\(\\hat{\\theta}\\) has an approximate normal distribution with mean \\(\\theta\\) and variance, say, \\(\\mathrm{Var}(\\hat{\\theta})\\). Take the square root of the variance and plug-in the estimate to define \\(se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}\\). A standard error is an estimated standard deviation. The next step in the mathematical statistics theory is to establish that \\((\\hat{\\theta}-\\theta)/se(\\hat{\\theta})\\) has a \\(t\\)-distribution with “degrees of freedom” (a parameter of the distribution) equal to the sample size minus the dimension of \\(\\theta\\). Assume that claims follow a lognormal distribution, so that logarithmic claims follow the familiar normal distribution. Under general maximum likelihood theory \\(\\hat{\\theta}\\) has an approximate normal distribution with mean \\(\\theta\\) and variance, say, \\(\\mathrm{Var}(\\hat{\\theta})\\). Take the square root of the variance and plug-in the estimate to define \\(se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}\\). A standard error is an estimated standard deviation. \\((\\hat{\\theta}-\\theta)/se(\\hat{\\theta})\\) has a \\(t\\)-distribution with “degrees of freedom” (a parameter of the distribution) equal to the sample size minus the dimension of \\(\\theta\\). As an application, we can invert this result to get a confidence interval for \\(\\theta\\). A pair of statistics, \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\), provide an interval of the form \\([\\hat{\\theta}_1, \\hat{\\theta}_2]\\) This interval is a \\(1-\\alpha\\) confidence interval for \\(\\theta\\) if \\(\\Pr\\left(\\hat{\\theta}_1 \\le \\theta \\le \\hat{\\theta}_2\\right) \\ge 1-\\alpha.\\) For example, \\(\\hat{\\theta}_1 = \\hat{\\mu} - (t-value) \\hat{\\sigma}/\\sqrt{n}\\) and \\(\\hat{\\theta}_2 = \\hat{\\mu} + (t-value) \\hat{\\sigma}/\\sqrt{n}\\) provide a confidence interval for \\(\\theta=\\mu\\). When \\(\\alpha = 0.05\\), \\(t-value \\approx 1.96\\). For the property fund, (7.715235, 7.893208) is a 95% confidence interval for \\(\\mu\\). 7.2.0.3 Lognormal Distribution and Hypothesis Testing An important statistical inference procedure involves verifying ideas about parameters. To illustrate, in the property fund, assume that mean logarithmic claims have historically been approximately been \\(\\mu_0 = log(5000)= 8.517\\). I might want to use 2010 data to see whether the mean of the distribution has changed. I also might want to test whether it has increased. The actual 2010 average was \\(\\hat{\\mu} =7.804\\). Is this a significant departure from \\(\\mu_0 = 8.517\\)? One way to think about it is in terms of standard errors. The deviation is \\((8.517-7.804)/(1.683/\\sqrt{1377}) = 15.72\\) standard errors. This is highly unlikely assuming an approximate normal distribution. One hypothesis testing procedure begin with the calculation the test statistic \\(t-stat=(\\hat{\\theta}-\\theta_0)/se(\\hat{\\theta})\\). Here, \\(\\theta_0\\) is an assumed value of the parameter. Then, one rejects the hypothesized value if the test statistic \\(t-stat\\) is “unusual.” To gauge “unusual,” use the same \\(t\\)-distribution as introduced for confidence intervals. If you only want to know about a difference, this is known as a “two-sided” test; use the same \\(t-value\\) as the case for confidence intervals. If you want to investigate whether there has been an increase (or decrease), then use a “one-sided” test. Another useful concept in hypothesis testing is the \\(p\\)-value, which is short hand for probability value. For a data set, a \\(p\\)-value is defined to be the smallest significance level for which the null hypothesis would be rejected. 7.2.0.4 Property Fund – Other Distributions For numerical stability and extensions to regression applications, statistical packages often work with transformed version of parameters The following estimates are from the R package VGAM (the function) Distribution Parameter Standard \\(t\\)-stat Estimate Error Gamma 10.190 0.050 203.831 -1.236 0.030 -41.180 Lognormal 7.804 0.045 172.089 0.520 0.019 27.303 Pareto 7.733 0.093 82.853 -0.001 0.054 -0.016 GB2 2.831 1.000 2.832 1.203 0.292 4.120 6.329 0.390 16.220 1.295 0.219 5.910 7.3 Maximum Likelihood Theory 7.3.1 Likelihood Function Let \\(\\mathrm{f}(\\cdot;\\boldsymbol\\theta)\\) be the probability mass function if \\(X\\) is discrete or the probability density function if it is continuous. The likelihood is a function of the parameters (\\(\\boldsymbol \\theta\\)) with the data (\\(\\mathbf{x}\\)) fixed rather than a function of the data with the parameters fixed. Define the log-likelihood function, \\[L(\\boldsymbol \\theta) = L(\\mathbf{x};\\boldsymbol \\theta ) = \\ln \\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta) = \\sum_{i=1}^n \\ln \\mathrm{f}(x_i;\\boldsymbol \\theta),\\] evaluated at a realization \\(\\mathbf{x}\\). In the case of independence, the joint density function can be expressed as a product of the marginal density functions and, by taking logarithms, we can work with sums. 7.3.1.1 Example. Pareto Distribution Suppose that \\(X_1, \\ldots, X_n\\) represent a random sample from a single-parameter Pareto with cumulative distribution function: \\[\\mathrm{F}(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500 .\\] In this case, the single parameter is \\(\\theta = \\alpha\\). The corresponding probability density function is \\(\\mathrm{f}(x) = 500^{\\alpha} \\alpha x^{-\\alpha-1}\\) and the logarithmic likelihood is \\[L(\\boldsymbol \\alpha) = \\sum_{i=1}^n \\ln \\mathrm{f}(x_i;\\alpha) = n \\alpha \\ln 500 +n \\ln \\alpha -(\\alpha+1) \\sum_{i=1}^n \\ln x_i .\\] 7.3.1.2 Properties of Likelihood Functions One basic property of likelihood functions is: \\[\\label{E11:ScoreZero} \\mathrm{E} \\left( \\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) \\right) = \\mathbf 0\\] The derivative of the log-likelihood function, \\(\\partial L(\\boldsymbol \\theta)/\\partial \\boldsymbol \\theta\\), is called the score function. To see this, \\[\\begin{aligned} \\mathrm{E} \\left( \\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) \\right) &amp;= \\mathrm{E} \\left( \\frac{\\frac{\\partial}{\\partial \\boldsymbol \\theta}\\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta)}{\\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta )} \\right) = \\int\\frac{\\partial}{\\partial \\boldsymbol \\theta} \\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta ) d \\mathbf y \\\\ &amp;= \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\int \\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta ) d \\mathbf y = \\frac{\\partial}{\\partial \\boldsymbol \\theta} 1 = \\mathbf 0.\\end{aligned}\\] Another basic property is: \\[ \\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right) + \\mathrm{E} \\left( \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta} \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = \\mathbf 0.\\] With this, we can define the information matrix \\[ \\mathbf{I}(\\boldsymbol \\theta) = \\mathrm{E} \\left( \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta} \\frac{ \\partial L(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right).\\] In general \\[\\frac{ \\partial}{\\partial \\boldsymbol \\theta} L(\\boldsymbol \\theta) =\\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln \\prod_{i=1}^n \\mathrm{f}(x_i;\\boldsymbol \\theta ) =\\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln \\mathrm{f}(x_i;\\boldsymbol \\theta ).\\] has a large sample normal distribution with mean 0 and variance \\(\\mathbf{I}(\\boldsymbol \\theta)\\). 7.3.1.3 Maximum Likelihood Estimators The value of \\(\\boldsymbol \\theta\\), say \\(\\boldsymbol \\theta_{MLE}\\), that maximizes \\(\\mathrm{f}(\\mathbf{x};\\boldsymbol \\theta)\\) is called the maximum likelihood estimator. Maximum likelihood estimators are values of the parameters \\(\\boldsymbol \\theta\\) that are “most likely” to have been produced by the data. Because \\(\\ln(\\cdot)\\) is a one-to-one function, we can also determine \\(\\boldsymbol \\theta_{MLE}\\) by maximizing the log-likelihood function, \\(L(\\boldsymbol \\theta)\\). Example. Course C/Exam 4. May 2000, 21. You are given the following five observations: 521, 658, 702, 819, 1217. You use the single-parameter Pareto with cumulative distribution function: \\[\\mathrm{F}(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500 .\\] Calculate the maximum likelihood estimate of the parameter \\(\\alpha\\). 7.3.1.4 Instructor Notes Example. Course C/Exam 4. May 2000, 21. You are given the following five observations: 521, 658, 702, 819, 1217. You use the single-parameter Pareto with cumulative distribution function: \\[\\mathrm{F}(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500 .\\] Calculate the maximum likelihood estimate of the parameter \\(\\alpha\\). Solution. With \\(n=5\\), the logarithmic likelihood is \\[L(\\alpha ) = \\sum_{i=1}^5 \\ln \\mathrm{f}(x_i;\\alpha ) = 5 \\alpha \\ln 500 + 5 \\ln \\alpha -(\\alpha+1) \\sum_{i=1}^5 \\ln x_i.\\] Solving for the root of the score function yields \\[\\frac{ \\partial}{\\partial \\alpha } L(\\alpha ) = 5 \\ln 500 + 5 / \\alpha - \\sum_{i=1}^5 \\ln x_i =_{set} 0 \\Rightarrow \\alpha_{MLE} = \\frac{5}{\\sum_{i=1}^5 \\ln x_i - 5 \\ln 500 } = 2.453 .\\] 7.3.1.5 Asymptotic Normality of Maximum Likelihood Estimators Under broad conditions, \\(\\boldsymbol \\theta_{MLE}\\) has a large sample normal distribution with mean \\(\\boldsymbol \\theta\\) and variance \\(\\left( \\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1}\\). \\(2 \\left( L(\\boldsymbol \\theta_{MLE}) - L(\\boldsymbol \\theta) \\right)\\) has a chi-square distribution with degrees of freedom equal to the dimension of \\(\\boldsymbol \\theta\\) . These are critical results upon which much of estimation and hypothesis testing is based. Example. Course C/Exam 4. Nov 2000, 13. A sample of ten observations comes from a parametric family \\(f(x,; \\theta_1, \\theta_2)\\) with log-likelihood function \\[L(\\theta_1, \\theta_2)= \\sum_{i=1}^{10} f(x_i; \\theta_1, \\theta_2) = -2.5 \\theta_1^2 - 3 \\theta_1 \\theta_2 - \\theta_2^2 + 5 \\theta_1 + 2 \\theta_2 + k,\\] where \\(k\\) is a constant. Determine the estimated covariance matrix of the maximum likelihood estimator, \\(\\hat{\\theta_1}, \\hat{\\theta_2}\\). 7.3.1.6 Instructor Notes Example. Course C/Exam 4. Nov 2000, 13. A sample of ten observations comes from a parametric family \\(f(x,; \\theta_1, \\theta_2)\\) with log-likelihood function \\[L(\\theta_1, \\theta_2)= \\sum_{i=1}^{10} f(x_i; \\theta_1, \\theta_2) = -2.5 \\theta_1^2 - 3 \\theta_1 \\theta_2 - \\theta_2^2 + 5 \\theta_1 + 2 \\theta_2 + k,\\] where \\(k\\) is a constant. Determine the estimated covariance matrix of the maximum likelihood estimator, \\(\\hat{\\theta_1}, \\hat{\\theta_2}\\). Solution. The matrix of second derivatives is \\[\\left( \\begin{array}{cc} \\frac{ \\partial ^2}{\\partial \\theta_1 ^2 } L &amp; \\frac{ \\partial ^2}{\\partial \\theta_1 \\partial \\theta_2 } L \\\\ \\frac{ \\partial ^2}{\\partial \\theta_1 \\partial \\theta_2 } L &amp; \\frac{ \\partial ^2}{\\partial \\theta_1 ^2 } L \\end{array} \\right) = \\left( \\begin{array}{cc} -5 &amp; -3 \\\\ -3 &amp; -2 \\end{array} \\right)\\] Thus, the information matrix is: \\[\\mathbf{I}(\\theta_1, \\theta_2) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} L(\\boldsymbol \\theta) \\right) = \\left( \\begin{array}{cc} 5 &amp; 3 \\\\ 3 &amp; 2 \\end{array} \\right)\\] and \\[\\mathbf{I}^{-1}(\\theta_1, \\theta_2) = \\frac{1}{5(2) - 3(3)}\\left( \\begin{array}{cc} 2 &amp; -3 \\\\ -3 &amp; 5 \\end{array} \\right) = \\left( \\begin{array}{cc} 2 &amp; -3 \\\\ -3 &amp; 5 \\end{array} \\right) .\\] 7.3.1.7 Maximum Likelihood Estimation (MLE) Why use maximum likelihood estimation? General purpose tool - works in many situations (data can be censored, truncated, include covariates, time-dependent, and so forth) It is “optimal,” the best, in the sense that it has the smallest variance among the class of all unbiased estimators. (Caveat: for large sample sizes). A drawback: Generally, maximum likelihood estimators are computed iteratively, no closed-form solution. For example, you may recall a “Newton-Raphson” iterative algorithm from calculus Iterative algorithms require starting values. For some problems, the choice of a close starting value is critical. 7.3.1.8 MLE and Statistical Significance One important type inference is to say whether a parameter estimate is “statistically significant” We learned earlier that \\(\\boldsymbol \\theta_{MLE}\\) has a large sample normal distribution with mean \\(\\boldsymbol \\theta\\) and variance \\(\\left( \\mathbf{I}(\\boldsymbol \\theta) \\right)^{-1}\\). Look to the \\(j\\)th element of \\(\\boldsymbol \\theta_{MLE}\\), say \\(\\theta_{MLE,j}\\). Define \\(se(\\theta_{MLE,j})\\), the standard error (estimated standard deviation) to be square root of the \\(j\\) diagonal element of \\(\\left( \\mathbf{I}(\\boldsymbol \\theta)_{MLE} \\right)^{-1}\\). To assess the hypothesis that \\(\\theta_j\\) is 0, we look at the rescaled estimate \\(t(\\theta_{MLE,j})=\\theta_{MLE,j}/se(\\theta_{MLE,j})\\). It is said to be a \\(t\\)-statistic or \\(t\\)-ratio. Under this hypothesis, it has a \\(t\\)-distribution with degrees of freedom equal to the sample size minus the dimension of \\(\\boldsymbol \\theta_{MLE}\\). For most actuarial applications, the \\(t\\)-distribution is very close to the (standard) normal distribution. Thus, sometimes this ratio is also known a \\(z\\)-statistic or “\\(z\\)-score.” 7.3.1.9 Assessing Statistical Significance If the \\(t\\)-statistic \\(t(\\theta_{MLE,j})\\) exceeds a cut-off (in absolute value), then the \\(j\\)th variable is said to be “statistically significant.” For example, if we use a 5% significance level, then the cut-off is 1.96 using a normal distribution approximation. More generally, using a \\(100 \\alpha \\%\\) significance level, then the cut-off is a \\(100(1-\\alpha/2)\\%\\) quantile from a \\(t\\)-distribution using degrees of freedom equal to the sample size minus the dimension of \\(\\boldsymbol \\theta_{MLE}\\). Another useful concept in hypothesis testing is the \\(p\\)-value, shorthand for probability value. For a data set, a \\(p\\)-value is defined as the smallest significance level for which the null hypothesis would be rejected. The \\(p\\)-value is a useful summary statistic for the data analyst to report because it allows the reader to understand the strength of the deviation from the null hypothesis. 7.3.1.10 MLE and Model Validation Another important type inference is to select a model from two choices, where one choice is a subset of the other Suppose that we have a (large) model and determine the maximum likelihood estimator, \\(\\boldsymbol \\theta_{MLE}\\). Now assume that \\(p\\) elements in \\(\\boldsymbol \\theta\\) are equal to zero and determine the maximum likelihood estimator over the remaining set. Call this estimator \\(\\boldsymbol \\theta_{Reduced}\\) The statistic, \\(LRT= 2 \\left( L(\\boldsymbol \\theta_{MLE}) - L(\\boldsymbol \\theta_{Reduced}) \\right)\\), is called the likelihood ratio (a difference of the logs is the log of the ratio. Hence, the term “ratio.”) Under the hypothesis that the reduce model is correct, the likelihood ratio has a chi-square distribution with degrees of freedom equal to \\(p\\), the number of variables set equal to zero. This allows us to judge which of the two models is correct. If the statistic \\(LRT\\) is large relative to the chi-square distribution, then we reject the simpler, reduced, model in favor of the larger one. 7.3.2 Information Criteria These statistics can be used when comparing several alternative models that are not necessarily nested. One picks the model that minimizes the criterion. Akaike’s Information Criterion \\[AIC = -2 \\times L(\\boldsymbol \\theta_{MLE}) + 2 \\times (number~of~parameters)\\] The additional term \\(2 \\times \\text{(number of parameters)}\\) is a penalty for the complexity of the model. Other things equal, a more complex model means more parameters, resulting in a larger value of the criterion. Bayesian Information Criterion, defined as \\[BIC = -2 \\times L(\\boldsymbol \\theta_{MLE}) + (number~of~parameters) \\times \\ln (number~of~observations)\\] This measure gives greater weight to the number of parameters. Other things being equal, \\(BIC\\) will suggest a more parsimonious model than \\(AIC\\). 7.3.2.1 Property Fund Information Criteria Both the \\(AIC\\) and \\(BIC\\) statistics suggest that the GB2 is the best fitting model whereas gamma is the worst. Distribution AIC BIC Gamma 28,305.2 28,315.6 Lognormal 26,837.7 26,848.2 Pareto 26,813.3 26,823.7 GB2 26,768.1 26,789.0 7.3.2.2 Property Fund Fitted Distributions In this graph, black represents actual (smoothed) logarithmic claims Best approximated by green which is fitted GB2 Pareto (purple) and Lognormal (lightblue) are also pretty good Worst are the exponential (in red) and gamma (in dark blue) ## [1] 6258 Figure 7.2: Fitted Claims Distribution R Code for Fitted Claims Distributions # R Code to fit several claims distributions ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); nrow(ClaimLev) ClaimData&lt;-subset(ClaimLev,Year==2010); #Use &quot;VGAM&quot; library for estimation of parameters library(VGAM) fit.LN &lt;- vglm(Claim ~ 1, family=lognormal, data = ClaimData) fit.gamma &lt;- vglm(Claim ~ 1, family=gamma2, data = ClaimData) theta.gamma&lt;-exp(coef(fit.gamma)[1])/exp(coef(fit.gamma)[2]) alpha.gamma&lt;-exp(coef(fit.gamma)[2]) fit.exp &lt;- vglm(Claim ~ 1, exponential, data = ClaimData) fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc=0, data = ClaimData) ################################################### # Inference assuming a GB2 Distribution - this is more complicated # The likelihood functon of GB2 distribution (negative for optimization) likgb2 &lt;- function(param) { a1 &lt;- param[1] a2 &lt;- param[2] mu &lt;- param[3] sigma &lt;- param[4] yt &lt;- (log(ClaimData$Claim)-mu)/sigma logexpyt&lt;-ifelse(yt&gt;23,yt,log(1+exp(yt))) logdens &lt;- a1*yt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpyt -log(ClaimData$Claim) return(-sum(logdens)) } # &quot;optim&quot; is a general purpose minimization function gb2bop &lt;- optim(c(1,1,0,1),likgb2,method=c(&quot;L-BFGS-B&quot;), lower=c(0.01,0.01,-500,0.01),upper=c(500,500,500,500),hessian=TRUE) ################################################### # Plotting the fit using densities (on a logarithmic scale) plot(density(log(ClaimData$Claim)), ylim=c(0,0.36),main=&quot;&quot;, xlab=&quot;Log Expenditures&quot;) x &lt;- seq(0,15,by=0.01) fexp_ex = dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1)*exp(x) lines(x,fexp_ex, col=&quot;red&quot;) fgamma_ex = dgamma(exp(x), shape = alpha.gamma, scale=theta.gamma)*exp(x) lines(x,fgamma_ex,col=&quot;blue&quot;) fpareto_ex = dparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1]))*exp(x) lines(x,fpareto_ex,col=&quot;purple&quot;) flnorm_ex = dlnorm(exp(x), mean = coef(fit.LN)[1], sd = exp(coef(fit.LN)[2]))*exp(x) lines(x,flnorm_ex, col=&quot;lightblue&quot;) # density for GB II gb2density &lt;- function(x){ a1 &lt;- gb2bop$par[1] a2 &lt;- gb2bop$par[2] mu &lt;- gb2bop$par[3] sigma &lt;- gb2bop$par[4] xt &lt;- (log(x)-mu)/sigma logexpxt&lt;-ifelse(xt&gt;23,yt,log(1+exp(xt))) logdens &lt;- a1*xt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpxt -log(x) exp(logdens) } fGB2_ex = gb2density(exp(x))*exp(x) lines(x,fGB2_ex, col=&quot;green&quot;) "],
["bibliography.html", "Bibliography", " Bibliography "]
]
