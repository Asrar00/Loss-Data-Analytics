[
["index.html", "Loss Data Analytics Preface", " Loss Data Analytics An open text authored by the Actuarial Community 2018-05-16 Preface Book Description Loss Data Analytics is an interactive, online, freely available text. The online version contains many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. A subset of the book is available for offline reading in pdf and EPUB formats. The online text will be available in multiple languages to promote access to a worldwide audience. What will success look like? The online text will be freely available to a worldwide audience. The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. Moreover, a subset of the book will be available in pdf format for low-cost printing. The online text will be available in multiple languages to promote access to a worldwide audience. How will the text be used? This book will be useful in actuarial curricula worldwide. It will cover the loss data learning objectives of the major actuarial organizations. Thus, it will be suitable for classroom use at universities as well as for use by independent learners seeking to pass professional actuarial examinations. Moreover, the text will also be useful for the continuing professional development of actuaries and other professionals in insurance and related financial risk management industries. Why is this good for the profession? An online text is a type of open educational resource (OER). One important benefit of an OER is that it equalizes access to knowledge, thus permitting a broader community to learn about the actuarial profession. Moreover, it has the capacity to engage viewers through active learning that deepens the learning process, producing analysts more capable of solid actuarial work. Why is this good for students and teachers and others involved in the learning process? Cost is often cited as an important factor for students and teachers in textbook selection (see a recent post on the $400 textbook). Students will also appreciate the ability to “carry the book around” on their mobile devices. Why loss data analytics? Although the intent is that this type of resource will eventually permeate throughout the actuarial curriculum, one has to start somewhere. Given the dramatic changes in the way that actuaries treat data, loss data seems like a natural place to start. The idea behind the name loss data analytics is to integrate classical loss data models from applied probability with modern analytic tools. In particular, we seek to recognize that big data (including social media and usage based insurance) are here and high speed computation s readily available. Project Goal The project goal is to have the actuarial community author our textbooks in a collaborative fashion. To get involved, please visit our Loss Data Analytics Project Site. "],
["contributor-list.html", "Contributor List", " Contributor List Zeinab Amin American University in Cairo Katrien Antonio, KU Leuven Jan Beirlant, KU Leuven Carolina Castro - University of Buenos Aires Gary Dean, Ball State University Edward W. (Jed) Frees is an emeritus professor, formerly the Hickman-Larson Chair of Actuarial Science at the University of Wisconsin-Madison. He is a Fellow of both the Society of Actuaries and the American Statistical Association. He has published extensively (a four-time winner of the Halmstad and Prize for best paper published in the actuarial literature) and has written three books. He also is a co-editor of the two-volume series Predictive Modeling Applications in Actuarial Science published by Cambridge University Press. Guojun Gan - University of Connecticut Lisa Gao is a doctoral student at the University of Wisconsin-Madison. José Garrido, Concordia University Noriszura Ismail, University Kebangsaan Malaysia Joseph Kim, Yonsei University Shyamalkumar Nariankadu - University of Iowa Nii-Armah Okine is a doctoral student at the University of Wisconsin-Madison. Margie Rosenberg - University of Wisconsin Emine Selin Sarıdaş, Mimar Sinan University Peng Shi - University of Wisconsin Jianxi Su, Purdue University Tim Verdonck, KU Leuven Krupa Viswanathan - Temple University "],
["acknowledgements.html", "Acknowledgements Reviewer Acknowledgment", " Acknowledgements Edward Frees acknowledges the John and Anne Oros Distinguished Chair for Inspired Learning in Business which provided seed money to support the project. Frees and his Wisconsin colleagues also acknowledge a Society of Actuaries Center of Excellence Grant that provided funding to support work in dependence modeling and health initiatives. We acknowledge the Society of Actuaries for permission to use problems from their examinations. We also wish to acknowledge the support and sponsorship of the International Association of Black Actuaries in our joint efforts to provide actuarial educational content to all. Reviewer Acknowledgment Hirokazu (Iwahiro) Iwasawa "],
["C-Intro.html", "Chapter 1 Introduction to Loss Data Analytics 1.1 Relevance of Analytics 1.2 Insurance Company Operations 1.3 Case Study: Wisconsin Property Fund 1.4 Contributors and Further Resources", " Chapter 1 Introduction to Loss Data Analytics Chapter Preview. This book introduces readers to methods of analyzing insurance data. Section 1.1 begins with a discussion of why the use of data is important in the insurance industry. Although obvious, the importance of data is critical - it is the whole premise of the book. Next, Section 1.2 gives a general overview of the purposes of analyzing insurance data which is reinforced in the Section 1.3 case study. Naturally, there is a huge gap between these broads goals and a case study application; this gap is covered through the methods and techniques of data analysis covered in the rest of the text. 1.1 Relevance of Analytics In this section, you learn how to: Motivate the relevance of insurance Describe analytics Describe data generating events associated with the timeline of a typical insurance contract This book introduces the process of using data to make decisions in an insurance context. It does not assume that readers are familiar with insurance but introduces insurance concepts as needed. Insurance may not be as entertaining as the sports industry nor as widely familiar as the agricultural industry but it does affect the financial livelihoods of many. By almost any measure, insurance is a major economy activity. On a global level, insurance premiums comprised about 6.3% of the world gross domestic product (GDP) in 2013, (Insurance Information Institute 2015). To illustrate, premiums accounted for 17.6% of GDP in Taiwan (the highest in the study) and represented 7.5% of GDP in the United States. On a personal level, almost everyone owning a home has insurance to protect themselves in the event of a fire, hailstorm, or some other calamitous event. Almost every country requires insurance for those driving a car. So, although not particulary entertaining nor widely familiar, insurance is an important piece of the economy and relevant to individual livelihoods. Insurance is a data-driven industry. Like other major corporations, insurers use data when trying to decide how much to pay employees, how many employees to retain, how to market their services, how to forecast financial trends, and so on. Although each industry retains its own nuances, these represent general areas of activities that are not specific to the insurance industry. You will find that the data methods and tools introduced in this text relevant for these general areas. Moreover, when introducing data methods, we will focus on losses that potentially arise from obligations in insurance contracts. This could be the amount of damage to one’s apartment under a renter’s insurance agreement, the amount needed to compensate someone that you hurt in a driving accident, and the like. We will call these insurance claims or loss amounts. With this focus, we will be able to introduce generally applicable statistical tools in techniques in real-life situations where the tools can be used directly. 1.1.1 What is Analytics? Insurance is a data-driven industry and analytics is a key to deriving information from data. But what is analytics? Making data-driven business decisions has been described as business analytics, business intelligence, and data science. These terms, among others, are sometimes used interchangeably and sometimes used separately, referring to distinct domains of applications. As an example of such distinctions, business intelligence may focus on processes of collecting data, often through databases and data warehouses, whereas business analytics utilizes tools and methods for statistical analyses of data. In contrast to these two terms that emphasize business applications, the term data science can encompass broader applications in many scientific domains. For our purposes, we use the term analytics to refer to the process of using data to make decisions. This process involves gathering data, understanding models of uncertainty, making general inferences, and communicating results. 1.1.2 Short-term Insurance This text will focus on short-term insurance contracts. By short-term, we mean contracts where the insurance coverage is typically provided for six months or a year. If you are new to insurance, then it is probably easiest to think about an insurance policy that covers the contents of an apartment or house that you are renting (known as renters insurance) or the contents and property of a building that is owned by you or a friend (known as homeowners insurance). Another easy example is automobile insurance. In the event of an accident, this policy may cover damage to your vehicle, damage to other vehicles in the accident, as well as medical expenses of those injured in the accident. In the US, policies such as renters and homeowners are known as property insurance whereas a policy such as auto that covers medical damages to people is known as casualty insurance. In the rest of the world, these are both known as nonlife or general insurance, to distinguish them from life insurance. Both life and nonlife insurances are important. To illustrate, (Insurance Information Institute 2015) estimates that direct insurance premiums in the world for 2013 was 2,608,091 for life and 2,032,850 for nonlife; these figures are in millions of US dollars. As noted earlier, the total represents 6.3% of the world GDP. Put another way, life accounts for 56.2% of insurance premiums and 3.5% of world GDP, nonlife accounts for 43.8% of insurance premiums and 2.7% of world GDP. Both life and nonlife represent important economic activities and are worthy of study in their own right. Yet, life insurance considerations differ from nonlife. In life insurance, the default is to have a multi-year contract. For example, if a person 25 years old purchases a whole life policy that pays upon death of the insured and that person does not die until age 100, then the contract is in force for 75 years. We think of this as a long-term contract. Further, in life insurance, the benefit amount is often stipulated in the contract provisions. In contrast, most short-term contracts provide for reimbursement of insured losses which are unknown before the accident. (Of course, there are usually limits placed on the reimbursement amounts.) In a multi-year life insurance contract, the time value of money plays a prominent role. In contrast, in a short-term nonlife contract, the random amount of reimbursement takes priority. In both life and nonlife insurances, the frequency of claims is very important. For many life insurance contracts, the insured event (such as death) happens only once. In contrast, for nonlife insurances such as automobile, it is common for individuals (especially young male drivers) to get into more than one accident during a year. So, our models need to reflect this observation; we will introduce different frequency models than you may have seen when studying life insurance. For short-term insurance, the framework of the probabilistic model is straightforward. We think of a one-period model (the period length, e.g., six months, will be specified in the situation). At the beginning of the period, the insured pays the insurer a known premium that is agreed upon by both parties to the contract. At the end of the period, the insurer reimburses the insured for a (possibly multivariate) random loss that we will denote as \\(y\\). This framework will be developed as we proceed but we first focus on integrating this framework with concerns about how the data may arise and what we can accomplish with this framework. 1.1.3 Insurance Processes One way to describe the data arising from operations of a company that sells insurance products is to adopt a granular approach. In this micro oriented view, we can think specifically about what happens to a contract at various stages of its existence. Consider Figure 1.1 that traces a timeline of a typical insurance contract. Throughout the existence of the contract, the company regularly processes events such as premium collection and valuation, described in Section 1.2; these are marked with an x on the timeline. Further, non-regular and unanticipated events also occur. To illustrate, times \\(\\mathrm{t}_2\\) and \\(\\mathrm{t}_4\\) mark the event of an insurance claim (some contracts, such as life insurance, can have only a single claim). Times \\(\\mathrm{t}_3\\) and \\(\\mathrm{t}_5\\) mark the events when a policyholder wishes to alter certain contract features, such as the choice of a deductible or the amount of coverage. Moreover, from a company perspective, one can even think about the contract initiation (arrival, time \\(\\mathrm{t}_1\\)) and contract termination (departure, time \\(\\mathrm{t}_6\\)) as uncertain events. Figure 1.1: Timeline of a Typical Insurance Policy. Arrows mark the occurrences of random events. Each x marks the time of scheduled events that are typically non-random. 1.2 Insurance Company Operations In this section, you learn how to: Describe five major operational areas of insurance companies. Identify the role of data and analytics opportunities within each operational area. Armed with insurance data and a method of organizing the data into variable types, the end goal is to use data to make decisions. Of course, we will need to learn more about methods of analyzing and extrapolating data but that is the purpose of the remaining chapters in the text. To begin, let us think about why we wish to do the analysis. To provide motivation, we take the insurer’s viewpoint (not a person) and introduce ways of bringing money in, paying it out, managing costs, and making sure that we have enough money to meet obligations. Specifically, in many insurance companies, it is customary to aggregate detailed insurance processes into larger operational units; many companies use these functional areas to segregate employee activities and areas of responsibilities. Actuaries and other financial analysts work within these units and use data for the following activities: Initiating Insurance. At this stage, the company makes a decision as to whether or not to take on a risk (the underwriting stage) and assign an appropriate premium (or rate). Insurance analytics has its actuarial roots in ratemaking, where analysts seek to determine the right price for the right risk. Renewing Insurance. Many contracts, particularly in general insurance, have relatively short durations such as 6 months or a year. Although there is an implicit expectation that such contracts will be renewed, the insurer has the opportunity to decline coverage and to adjust the premium. Analytics is also used at this policy renewal stage where the goal is to retain profitable customers. Claims Management. Analytics has long been used in (1) detecting and preventing claims fraud, (2) managing claim costs, including identifying the appropriate support for claims handling expenses, as well as (3) understanding excess layers for reinsurance and retention. Loss Reserving. Analytic tools are used to provide management with an appropriate estimate of future obligations and to quantify the uncertainty of the estimates. Solvency and Capital Allocation. Deciding on the requisite amount of capital and ways of allocating capital to alternative investment activities represent other important analytics activities. Companies must understand how much capital is needed so that they will have sufficient flow of cash available to meet their obligations. This is an important question that concerns not only company managers but also customers, company shareholders, regulatory authorities, as well as the public at large. Related to issues of how much capital is the question of how to allocate capital to differing financial projects, typically to maximize an investor’s return. Although this question can arise at several levels, insurance companies are typically concerned with how to allocate capital to different lines of business within a firm and to different subsidiaries of a parent firm. Although data is a critical component of solvency and capital allocation, other components including an economic framework and financial investments environment are also important. Because of the background needed to address these components, we will not address solvency and capital allocation issues further in this text. Nonetheless, for all operating functions, we emphasize that analytics in the insurance industry is not an exercise that a small group of analysts can do by themselves. It requires an insurer to make significant investments in their information technology, marketing, underwriting, and actuarial functions. As these areas represent the primary end goals of the analysis of data, additional background on each operational unit is provided in the following subsections. 1.2.1 Initiating Insurance Setting the price of an insurance good can be a perplexing problem. In manufacturing, the cost of a good is (relatively) known and provides a benchmark for assessing a market demand price. In other areas of financial services, market prices are available and provide the basis for a market-consistent pricing structure of products. In contrast, for many lines of insurance, the cost of a good is uncertain and market prices are unavailable. Expectations of the random cost is a reasonable place to start for a price, as this is the optimal price for a risk-neutral insurer. Thus, it has been traditional in insurance pricing to begin with the expected cost and to add to this so-called margins to account for the product’s riskiness, expenses incurred in servicing the product, and a profit/surplus allowance for the insurance company. For some lines of business, especially automobile and homeowners insurance, analytics has served to sharpen the market by making the calculation of the good’s expectation more precise. The increasing availability of the internet among consumers has promoted transparency in pricing. Insurers seek to increase their market share by refining their risk classification systems and employing skimming the cream underwriting strategies. Recent surveys (e.g., (Earnix 2013)) indicate that pricing is the most common use of analytics among insurers. Underwriting, the process of classifying risks into homogenous categories and assigning policyholders to these categories, lies at the core of ratemaking. Policyholders within a class have similar risk profiles and so are charged the same insurance price. This is the concept of an actuarially fair premium; it is fair to charge different rates to policyholders only if they can be separated by identifiable risk factors. To illustrate, an early contribution, Two Studies in Automobile Insurance Ratemaking, by (Bailey and LeRoy 1960) provided a catalyst to the acceptance of analytic methods in the insurance industry. This paper addresses the problem of classification ratemaking. It describes an example of automobile insurance that has five use classes cross-classified with four merit rating classes. At that time, the contribution to premiums for use and merit rating classes were determined independently of each other. Thinking about the interacting effects of different classification variables is a more difficult problem. 1.2.2 Renewing Insurance Insurance is a type of financial service and, like many service contracts, insurance coverage is often agreed upon for a limited time period, such as six months or a year, at which time commitments are complete. Particularly for general insurance, the need for coverage continues and so efforts are made to issue a new contract providing similar coverage. Renewal issues can also arise in life insurance, e.g., term (temporary) life insurance, although other contracts, such as life annuities, terminate upon the insured’s death and so issues of renewability are irrelevant. In absence of legal restrictions, at renewal the insurer has the opportunity to: accept or decline to underwrite the risk and determine a new premium, possibly in conjunction with a new classification of the risk. Risk classification and rating at renewal is based on two types of information. First, as at the initial stage, the insurer has available many rating variables upon which decisions can be made. Many variables will not change, e.g., sex, whereas others are likely to have changed, e.g., age, and still others may or may not change, e.g., credit score. Second, unlike the initial stage, at renewal the insurer has available a history of policyholder’s loss experience, and this history can provide insights into the policyholder that are not available from rating variables. Modifying premiums with claims history is known as experience rating, also sometimes referred to as merit rating. Experience rating methods are either applied retrospectively or prospectively. With retrospective methods, a refund of a portion of the premium is provided to the policyholder in the event of favorable (to the insurer) experience. Retrospective premiums are common in life insurance arrangements (where policyholders earned dividends in the U.S. and bonuses in the U.K.). In general insurance, prospective methods are more common, where favorable insured experience is rewarded through a lower renewal premium. Claims history can provide information about a policyholder’s risk appetite. For example, in personal lines it is common to use a variable to indicate whether or not a claim has occurred in the last three years. As another example, in a commercial line such as worker’s compensation, one may look to a policyholder’s average claim over the last three years. Claims history can reveal information that is hidden (to the insurer) about the policyholder. 1.2.3 Claims and Product Management In some of areas of insurance, the process of paying claims for insured events is relatively straightforward. For example, in life insurance, a simple death certificate is all that is needed as the benefit amount is provided in the contract terms. However, in non-life areas such as property and casualty insurance, the process is much more complex. Think about even a relatively simple insured event such as automobile accident. Here, it is often helpful to determine which party is at fault, one needs to assess damage to all of the vehicles and people involved in the incident, both insured and non-insured, the expenses incurred in assessing the damages, and so forth. The process of determining coverage, legal liability, and settling claims is known as claims adjustment. Insurance managers sometimes use the phrase claims leakage to mean dollars lost through claims management inefficiencies. There are many ways in which analytics can help manage the claims process, (Gorman and Swenson 2013). Historically, the most important has been fraud detection. The claim adjusting process involves reducing information asymmetry (the claimant knows exactly what happened; the company knows some of what happened). Mitigating fraud is an important part of claims management process. One can think about the management of claims severity as consisting of the following components: Claims triaging. Just as in the medical world, early identification and appropriate handling of high cost claims (patients, in the medical world), can lead to dramatic company savings. For example, in workers compensation, insurers look to achieve early identification of those claims that run the risk of high medical costs and a long payout period. Early intervention into those cases could give insurers more control over the handling of the claim, the medical treatment, and the overall costs with an earlier return-to-work. Claims processing. The goal is to use analytics to identify situations suitable for small claims handling processes and those for adjuster assignment to complex claims. Adjustment decisions. Once a complex claim has been identified and assigned to an adjuster, analytic driven routines can be established to aid subsequent decision-making processes. Such processes can also be helpful for adjusters in developing case reserves, an important input to the insurer’s loss reserves, Section 1.2.4. In addition to the insured’s reimbursement for insured losses, the insurer also needs to be concerned with another source of revenue outflow, expenses. Loss adjustment expenses are part of an insurer’s cost of managing claims. Analytics can be used to reduce expenses directly related to claims handling (allocated) as well as general staff time for overseeing the claims processes (unallocated). The insurance industry has high operating costs relative to other portions of the financial services sectors. In addition to claims payments, there are many other ways in which insurers use to data to manage their products. We have already discussed the need for analytics in underwriting, that is, risk classification at the initial acquisition stage. Insurers are also interested in which policyholders elect to renew their contract and, as with other products, monitor customer loyalty. Analytics can also be used to manage the portfolio, or collection, of risks that an insurer has acquired. When the risk is initially obtained, the insurer’s risk can be managed by imposing contract parameters that modify contract payouts. In Chapter xx introduces common modifications including coinsurance, deductibles, and policy upper limits. After the contract has been agreed upon with an insured, the insurer may still modify its net obligation by entering into a reinsurance agreement. This type of agreement is with a reinsurer, an insurer of an insurer. It is common for insurance companies to purchase insurance on its portfolio of risks to gain protection from unusual events, just as people and other companies do. 1.2.4 Loss Reserving An important feature that distinguishes insurance from other sectors of the economy is the timing of the exchange of considerations. In manufacturing, payments for goods are typically made at the time of a transaction. In contrast, for insurance, money received from a customer occurs in advance of benefits or services; these are rendered at a later date. This leads to the need to hold a reservoir of wealth to meet future obligations in respect to obligations made. The size of this reservoir of wealth, and the importance of ensuring its adequacy in regard to liabilities already assumed, is a major concern for the insurance industry. Setting aside money for unpaid claims is known as loss reserving; in some jurisdictions, reserves are also known as technical provisions. We saw in Figure 1.1 how future obligations arise naturally at a specific (valuation) date; a company must estimate these outstanding liabilities when determining its financial strength. Accurately determining loss reserves is important to insurers for many reasons. Loss reserves represent a loan that the insurer owes its customers. Under-reserving may result in a failure to meet claim liabilities. Conversely, an insurer with excessive reserves may present a weaker financial position than it truly has and lose market share. Reserves provide an estimate for the unpaid cost of insurance that can be used for pricing contracts. Loss reserving is required by laws and regulations. The public has a strong interest in the financial strength of insurers. In addition to the insurance company management and regulators, other stakeholders such as investors and customers make decisions that depend on company loss reserves. Loss reserving is a topic where there are substantive differences between life and general (also known as property and casualty, or non-life), insurance. In life insurance, the severity (amount of loss) is often not a source of concern as payouts are specified in the contract. The frequency, driven by mortality of the insured, is a concern. However, because of the length of time for settlement of life insurance contracts, the time value of money uncertainty as measured from issue to date of death can dominate frequency concerns. For example, for an insured who purchases a life contract at age 20, it would not be unusual for the contract to still be open in 60 years time. See, for example, (Bowers et al. 1986) or (Dickson, Hardy, and Waters 2013) for introductions to reserving for life insurance. 1.3 Case Study: Wisconsin Property Fund In this section, for a real case study such as the Wisconsin Property Fund, you learn how to: Describe how data generating events can produce data of interest to insurance analysts. Identify the type of each variable. Produce relevant summary statistics for each variable. Describe how these summary statistcs can be used in each of the major operational areas of an insurance company. Let us illustrate the kind of data under consideration and the goals that we wish to achieve by examining the Local Government Property Insurance Fund (LGPIF), an insurance pool administered by the Wisconsin Office of the Insurance Commissioner. The LGPIF was established to provide property insurance for local government entities that include counties, cities, towns, villages, school districts, and library boards. The fund insures local government property such as government buildings, schools, libraries, and motor vehicles. The fund covers all property losses except those resulting from flood, earthquake, wear and tear, extremes in temperature, mold, war, nuclear reactions, and embezzlement or theft by an employee. The property fund covers over a thousand local government entities who pay approximately $25 million in premiums each year and receive insurance coverage of about $75 billion. State government buildings are not covered; the LGPIF is for local government entities that have separate budgetary responsibilities and who need insurance to moderate the budget effects of uncertain insurable events. Coverage for local government property has been made available by the State of Wisconsin since 1911. 1.3.1 Fund Claims Variables At a fundamental level, insurance companies accept premiums in exchange for promises to indemnify a policyholder upon the uncertain occurrence of an insured event. This indemnification is known as a claim. A positive amount, also known as the severity of the claim, is a key financial expenditure for an insurer. So, knowing only the claim amount summarizes the reimbursement to the policyholder. Ignoring expenses, an insurer that examines only amounts paid would be indifferent to two claims of 100 when compared to one claim of 200, even though the number of claims differ. Nonetheless, it is common for insurers to study how often claims arise, known as the frequency of claims. The frequency is important for expenses, but it also influences contractual parameters (such as deductibles and policy limits) that are written on a per occurrence basis, is routinely monitored by insurance regulators, and is often a key driven in the overall indemnification obligation of the insurer. We shall consider the two claims variables, the severity and frequency, as the two main outcome variables that we wish to understand, model, and manage. To illustrate, in 2010 there were 1,110 policyholders in the property fund. Table 1.1 shows the distribution of the 1,377 claims. Almost two-thirds (0.637) of the policyholders did not have any claims and an additional 18.8% only had one claim. The remaining 17.5% (=1 - 0.637 - 0.188) had more than one claim; the policyholder with the highest number recorded 239 claims. The average number of claims for this sample was 1.24 (=1377/1110). Table 1.1: 2010 Claims Frequency Distribution Type Number 0 1 2 3 4 5 6 7 8 9 or more Sum Count 707 209 86 40 18 12 9 4 6 19 1,110 Proportion 0.637 0.188 0.077 0.036 0.016 0.011 0.008 0.004 0.005 0.017 1.000 R Code for Frequency Table Insample &lt;- read.csv(&quot;Insample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) Insample2010 &lt;- subset(Insample, Year==2010) table(Insample2010$Freq) For the severity distribution, one common approach is to examine the distribution of the sample of 1,377 claims. However, another common approach is to examine the distribution of the average claims of those policyholders with claims. In our 2010 sample, there were 403 (=1110-707) such policyholders. For 209 of these policyholders with one claim, the average claim equals the only claim they experienced. For the policyholder with highest frequency, the average claim is an average over 239 separately reported claim events. The total severity divided by the number of claims is also known as the pure premium or loss cost. Table 1.2 summarizes the sample distribution of average severities from the 403 policyholders; it shows that the average claim amount was 56,330 (all amounts are in US Dollars). However, the average gives only a limited look at the distribution. More information can be gleaned from the summary statistics which show a very large claim in the amount of 12,920,000. Figure 1.2 provides further information about the distribution of sample claims, showing a distribution that is dominated by this single large claim so that the histogram is not very helpful. Even when removing the large claim, you will find a distribution that is skewed to the right. A generally accepted technique is to work with claims in logarithmic units especially for graphical purposes; the corresponding figure in the right-hand panel is much easier to interpret. Table 1.2: 2010 Average Severity Distribution Minimum First Quartile Median Mean Third Quartile Maximum 167 2,226 4,951 56,330 11,900 12,920,000 Figure 1.2: Distribution of Positive Average Severities R Code for Severity Distribution Table and Figures Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) Insample2010 &lt;- subset(Insample, Year==2010) InsamplePos2010 &lt;- subset(Insample2010, yAvg&gt;0) # Table summary(InsamplePos2010$yAvg) length(InsamplePos2010$yAvg) # Figures par(mfrow=c(1, 2)) hist(InsamplePos2010$yAvg, main=&quot;&quot;, xlab=&quot;Average Claims&quot;) hist(log(InsamplePos2010$yAvg), main=&quot;&quot;, xlab=&quot;Logarithmic Average Claims&quot;) 1.3.2 Fund Rating Variables Developing models to represent and manage the two outcome variables, frequency and severity, is the focus of the early chapters of this text. However, when actuaries and other financial analysts use those models, they do so in the context of externally available variables. In general statistical terminology, one might call these explanatory or predictor variables; there are many other names in statistics, economics, psychology, and other disciplines. Because of our insurance focus, we call them rating variables as they will be useful in setting insurance rates and premiums. We earlier considered a sample of 1,110 observations which may seem like a lot. However, as we will seen in our forthcoming applications, because of the preponderance of zeros and the skewed nature of claims, actuaries typically yearn for more data. One common approach that we adopt here is to examine outcomes from multiple years, thus increasing the sample size. We will discuss the strengths and limitations of this strategy later but, at this juncture, just want to show the reader how it works. Specifically, Table 1.3 shows that we now consider policies over five years of data, years 2006, …, 2010, inclusive. The data begins in 2006 because there was a shift in claim coding in 2005 so that comparisons with earlier years are not helpful. To mitigate the effect of open claims, we consider policy years prior to 2011. An open claim means that all of the obligations are not known at the time of the analysis; for some claims, such an injury to a person in an auto accident or in the workplace, it can take years before costs are fully known. Table 1.3 shows that the average claim varies over time, especially with the high 2010 value due to a single large claim. The total number of policyholders is steadily declining and, conversely, the coverage is steadily increasing. The coverage variable is the amount of coverage of the property and contents. Roughly, you can think of it as the maximum possible payout of the insurer. For our immediate purposes, it is our first rating variable. Other things being equal, we would expect that policyholders with larger coverage will have larger claims. We will make this vague idea much more precise as we proceed. Table 1.3: Building and Contents Claims Summary Year Average Frequency Average Severity Average Coverage Number of Policyholders 2006 0.951 9,695 32,498,186 1,154 2007 1.167 6,544 35,275,949 1,138 2008 0.974 5,311 37,267,485 1,125 2009 1.219 4,572 40,355,382 1,112 2010 1.241 20,452 41,242,070 1,110 R Code for Building and Contents Claims Summary Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) library(doBy) T1A &lt;- summaryBy(Freq ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) T1B &lt;- summaryBy(yAvg ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) T1C &lt;- summaryBy(BCcov ~ Year, data = Insample, FUN = function(x) { c(m = mean(x), num=length(x)) } ) Table1In &lt;- cbind(T1A[1],T1A[2],T1B[2],T1C[2],T1A[3]) names(Table1In) &lt;- c(&quot;Year&quot;, &quot;Average Frequency&quot;,&quot;Average Severity&quot;, &quot;Average&quot;,&quot;Number of Policyholders&quot;) Table1In For a different look at this five-year sample, Table 1.4 summarizes the distribution of our two outcomes, frequency and claims amount. In each case, the average exceeds the median, suggesting that the two distributions are right-skewed. In addition, the table summarizes our continuous rating variables, coverage and deductible amount. The table also suggests that these variables also have right-skewed distributions. Table 1.4: Summary of Claim Frequency and Severity, Deductibles, and Coverages Minimum Median Average Maximum Claim Frequency 0 0 1.109 263 Claim Severity 0 0 9,292 12,922,218 Deductible 500 1,000 3,365 100,000 Coverage (000’s) 8.937 11,354 37,281 2,444,797 R Code for Summary of Claim Frequency and Severity, Deductibles, and Coverages Insample &lt;- read.csv(&quot;Data/PropertyFundInsample.csv&quot;, header=T, na.strings=c(&quot;.&quot;), stringsAsFactors=FALSE) t1&lt;- summaryBy(Insample$Freq ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x),m=mean(x),mb=max(x)) } ) names(t1) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t2 &lt;- summaryBy(Insample$yAvg ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t2) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t3 &lt;- summaryBy(Deduct ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t3) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) t4 &lt;- summaryBy(BCcov/1000 ~ 1, data = Insample, FUN = function(x) { c(ma=min(x), m1=median(x), m=mean(x),mb=max(x)) } ) names(t4) &lt;- c(&quot;Minimum&quot;, &quot;Median&quot;,&quot;Average&quot;, &quot;Maximum&quot;) Table2 &lt;- rbind(t1,t2,t3,t4) Table2a &lt;- round(Table2,3) Rowlable &lt;- rbind(&quot;Claim Frequency&quot;,&quot;Claim Severity&quot;,&quot;Deductible&quot;,&quot;Coverage (000&#39;s)&quot;) Table2aa &lt;- cbind(Rowlable,as.matrix(Table2a)) Table2aa The following display describes the rating variables considered in this chapter. To handle the skewness, we henceforth focus on logarithmic transformations of coverage and deductibles. To get a sense of the relationship between the non-continuous rating variables and claims, Table 1.5 relates the claims outcomes to these categorical variables. Table 1.5 suggests substantial variation in the claim frequency and average severity of the claims by entity type. It also demonstrates higher frequency and severity for the \\({\\tt Fire5}\\) variable and the reverse for the \\({\\tt NoClaimCredit}\\) variable. The relationship for the \\({\\tt Fire5}\\) variable is counter-intuitive in that one would expect lower claim amounts for those policyholders in areas with better public protection (when the protection code is five or less). Naturally, there are other variables that influence this relationship. We will see that these background variables are accounted for in the subsequent multivariate regression analysis, which yields an intuitive, appealing (negative) sign for the \\({\\tt Fire5}\\) variable. Description of Rating Variables \\[{\\small \\begin{matrix} \\begin{array}{ l | l} \\hline Variable &amp; Description \\\\ \\hline \\text{EntityType} &amp; \\text{Categorical variable that is one of six types: (Village, City,} \\\\ &amp; ~~~~ \\text{County, Misc, School, or Town)} \\\\ \\text{LnCoverage} &amp; \\text{Total building and content coverage, in logarithmic millions of dollars}\\\\ \\text{LnDeduct} &amp; \\text{Deductible, in logarithmic dollars} \\\\ \\text{AlarmCredit} &amp; \\text{Categorical variable that is one of four types: (0, 5, 10, or 15)} \\\\ &amp; ~~~~ \\text{for automatic smoke alarms in main rooms} \\\\ \\text{NoClaimCredit} &amp; \\text{Binary variable to indicate no claims in the past two years} \\\\ \\text{Fire5 } &amp; \\text{Binary variable to indicate the fire class is below 5} \\\\ &amp; ~~~~ \\text{(The range of fire class is 0 to 10} \\\\ \\hline \\end{array} \\end{matrix}}\\] Table 1.5: Claims Summary by Entity Type, Fire Class, and No Claim Credit Variable Number of Policies Claim Frequency Average Severity EntityType Village 1,341 0.452 10,645 City 793 1.941 16,924 County 328 4.899 15,453 Misc 609 0.186 43,036 School 1,597 1.434 64,346 Town 971 0.103 19,831 Fire5=0 2,508 0.502 13,935 Fire5=1 3,131 1.596 41,421 NoClaimCredit=0 3,786 1.501 31,365 NoClaimCredit=1 1,853 0.310 30,499 Total 5,639 1.109 31,206 R Code for Claims Summary by Entity Type, Fire Class, and No Claim Credit ByVarSumm&lt;-function(datasub){ tempA &lt;- summaryBy(Freq ~ 1 , data = datasub, FUN = function(x) { c(m = mean(x), num=length(x)) } ) datasub1 &lt;- subset(datasub, yAvg&gt;0) tempB &lt;- summaryBy(yAvg ~ 1, data = datasub1,FUN = function(x) { c(m = mean(x)) } ) tempC &lt;- merge(tempA,tempB,all.x=T)[c(2,1,3)] tempC1 &lt;- as.matrix(tempC) return(tempC1) } datasub &lt;- subset(Insample, TypeVillage == 1); t1 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCity == 1); t2 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCounty == 1); t3 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeMisc == 1); t4 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeSchool == 1); t5 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeTown == 1); t6 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Fire5 == 0); t7 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Fire5 == 1); t8 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Insample$NoClaimCredit == 0); t9 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, Insample$NoClaimCredit == 1); t10 &lt;- ByVarSumm(datasub) t11 &lt;- ByVarSumm(Insample) Tablea &lt;- rbind(t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11) Tableaa &lt;- round(Tablea,3) Rowlable &lt;- rbind(&quot;Village&quot;,&quot;City&quot;,&quot;County&quot;,&quot;Misc&quot;,&quot;School&quot;, &quot;Town&quot;,&quot;Fire5--No&quot;,&quot;Fire5--Yes&quot;,&quot;NoClaimCredit--No&quot;, &quot;NoClaimCredit--Yes&quot;,&quot;Total&quot;) Table4 &lt;- cbind(Rowlable,as.matrix(Tableaa)) Table4 Table 1.6 shows the claims experience by alarm credit. It underscores the difficulty of examining variables individually. For example, when looking at the experience for all entities, we see that policyholders with no alarm credit have on average lower frequency and severity than policyholders with the highest (15%, with 24/7 monitoring by a fire station or security company) alarm credit. In particular, when we look at the entity type School, the frequency is 0.422 and the severity 25,257 for no alarm credit, whereas for the highest alarm level it is 2.008 and 85,140. This may simply imply that entities with more claims are the ones that are likely to have an alarm system. Summary tables do not examine multivariate effects; for example, Table 1.5 ignores the effect of size (as we measure through coverage amounts) that affect claims. Table 1.6: Claims Summary by Entity Type and Alarm Credit Category Entity Type Claim Frequency Avg. Severity Num. Policies Claim Frequency Avg. Severity Num. Policies Village 0.326 11,078 829 0.278 8,086 54 City 0.893 7,576 244 2.077 4,150 13 County 2.140 16,013 50 - - 1 Misc 0.117 15,122 386 0.278 13,064 18 School 0.422 25,523 294 0.410 14,575 122 Town 0.083 25,257 808 0.194 3,937 31 Total 0.318 15,118 2,611 0.431 10,762 239 Claims Summary by Entity Type and Alarm Credit Category Entity Type Claim Frequency Avg. Severity Num. Policies Claim Frequency Avg. Severity Num. Policies Village 0.500 8,792 50 0.725 10,544 408 City 1.258 8,625 31 2.485 20,470 505 County 2.125 11,688 8 5.513 15,476 269 Misc 0.077 3,923 26 0.341 87,021 179 School 0.488 11,597 168 2.008 85,140 1,013 Town 0.091 2,338 44 0.261 9,490 88 Total 0.517 10,194 327 2.093 41,458 2,462 R Code for Claims Summary by Entity Type and Alarm Credit Category #Claims Summary by Entity Type and Alarm Credit ByVarSumm&lt;-function(datasub){ tempA &lt;- summaryBy(Freq ~ AC00 , data = datasub, FUN = function(x) { c(m = mean(x), num=length(x)) } ) datasub1 &lt;- subset(datasub, yAvg&gt;0) if(nrow(datasub1)==0) { n&lt;-nrow(datasub) return(c(0,0,n)) } else { tempB &lt;- summaryBy(yAvg ~ AC00, data = datasub1, FUN = function(x) { c(m = mean(x)) } ) tempC &lt;- merge(tempA,tempB,all.x=T)[c(2,4,3)] tempC1 &lt;- as.matrix(tempC) return(tempC1) } } AlarmC &lt;- 1*(Insample$AC00==1) + 2*(Insample$AC05==1)+ 3*(Insample$AC10==1)+ 4*(Insample$AC15==1) ByVarCredit&lt;-function(ACnum){ datasub &lt;- subset(Insample, TypeVillage == 1 &amp; AlarmC == ACnum); t1 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCity == 1 &amp; AlarmC == ACnum); t2 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeCounty == 1 &amp; AlarmC == ACnum); t3 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeMisc == 1 &amp; AlarmC == ACnum); t4 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeSchool == 1 &amp; AlarmC == ACnum); t5 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, TypeTown == 1 &amp; AlarmC ==ACnum); t6 &lt;- ByVarSumm(datasub) datasub &lt;- subset(Insample, AlarmC == ACnum); t7 &lt;- ByVarSumm(datasub) Tablea &lt;- rbind(t1,t2,t3,t4,t5,t6,t7) Tableaa &lt;- round(Tablea,3) Rowlable &lt;- rbind(&quot;Village&quot;,&quot;City&quot;,&quot;County&quot;,&quot;Misc&quot;,&quot;School&quot;, &quot;Town&quot;,&quot;Total&quot;) Table4 &lt;- cbind(Rowlable,as.matrix(Tableaa)) } Table4a &lt;- ByVarCredit(1) #Claims Summary by Entity Type and Alarm Credit==00 Table4b &lt;- ByVarCredit(2) #Claims Summary by Entity Type and Alarm Credit==05 Table4c &lt;- ByVarCredit(3) #Claims Summary by Entity Type and Alarm Credit==10 Table4d &lt;- ByVarCredit(4) #Claims Summary by Entity Type and Alarm Credit==15 1.3.3 Fund Operations We have now seen the Fund’s two outcome variables, a count variable for the number of claims and a continuous variable for the claims amount. We have also introduced a continuous rating variable, coverage, discrete quantitative variable, (logarithmic) deductibles, two binary rating variable, no claims credit and fire class, as well as two categorical rating variables, entity type and alarm credit. Subsequent chapters will explain how to analyze and model the distribution of these variables and their relationships. Before getting into these technical details, let us first think about where we want to go. General insurance company functional areas are described in Section 1.2; let us now think about how these areas might apply in the context of the property fund. Initiating Insurance Because this is a government sponsored fund, we do not have to worry about selecting good or avoiding poor risks; the fund is not allowed to deny a coverage application from a qualified local government entity. If we do not have to underwrite, what about how much to charge? We might look at the most recent experience in 2010, where the total fund claims were approximately 28.16 million USD (\\(=1377 \\text{ claims} \\times 20452 \\text{ average severity}\\)). Dividing that among 1,110 policyholders, that suggests a rate of 24,370 ( \\(\\approx\\) 28,160,000/1110). However, 2010 was a bad year; using the same method, our premium would be much lower based on 2009 data. This swing in premiums would defeat the primary purpose of the fund, to allow for a steady charge that local property managers could utilize in their budgets. Having a single price for all policyholders is nice but hardly seems fair. For example, Table 1.5 suggests that Schools have much higher claims than other entities and so should pay more. However, simply doing the calculation on an entity by entity basis is not right either. For example, we saw in Table 1.6 that had we used this strategy, entities with a 15% alarm credit (for good behavior, having top alarm systems) would actually wind up paying more. So, we have the data for thinking about the appropriate rates to charge but will need to dig deeper into the analysis. We will explore this topic further in Chapter 6 on premium calculation fundamentals. Selecting appropriate risks is introduced in Chapter 7 on risk classification. Renewing Insurance Although property insurance is typically a one-year contract, Table 1.3 suggests that policyholders tend to renew; this is typical of general insurance. For renewing policyholders, in addition to their rating variables we have their claims history and this claims history can be a good predictor of future claims. For example, Table 1.3 shows that policyholders without a claim in the last two years had much lower claim frequencies than those with at least one accident (0.310 compared to 1.501); a lower predicted frequency typically results in a lower premium. This is why it is common for insurers to use variables such as \\({\\tt NoClaimCredit}\\) in their rating. We will explore this topic further in Chapter 8 on experience rating. Claims Management Of course, the main story line of 2010 experience was the large claim of over 12 million USD, nearly half the claims for that year. Are there ways that this could have been prevented or mitigated? Are their ways for the fund to purchase protection against such large unusual events? Another unusual feature of the 2010 experience noted earlier was the very large frequency of claims (239) for one policyholder. Given that there were only 1,377 claims that year, this means that a single policyholder had 17.4 % of the claims. This also suggestions opportunities for managing claims, the subject of Chapter 9. Loss Reserving In our case study, we look only at the one year outcomes of closed claims (the opposite of open). However, like many lines of insurance, obligations from insured events to buildings such as fire, hail, and the like, are not known immediately and may develop over time. Other lines of business, including those were there are injuries to people, take much longer to develop. Chapter 10 introduces this concern and loss reserving, the discipline of determining how much the insurance company should retain to meet its obligations. 1.4 Contributors and Further Resources Contributor Edward W. (Jed) Frees, University of Wisconsin-Madison, is the principal author of the initital version of this chapter. Email: jfrees@bus.wisc.edu for chapter comments and suggested improvements. This book introduces loss data analytic tools that are most relevant to actuaries and other financial risk analysts. Here are a few reference cited in the chapter. Bibliography "],
["C-Frequency.html", "Chapter 2 Frequency Distributions 2.1 How Frequency Augments Severity Information 2.2 Basic Frequency Distributions 2.3 The (a, b, 0) Class 2.4 Estimating Frequency Distributions 2.5 Other Frequency Distributions 2.6 Mixture Distributions 2.7 Goodness of Fit 2.8 Exercises", " Chapter 2 Frequency Distributions Chapter Preview. These are overheads from a course that provides some structure for this chapter. 2.1 How Frequency Augments Severity Information 2.1.0.1 Basic Terminology Claim - indemnification upon the occurrence of an insured event Loss - some authors use claim and loss interchangeably, others think of loss as the amount suffered by the insured whereas claim is the amount paid by the insurer Frequency - how often an insured event occurs, typically within a policy contract Count - In this chapter, we focus on count random variables that represent the number of claims, that is, how frequently an event occurs Severity - Amount, or size, of each payment for an insured event 2.1.0.2 The Importance of Frequency Insurers pay claims in monetary units, e.g., US dollars. So, why should they care about how frequently claims occur? Many ways to use claims modeling – easiest to motivate in terms of pricing for personal lines insurance Recall from Chapter 1 that setting the price of an insurance good can be a perplexing problem. In manufacturing, the cost of a good is (relatively) known Other financial service areas, market prices are available Insurance tradition: Start with an expected cost. Add “margins” to account for the product’s riskiness, expenses incurred in servicing the product, and a profit/surplus allowance for the insurance company. Think of the expected cost as the expected number of claims times the expected amount per claims, that is, expected frequency times severity. Claim amounts, or severities, will turn out to be relatively homogeneous for many lines of business and so we begin our investigations with frequency modeling. 2.1.0.3 Other Ways that Frequency Augments Severity Information Contractual - For example, deductibles and policy limits are often in terms of each occurrence of an insured event Behaviorial - Explanatory (rating) variables can have different effects on models of how often an event occurs in contrast to the size of the event. In healthcare, the decision to utilize healthcare by individuals is related primarily to personal characteristics whereas the cost per user may be more related to characteristics of the healthcare provider (such as the physician). Databases. Many insurers keep separate data files that suggest developing separate frequency and severity models. This recording process makes it natural for insurers to model the frequency and severity as separate processes. Policyholder file that is established when a policy is written. This file records much underwriting information about the insured(s), such as age, gender and prior claims experience, policy information such as coverage, deductibles and limitations, as well as the insurance claims event. Claims file, records details of the claim against the insurer, including the amount. (There may also be a “payments” file that records the timing of the payments although we shall not deal with that here.) Regulatory and Administrative Regulators routinely require the reporting of claims numbers as well as amounts. This may be due to the fact that there can be alternative definitions of an “amount,” e.g., paid versus incurred, and there is less potential error when reporting claim numbers. 2.2 Basic Frequency Distributions 2.2.1 Foundations Claim count \\(N\\) has support on the non-negative integers \\(k=0,1,2, \\ldots\\). The probability mass function is denoted as \\(\\Pr(N = k) = p_k\\) We can summarize the distribution through its moments The mean, or first moment, is \\[\\mathrm{E~} N = \\mu_1 = \\mu = \\sum^{\\infty}_{k=0} k p_k .\\] More generally, the \\(r\\)th moment is \\[\\mathrm{E~} N^r = \\mu_r^{\\prime} = \\sum^{\\infty}_{k=0} k^r p_k .\\] The variance is \\[\\mathrm{Var~} N = \\mathrm{E~} (N-\\mu)^2 = \\mathrm{E~} N^2 - \\mu^2\\] Also recall the moment generating function \\[M_N(t) = \\mathrm{E~}e^{tN} = \\sum^{\\infty}_{k=0} e^{tk} p_k .\\] 2.2.2 Probability Generating Function The probability generating function is \\[\\begin{aligned} \\mathrm{P}(z) &amp;= \\mathrm{E~}z^N = \\mathrm{E~}\\exp{(N \\ln z)} = M_N(\\ln{z})\\\\ &amp;= \\sum^{\\infty}_{k=0} z^k p_k .\\end{aligned}\\] By taking the \\(m\\)th derivative, we see that \\[\\begin{aligned} \\left. P^{(m)}(z)\\right|_{z=0} &amp;= \\frac{\\partial^m }{\\partial z^m} P(z)|_{z=0} = p_m m!\\end{aligned}\\] the pgf “generates” the probabilities. Further, the pgf can be used to generate moments \\[\\begin{aligned} P^{(1)}(1) &amp;= \\sum k p_k = \\mathrm{E~}N .\\end{aligned}\\] and \\[P^{(2)}(1) = \\mathrm{E~}N(N-1).\\] 2.2.3 Important Frequency Distributions The three important (in insurance) frequency distributions are: Poisson Negative binomial Binomial They are important because: They fit well many insurance data sets of interest They provide the basis for more complex distributions that even better approximate real situations of interest to us 2.2.3.1 Poisson Distribution This distribution has parameter \\(\\lambda\\), probability mass function \\[p_k = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\] and pgf \\[\\begin{aligned} P(z) &amp;= M_N (\\ln z) = \\exp(\\lambda(z-1))\\end{aligned}\\] The expectation is \\(\\mathrm{E~}N = \\lambda\\) which is the same as the variance, \\(\\mathrm{Var~}N = \\lambda\\). 2.2.3.2 Negative Binomial Distribution This distribution has parameters \\((r, \\beta)\\), probability mass function (pmf) \\[p_k = {k+r-1\\choose k} \\left(\\frac{1}{1+\\beta}\\right)^r \\left(\\frac{\\beta}{1+\\beta}\\right)^k\\] and probability generating function (pgf) \\[\\begin{aligned} P(z) &amp;= (1-\\beta(z-1))^{-r} \\end{aligned}\\] The expectation is \\(\\mathrm{E~}N = r\\beta\\) and the variance is \\(\\mathrm{Var~}N = r\\beta(1+\\beta)\\). When \\(\\beta&gt;0\\), we have \\(\\mathrm{Var~}N &gt;\\mathrm{E~}N\\). This distribution is said to be overdispersed (relative to the Poisson). 2.2.3.3 Binomial Distribution This distribution has parameters \\((m,q)\\), probability mass function \\[p_k = {m\\choose k} q^k (1-q)^{m-k}\\] and pgf \\[\\begin{aligned} P(z) &amp;= (1+q(z-1))^m\\end{aligned}\\] The mean is \\(\\mathrm{E~}N = mq\\) and the variance is \\(\\mathrm{Var~}N = mq(1-q)\\). 2.3 The (a, b, 0) Class Recall the notation \\(p_k= \\Pr(N = k)\\). Definition. A count distribution is a member of the (\\(a, b\\), 0) class if the probabilities \\(p_k\\) satisfy \\[\\frac{p_k}{p_{k-1}}=a+\\frac{b}{k},\\] for constants \\(a,b\\) and for $k=1,2,3, $. There are only three distributions that are members of the (\\(a,b\\),0) class. They are the Poisson (\\(a=0\\)), binomial(\\(a&lt;0\\)), and negative binomial (\\(a&gt;0\\)). The recursive expression provides a computationally efficient way to generate probabilities. 2.3.0.1 The (a, b, 0) Class - Special Cases Example: Poisson Distribution. Recall the pmf \\(p_k =\\frac{\\lambda^k}{k!}e^{-\\lambda}\\). Examining the ratio, \\[\\frac{p_k}{p_{k-1}} = \\frac{\\lambda^k/k!}{\\lambda^{k-1}/(k-1)!}\\frac{e^{-\\lambda}}{e^{-\\lambda}}= \\frac{\\lambda}{k}\\] Thus, the Poisson is a member of the (\\(a, b\\), 0) class with \\(a = 0\\), \\(b = \\lambda\\), and initial starting value \\(p_0 = e^{-\\lambda}\\). Other special cases (Please check) Example: Binomial Distribution. Use a similar technique to check that the binomial distribution is a member of the (\\(a, b\\), 0) class with \\(a = \\frac{-q}{1-q},\\) \\(b = \\frac{(m+1)q}{1-q},\\) and initial starting value \\(p_0 = (1-q)^m\\). Another special case of the (\\(a, b\\), 0) Class (Please check) Example: Negative Binomial Distribution. Check that the negative binomial distribution is a member of the (\\(a, b\\), 0) class with \\(a = \\frac{\\beta}{1+\\beta},\\) \\(b = \\frac{(r-1)\\beta}{1+\\beta},\\) and initial starting value \\(p_0 = (1+\\beta)^{-r}\\). Exercise. A discrete probability distribution has the following properties \\[\\begin{aligned} p_k&amp;=c\\left( 1+\\frac{2}{k}\\right) p_{k-1} \\:\\:\\: k=1,2,3,\\\\ p_1&amp;= \\frac{9}{256}\\end{aligned}\\] Determine the expected value of this discrete random variable (Ans: 9) 2.3.1 The (a, b, 0) Class - Example Exercise. A discrete probability distribution has the following properties \\[\\begin{aligned} \\Pr(N=k) = \\left( \\frac{3k+9}{8k}\\right) \\Pr(N=k-1), ~~~k=1,2,3,\\ldots\\end{aligned}\\] Determine the value of \\(\\Pr(N=3)\\). (Ans: 0.1609) 2.4 Estimating Frequency Distributions 2.4.0.1 Parameter estimation The customary method of estimation is maximum likelihood. To provide intuition, we outline the ideas in the context of Bernoulli distribution. This is a special case of the binomial distribution with \\(m=1\\) For count distributions, either there is a claim \\(N=1\\) or not \\(N=0\\). The probability mass function is \\[p_k = \\Pr (N=k) = \\left\\{ \\begin{array}{ll} 1-q &amp; \\mathrm{if}\\ k=0 \\\\ q&amp; \\mathrm{if}\\ k=1 \\end{array} \\right. .\\] The Statistical Inference Problem Now suppose that we have a collection of independent random variables. The \\(i\\)th variable is denoted as \\(N_i\\). Further assume they have the same Bernoulli distribution with parameter \\(q\\). In statistical inference, we assume that we observe a sample of such random variables. The observed value of the \\(i\\)th random variable is \\(n_i\\). Assuming that the Bernoulli distribution is correct, we wish to say something about the probability parameter \\(q\\). 2.4.0.2 Bernoulli Likelihoods Definition. The likelihood is the observed value of the mass function. For a single observation, the likelihood is \\[\\left\\{ \\begin{array}{ll} 1-q &amp; \\mathrm{if}\\ n_i=0 \\\\ q &amp; \\mathrm{if}\\ n_i=1 \\end{array} \\right. .\\] The objective of maximum likelihood estimation (MLE) is to find the parameter values that produce the largest likelihood. Finding the maximum of the logarithmic function yields the same solution as finding the maximum of the corresponding function. Because it is generally computationally simpler, we consider the logarithmic (log-) likelihood, written as \\[\\left\\{ \\begin{array}{ll} \\ln \\left( 1-q\\right) &amp; \\mathrm{if}\\ n_i=0 \\\\ \\ln q &amp; \\mathrm{if}\\ n_i=1 \\end{array}\\right. .\\] 2.4.0.3 Bernoulli MLE More compactly, the log-likelihood of a single observation is \\[n_i \\ln q + (1-n_i)\\ln ( 1-q ) ,\\] Assuming independence, the log-likelihood of the data set is \\[L_{Bern}(q)=\\sum_i \\left\\{ n_i \\ln q + (1-n_i)\\ln ( 1-q ) \\right\\}\\] The (log) likelihood is viewed as a function of the parameters, with the data held fixed. In contrast, the joint probability mass function is viewed as a function of the realized data, with the parameters held fixed. The method of maximum likelihood means finding the values of \\(q\\) that maximize the log-likelihood. We began with the Bernoulli distribution in part because the log-likelihood is easy to maximize. Take a derivative of \\(L_{Bern}(q)\\) to get \\[\\frac{\\partial}{\\partial q} L_{Bern}(q)=\\sum_i \\left\\{ n_i \\frac{1}{q} - (1-n_i)\\frac{1}{1-q} \\right\\}\\] and solving the equation \\(\\frac{\\partial}{\\partial q} L_{Bern}(q) =0\\) yields \\[\\hat{q} = \\frac{\\sum_i n_i}{\\mathrm{sample ~size}}\\] or, in words, the \\(MLE\\) \\(\\hat{q}\\) is the fraction of one’s in the sample. Just to be complete, you should check, by taking derivatives, that when we solve \\(\\frac{\\partial}{\\partial q} L_{Bern}(q) =0\\) we are maximizing the function \\(L_{Bern}(q)\\), not minimizing it. 2.4.0.4 Frequency Distributions MLE We can readily extend this procedure to all frequency distributions For notation, suppose that \\(\\theta\\) (“theta”) is a parameter that describes a given frequency distribution \\(\\Pr(N=k; \\theta) = p_k(\\theta)\\) In later developments we will let \\(\\theta\\) be a vector but for the moment assume it to be a scalar. The log-likelihood of a a single observation is \\[\\left\\{ \\begin{array}{ll} \\ln p_0(\\theta) &amp; \\mathrm{if}\\ n_i=0 \\\\ \\ln p_1(\\theta) &amp; \\mathrm{if}\\ n_i=1 \\\\ \\vdots &amp; \\vdots \\end{array} \\right. .\\] that can be written more compactly as \\[\\sum_k I(n_i=k) \\ln p_k(\\theta).\\] this uses the notation \\(I(\\cdot)\\) to be the indicator of a set (it returns one if the event is true and 0 otherwise). Assuming independence, the log-likelihood of the data set is \\[L(\\theta)=\\sum_i \\left\\{ \\sum_k I(n_i=k) \\ln p_k(\\theta) \\right\\} = \\left\\{ \\sum_k m_k\\ln p_k(\\theta) \\right\\}\\] where we use the notation \\(m_k\\) to denote the number of observations that are observed having count \\(k\\). Using notation, \\(m_k = \\sum_i I(n_i=k)\\). Special Case. Poisson. A simple exercise in calculus yields \\[\\hat{\\lambda} = \\frac{\\mathrm{number ~of ~claims}}{\\mathrm{sample ~size}} = \\frac{\\sum_k k m_k}{\\sum_k m_k}\\] the average claim count. 2.5 Other Frequency Distributions Naturally, there are many other count distributions needed in practice For many insurance applications, one can work with one of our three basic distributions (binomial, Poisson, negative binomial) and allow the parameters to be a function of known explanatory variables. This allows us to explain claim probabilities in terms of known (to the insurer) variables such as age, sex, geographic location (territory), and so forth. This field of statistical study is known as regression analysis - it is an important topic that we will not pursue in this course. To extend our basic count distributions to alternatives needed in practice, we consider two approaches: Zero truncation or modification Mixing 2.5.1 Zero Truncation or Modification Why truncate or modify zero? If we work with a database of claims, then there are no zero! In personal lines (like auto), people may not want to report that first claim because they fear it will increase future insurance rates. Let’s modify zero probabilities in terms of the \\((a,b,0)\\) class Definition. A count distribution is a member of the (\\(a, b\\), 1) class if the probabilities \\(p_k\\) satisfy \\[\\frac{p_k}{p_{k-1}}=a+\\frac{b}{k},\\] for constants \\(a,b\\) and for \\(k=2,3, \\ldots\\). Note that this starts at \\(k=2\\), not \\(k=1\\). That is, the most important thing about this definition is that the recursion starts at \\(p_1\\), not \\(p_0\\). Thus, all distributions that are members of the (\\(a, b\\), 0) are members of the (\\(a, b\\), 1) class. Naturally, there are additional distributions that are members of this wider class. To see how this works, pick a specific distribution in the (\\(a, b\\), 0) class. Consider \\(p_k^0\\) to be a probability for this member of \\((a,b,0)\\). Let \\(p_k^M\\) be the corresponding probability for a member of \\((a,b,1)\\), where the \\(M\\) stands for “modified”. Pick a new probability of a zero claim, \\(p_0^M\\), and define \\[\\begin{aligned} c = \\frac{1-p_0^M}{1-p_0^0} .\\end{aligned}\\] We then calculate the rest of the modified distribution as \\[\\begin{aligned} p_k^M =c p_k^0\\end{aligned}\\] 2.5.1.1 Special Case: Poisson Truncated at Zero. For this case, we assume that \\(p_0^M=0\\), so that the probability of \\(N=0\\) is zero, hence the name “truncated at zero.” For this case, we use the letter \\(T\\) to denote probabilities instead of \\(M\\), so we use \\(p_k^T\\) for probabilities. Thus, \\[\\begin{aligned} p_k^T&amp;= \\left \\{ \\begin{array}{cc} 0 &amp; k=0\\\\ \\frac{1}{1-p_0^0}p_k^0 &amp; k \\ge 1\\\\ \\end{array} \\right.\\end{aligned}\\] 2.5.1.2 Modified Poisson Example Example: Zero Truncated/Modified Poisson. Consider a Poisson distribution with parameter \\(\\lambda=2\\). We show how to calculate \\(p_k, k=0,1,2,3\\), for the usual (unmodified), truncated and a modified version with \\((p_0^M=0.6)\\). Solution. For the Poisson distribution as a member of the (\\(a,b\\),0) class, we have \\(a=0\\) and \\(b=\\lambda=2\\). Thus, we may use the recursion \\(p_k = \\lambda p_{k-1}/k= 2 p_{k-1}/k\\) for each type, after determining starting probabilities. k \\(p_k\\) \\(p_k^T\\) \\(p_k^M\\) 0 \\(p_0=e^{-\\lambda}=0.135335\\) 0 0.6 1 \\(p_1=p_0(0+\\frac{\\lambda}{1})=0.27067\\) \\(\\frac{p_1}{1-p_0}=0.313035\\) \\(\\frac{1-p_0^M}{1-p_0}~p_1=0.125214\\) 2 \\(p_2=p_1\\left( \\frac{\\lambda}{2}\\right)=0.27067\\) \\(p_2^T=p_1^T\\left(\\frac{\\lambda}{2}\\right)=0.313035\\) \\(p_2^M=0.125214\\) 3 \\(p_3=p_2\\left(\\frac{\\lambda}{3}\\right)=0.180447\\) \\(p_3^T=p_2^T\\left(\\frac{\\lambda}{3}\\right)=0.208690\\) \\(p_3^M=p_2^M\\left(\\frac{\\lambda}{2}\\right)=0.083476\\) 2.5.1.3 Modified Poisson Exercise Exercise: Course 3, May 2000, Exercise 37. You are given: \\(p_k\\) denotes the probability that the number of claims equals \\(k\\) for \\(k=0,1,2,\\ldots\\) \\(\\frac{p_n}{p_m}=\\frac{m!}{n!}, m\\ge 0, n\\ge 0\\) Using the corresponding zero-modified claim count distribution with \\(p_0^M=0.1\\), calculate \\(p_1^M\\). 2.6 Mixture Distributions 2.6.1 Mixtures of Finite Populations Suppose that our population consists of several subgroups, each having their own distribution We randomly draw an observation from the population, without knowing which subgroup that we are drawing from For example, suppose that \\(N_1\\) represents claims form “good” drivers and \\(N_2\\) represents claims from “bad” drivers. We draw \\[N = \\begin{cases} N_1 &amp; \\text{with prob~}\\alpha\\\\ N_2 &amp; \\text{with prob~}(1-\\alpha) .\\\\ \\end{cases}\\] Here, \\(\\alpha\\) represents the probability of drawing a “good” driver. Our is said to be a “mixture” of two subgroups 2.6.1.1 Finite Population Mixture Example Exercise. Exam “C” 170. In a certain town the number of common colds an individual will get in a year follows a Poisson distribution that depends on the individual’s age and smoking status. The distribution of the population and the mean number of colds are as follows: Proportion of population Mean number of colds Children 0.3 3 Adult Non-Smokers 0.6 1 Adult Smokers 0.1 4 Calculate the probability that a randomly drawn person has 3 common colds in a year. Calculate the conditional probability that a person with exactly 3 common colds in a year is an adult smoker. 2.6.2 Mixtures of Infinitely Many Populations We can extend the mixture idea to an infinite number of populations. To illustrate, suppose we have a population of drivers. The \\(i\\)th person has their own (personal) expected number of claims, \\(\\lambda_i\\). For some driver’s, \\(\\lambda\\) is small (good drivers), for others it is high (not so good drivers). There is a distribution of \\(\\lambda\\). A convenient distribution is to use a gamma distribution with parameters \\((\\alpha, \\theta)\\). Then, one can check that \\[\\begin{aligned} N &amp;\\sim&amp; \\text{Negative Binomial} (r = \\alpha, \\beta = \\theta) .\\end{aligned}\\] See, for example, KPW, page 84. Mixture is very important in insurance applications, more on this later… 2.6.2.1 Negative Binomial as a Gamma Mixture of Poissons - Example Example. Suppose that \\(N|\\Lambda \\sim\\) Poisson\\((\\Lambda)\\) and that \\(\\Lambda \\sim\\) gamma with mean of 1 and variance of 2. Determine the probability that \\(N=1\\). Solution. For a gamma distribution with parameters \\((\\alpha, \\theta)\\), we have that mean is \\(\\alpha \\theta\\) and the variance is \\(\\alpha \\theta^2\\). Thus \\[\\begin{aligned} \\alpha &amp;= \\frac{1}{2} \\text{ and } \\theta =2.\\end{aligned}\\] Now, one can directly use the negative binomial approach to get \\(r = \\alpha = \\frac{1}{2}\\) and \\(\\beta= \\theta =2\\). Thus \\[\\begin{aligned} \\Pr(N=1) = p_1 &amp;= {1+r-1 \\choose 1}(\\frac{1}{(1+\\beta)^r})(\\frac{\\beta}{1+\\beta})^1 \\\\ &amp;= {1+\\frac{1}{2}-1 \\choose 1}{\\frac{1}{(1+2)^{1/2}}}(\\frac{2}{1+2})^1\\\\ &amp;= \\frac{1}{3^{3/2}} = 0.19245 .\\end{aligned}\\] 2.7 Goodness of Fit 2.7.0.1 Example: Singapore Automobile Data A 1993 portfolio of \\(n=7,483\\) automobile insurance policies from a major insurance company in Singapore. The count variable is the number of automobile accidents per policyholder. There were on average 0.06989 accidents per person. \\[ \\begin{matrix} \\hline \\textbf{Table. Comparison of Observed to Fitted Counts } \\\\ \\textbf{Based on Singapore Automobile Data} \\\\ \\begin{array}{crr} \\hline \\text{Count} &amp; \\text{Observed} &amp; \\text{Fitted Counts using the} \\\\ (k) &amp; (m_k) &amp; \\text{Poisson Distribution} (n\\widehat{p}_k) \\\\ \\hline 0 &amp; 6,996 &amp; 6,977.86 \\\\ 1 &amp; 455 &amp; 487.70 \\\\ 2 &amp; 28 &amp; 17.04 \\\\ 3 &amp; 4 &amp; 0.40 \\\\ 4 &amp; 0 &amp; 0.01 \\\\ \\hline Total &amp; 7,483 &amp; 7,483.00 \\\\ \\hline \\end{array} \\end{matrix}\\] The average is \\(\\bar{N} = \\frac{0\\cdot 6996 + 1 \\cdot 455 + 2 \\cdot 28 + 3 \\cdot 4 + 4 \\cdot 0}{7483} = 0.06989\\). 2.7.0.2 Singapore Data: Adequacy of the Poisson Model With the Poisson distribution The maximum likelihood estimator of \\(\\lambda\\) is \\(\\widehat{\\lambda}=\\overline{N}\\). Estimated probabilities, using \\(\\widehat{\\lambda}\\), are denoted as \\(\\widehat{p}_k\\). For goodness of fit, consider Pearson’s chi-square statistic \\[\\sum_k\\frac{\\left( m_k-n\\widehat{p}_k \\right) ^{2}}{n\\widehat{p}_k}.\\] Assuming that the Poisson distribution is a correct model; this statistic has an asymptotic chi-square distribution The degrees of freedom (\\(df\\)) equals the number of cells minus one minus the number of estimated parameters. For the Singapore data, this is \\(df=5-1-1=3\\). The statistic is 41.98; the basic Poisson model is inadequate. 2.7.0.3 Example. Course C/Exam 4. May 2001, 19. During a one-year period, the number of accidents per day was distributed as follows: Number of Accidents 0 1 2 3 4 5 Number of Days 209 111 33 7 5 2 You use a chi-square test to measure the fit of a Poisson distribution with mean 0.60. The minimum expected number of observations in any group should be 5. The maximum number of groups should be used. Determine the chi-square statistic. 2.8 Exercises Here are a set of exercises that guide the viewer through some of the theoretical foundations of Loss Data Analytics. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C. Frequency Distribution Guided Tutorials "],
["C-Severity.html", "Chapter 3 Modeling Loss Severity 3.1 Basic Distributional Quantities 3.2 Continuous Distributions for Modeling Loss Severity 3.3 Methods of Creating New Distributions 3.4 Coverage Modifications 3.5 Maximum Likelihood Estimation 3.6 Further Resources and Contributors 3.7 Exercises", " Chapter 3 Modeling Loss Severity October 27, 2016 Chapter Preview The traditional loss distribution approach to modeling aggregate losses starts by separately fitting a frequency distribution to the number of losses and a severity distribution to the size of losses. The estimated aggregate loss distribution combines the loss frequency distribution and the loss severity distribution by convolution. Discrete distributions often referred to as counting or frequency distributions were used in Chapter 2 to describe the number of events such as number of accidents to the driver or number of claims to the insurer. Lifetimes, asset values, losses and claim sizes are usually modeled as continuous random variables and as such are modeled using continuous distributions, often referred to as loss or severity distributions. Mixture distributions are used to model phenomenon investigated in a heterogeneous population, such as modelling more than one type of claims in liability insurance (small frequent claims and large relatively rare claims). In this chapter we explore the use of continuous as well as mixture distributions to model the random size of loss. We present key attributes that characterize continuous models and means of creating new distributions from existing ones. In this chapter we explore the effect of coverage modifications, which change the conditions that trigger a payment, such as applying deductibles, limits, or adjusting for inflation, on the distribution of individual loss amounts. 3.1 Basic Distributional Quantities In this section we calculate the basic distributional quantities: moments, percentiles and generating functions. 3.1.1 Moments Let \\(X\\) be a continuous random variable with probability density function \\(f_{X}\\left( x \\right)\\). The k-th raw moment of \\(X\\), denoted by \\(\\mu_{k}^{\\prime}\\), is the expected value of the k-th power of \\(X\\), provided it exists. The first raw moment \\(\\mu_{1}^{\\prime}\\) is the mean of \\(X\\) usually denoted by \\(\\mu\\). The formula for \\(\\mu_{k}^{\\prime}\\) is given as \\[\\mu_{k}^{\\prime} = E\\left( X^{k} \\right) = \\int_{0}^{\\infty}{x^{k}f_{X}\\left( x \\right)dx } .\\] The support of the random variable \\(X\\) is assumed to be nonnegative since actuarial phenomena are rarely negative. The k-th central moment of \\(X\\), denoted by \\(\\mu_{k}\\), is the expected value of the k-th power of the deviation of \\(X\\) from its mean \\(\\mu\\). The formula for \\(\\mu_{k}\\) is given as \\[\\mu_{k} = E\\left\\lbrack {(X - \\mu)}^{k} \\right\\rbrack = \\int_{0}^{\\infty}{\\left( x - \\mu \\right)^{k}f_{X}\\left( x \\right) dx }.\\] The second central moment \\(\\mu_{2}\\) defines the variance of \\(X\\), denoted by \\(\\sigma^{2}\\). The square root of the variance is the standard deviation \\(\\sigma\\). A further characterization of the shape of the distribution includes its degree of symmetry as well as its flatness compared to the normal distribution. The ratio of the third central moment to the cube of the standard deviation \\(\\left( \\mu_{3} / \\sigma^{3} \\right)\\) defines the coefficient of skewness which is a measure of symmetry. A positive coefficient of skewness indicates that the distribution is skewed to the right (positively skewed). The ratio of the fourth central moment to the fourth power of the standard deviation \\(\\left(\\mu_{4} / \\sigma^{4} \\right)\\) defines the coefficient of kurtosis. The normal distribution has a coefficient of kurtosis of 3. Distributions with a coefficient of kurtosis greater than 3 have heavier tails and higher peak than the normal, whereas distributions with a coefficient of kurtosis less than 3 have lighter tails and are flatter. Example 3.1 (SOA) \\(X\\) has a gamma distribution with mean 8 and skewness 1. Find the variance of \\(X\\). Solution The probability density function of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\frac{\\left( x / \\theta \\right)^{\\alpha}}{x\\Gamma\\left( \\alpha \\right)} e^{- x / \\theta} \\] for \\(x &gt; 0\\). For \\(\\alpha&gt;0\\), \\[\\mu_{k}^{\\prime} = E\\left( X^{k} \\right) = \\int_{0}^{\\infty}{\\frac{1}{\\left( \\alpha - 1 \\right)!\\theta^{\\alpha}}x^{k + \\alpha - 1}e^{- x / \\theta} dx} = \\frac{\\Gamma\\left( k + \\alpha \\right)}{\\Gamma\\left( \\alpha \\right)}\\theta^{k}\\] Given \\(\\Gamma\\left( r + 1 \\right) = r\\Gamma\\left( r \\right)\\), then \\(\\mu_{1}^{\\prime} = E\\left( X \\right) = \\alpha\\theta\\), \\(\\mu_{2}^{\\prime} = E\\left( X^{2} \\right) = \\left( \\alpha + 1 \\right)\\alpha\\theta^{2}\\), \\(\\mu_{3}^{\\prime} = E\\left( X^{3} \\right) = \\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3}\\), and \\(Var\\left( X \\right) = \\alpha\\theta^{2}\\). \\[\\text{Skewness} = \\frac{E\\left\\lbrack {(X - \\mu_{1}^{\\prime})}^{3} \\right\\rbrack}{{Var\\left( X \\right)}^{3/2}} = \\frac{\\mu_{3}^{\\prime} - 3\\mu_{2}^{\\prime}\\mu_{1}^{\\prime} + 2{\\mu_{1}^{\\prime}}^{3}}{{Var\\left( X \\right)}^{3/2}} \\\\ = \\frac{\\left( \\alpha + 2 \\right)\\left( \\alpha + 1 \\right)\\alpha\\theta^{3} - 3\\left( \\alpha + 1 \\right)\\alpha^{2}\\theta^{3} + 2\\alpha^{3}\\theta^{3}}{\\left( \\alpha\\theta^{2} \\right)^{3/2}} = \\frac{2}{\\alpha^{1/2}} = 1\\] Hence, \\(\\alpha = 4\\). Since, \\(E\\left( X \\right) = \\alpha\\theta = 8\\), then \\(\\theta = 2\\) and \\(Var\\left( X \\right) = \\alpha\\theta^{2} = 16\\). 3.1.2 Quantiles Percentiles can also be used to describe the characteristics of the distribution of \\(X\\). The 100pth percentile of the distribution of \\(X\\), denoted by \\(\\pi_{p}\\), is the value of \\(X\\) which satisfies \\[F_{X}\\left( {\\pi_{p}}^{-} \\right) \\leq p \\leq F\\left( \\pi_{p} \\right) ,\\] for \\(0 \\leq p \\leq 1\\). The 50-th percentile or the middle point of the distribution, \\(\\pi_{0.5}\\), is the median. Unlike discrete random variables, percentiles of continuous variables are distinct. Example 3.2 (SOA) Let \\(X\\) be a continuous random variable with density function \\(f_{X}\\left( x \\right) = \\theta e^{- \\theta x}\\), for \\(x &gt; 0\\) and 0 elsewhere. If the median of this distribution is \\(\\frac{1}{3}\\), find \\(\\theta\\). Solution \\(F_{X}\\left( x \\right) = 1 - e^{- \\theta x}\\). Then, \\(F_{X}\\left( \\pi_{0.5} \\right) = 1 - e^{- \\theta\\pi_{0.5}} = 0.5\\). Thus, \\(1 - e^{-\\theta / 3} = 0.5\\) and \\(\\theta = 3 \\ln 2\\). 3.1.3 The Moment Generating Function The moment generating function, denoted by \\(M_{X}\\left( t \\right)\\) uniquely characterizes the distribution of \\(X\\). While it is possible for two different distributions to have the same moments and yet still differ, this is not the case with the moment generating function. That is, if two random variables have the same moment generating function, then they have the same distribution. The moment generating is a real function whose k-th derivative at zero is equal to the k-th raw moment of \\(X\\). The moment generating function is given by \\[M_{X}\\left( t \\right) = E\\left( e^{\\text{tX}} \\right) = \\int_{0}^{\\infty}{e^{\\text{tx}}f_{X}\\left( x \\right) dx }\\] for all \\(t\\) for which the expected value exists. Example 3.3 (SOA) The random variable \\(X\\) has an exponential distribution with mean \\(\\frac{1}{b}\\). It is found that \\(M_{X}\\left( - b^{2} \\right) = 0.2\\). Find \\(b\\). Solution \\[M_{X}\\left( t \\right) = E\\left( e^{\\text{tX}} \\right) = \\int_{0}^{\\infty}{e^{\\text{tx}}be^{- bx} dx} = \\int_{0}^{\\infty}{be^{- x\\left( b - t \\right)} dx} = \\frac{b}{\\left( b - t \\right)}.\\] Then, \\[M_{X}\\left( - b^{2} \\right) = \\frac{b}{\\left( b + b^{2} \\right)} = \\frac{1}{\\left( 1 + b \\right)} = 0.2.\\] Thus, \\(b = 4\\). Example 3.4 Let \\(X_{1}\\), \\(X_{2}\\), ., \\(X_{n}\\) be independent \\(\\text{Ga}\\left( \\alpha_{i},\\theta \\right)\\) random variables. Find the distribution of \\(S = \\sum_{i = 1}^{n}X_{i}\\). Solution The moment generating function of \\(S\\) is \\[M_{S}\\left( t \\right) = \\text{E}\\left( e^{\\text{tS}} \\right) = E\\left( e^{t\\sum_{i = 1}^{n}X_{i}} \\right) \\\\ = E\\left( \\prod_{i = 1}^{n}e^{tX_{i}} \\right) = \\prod_{i = 1}^{n}{E\\left( e^{tX_{i}} \\right) = \\prod_{i = 1}^{n}{M_{X_{i}}\\left( t \\right)}} .\\] The moment generating function of \\(X_{i}\\) is \\(M_{X_{i}}\\left( t \\right) = \\left( 1 - \\theta t \\right)^{- \\alpha_{i}}\\). Then, \\[M_{S}\\left( t \\right) = \\prod_{i = 1}^{n}\\left( 1 - \\theta t \\right)^{- \\alpha_{i}} = \\left( 1 - \\theta t \\right)^{- \\sum_{i = 1}^{n}\\alpha_{i}}, \\] indicating that \\(S\\sim Ga\\left( \\sum_{i = 1}^{n}\\alpha_{i},\\theta \\right)\\). By finding the first and second derivatives of \\(M_{S}\\left( t \\right)\\) at zero, we can show that \\(E\\left( S \\right) = \\left. \\ \\frac{\\partial M_{S}\\left( t \\right)}{\\partial t} \\right|_{t = 0} = \\alpha\\theta\\) where \\(\\alpha = \\sum_{i = 1}^{n}\\alpha_{i}\\), and \\[E\\left( S^{2} \\right) = \\left. \\ \\frac{\\partial^{2}M_{S}\\left( t \\right)}{\\partial t^{2}} \\right|_{t = 0} = \\left( \\alpha + 1 \\right)\\alpha\\theta^{2}.\\] Hence, \\(Var\\left( S \\right) = \\alpha\\theta^{2}\\). 3.1.4 Probability Generating Function The probability generating function, denoted by \\(P_{X}\\left( z \\right)\\), also uniquely characterizes the distribution of \\(X\\). It is defined as \\[P_{X}\\left( z \\right) = E\\left( z^{X} \\right) = \\int_{0}^{\\infty}{z^{x}f_{X}\\left( x \\right) dx}\\] for all \\(z\\) for which the expected value exists. We can also use the probability generating function to generate moments of \\(X\\). By taking the k-th derivative of \\(P_{X}\\left( z \\right)\\) with respect to \\(z\\) and evaluate it at \\(z\\ = \\ 1\\), we get \\[E\\left\\lbrack X\\left( X - 1 \\right)\\ldots\\left( X - k + 1 \\right) \\right\\rbrack .\\] 3.2 Continuous Distributions for Modeling Loss Severity In this section we explain the characteristics of distributions suitable for modeling severity of losses, including gamma, Pareto, Weibull and generalized beta distribution of the second kind. Applications for which each distribution may be used are identified. 3.2.1 The Gamma Distribution The gamma distribution is commonly used in modeling claim severity. The traditional approach in modelling losses is to fit separate models for claim frequency and claim severity. When frequency and severity are modeled separately it is common for actuaries to use the Poisson distribution for claim count and the gamma distribution to model severity. An alternative approach for modelling losses that has recently gained popularity is to create a single model for pure premium (average claim cost) that will be described in Chapter 4. The continuous variable \\(X\\) is said to have the gamma distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) if its probability density function is given by \\[f_{X}\\left( x \\right) = \\frac{\\left( x/ \\theta \\right)^{\\alpha}}{x\\Gamma\\left( \\alpha \\right)}\\exp \\left( -x/ \\theta \\right) \\ \\ \\ \\text{for } x &gt; 0 .\\] Note that \\(\\ \\alpha &gt; 0,\\ \\theta &gt; 0\\). Figures 3.1 and 3.2 demonstrate the effect of the scale and shape parameters on the gamma density function. Figure 3.1: Gamma Density, with shape=2 and Varying Scale Figure 3.2: Gamma Density, with scale=100 and Varying Shape R Code for Gamma Density Plots # Varying Scale Gamma Densities scaleparam &lt;- seq(100,250,by=50) shapeparam &lt;- 2:5 x = seq(0,1000,by=1) par(mar = c(4, 4, .1, .1)) fgamma &lt;- dgamma(x, shape = 2, scale = scaleparam[1]) plot(x, fgamma, type = &quot;l&quot;, ylab = &quot;Gamma Density&quot;) for(k in 2:length(scaleparam)){ fgamma &lt;- dgamma(x,shape = 2, scale = scaleparam[k]) lines(x,fgamma, col = k) } legend(&quot;topright&quot;, c(&quot;scale=100&quot;, &quot;scale=150&quot;, &quot;scale=200&quot;, &quot;scale=250&quot;), lty=1, col = 1:4) # Varying Shape Gamma Densities par(mar = c(4, 4, .1, .1)) fgamma &lt;- dgamma(x, shape = shapeparam[1], scale = 100) plot(x, fgamma, type = &quot;l&quot;, ylab = &quot;Gamma Density&quot;) for(k in 2:length(shapeparam)){ fgamma &lt;- dgamma(x,shape = shapeparam[k], scale = 100) lines(x,fgamma, col = k) } legend(&quot;topright&quot;, c(&quot;shape=2&quot;, &quot;shape=3&quot;, &quot;shape=4&quot;, &quot;shape=5&quot;), lty=1, col = 1:4) When \\(\\alpha = 1\\) the gamma reduces to an exponential distribution and when \\(\\alpha = \\frac{n}{2}\\) and \\(\\theta = 2\\) the gamma reduces to a chi-square distribution with \\(n\\) degrees of freedom. As we will see in Section 3.5.2, the chi-square distribution is used extensively in statistical hypothesis testing. The distribution function of the gamma model is the incomplete gamma function, denoted by \\(\\Gamma\\left( \\frac{\\alpha;x}{\\theta} \\right)\\), and defined as \\[F_{X}\\left( x \\right) = \\Gamma\\left( \\alpha; \\frac{x}{\\theta} \\right) = \\frac{1}{\\Gamma\\left( \\alpha \\right)}\\int_{0}^{x /\\theta}t^{\\alpha - 1}e^{- t}\\text{dt}\\] \\(\\alpha &gt; 0,\\ \\theta &gt; 0\\). The \\(k\\)-th moment of the gamma distributed random variable for any positive \\(k\\) is given by \\[E\\left( X^{k} \\right) = \\theta^{k} \\frac{\\Gamma\\left( \\alpha + k \\right)}{\\Gamma\\left( \\alpha \\right)} \\ \\ \\ \\text{for } k &gt; 0 .\\] The mean and variance are given by \\(E\\left( X \\right) = \\alpha\\theta\\) and \\(Var\\left( X \\right) = \\alpha\\theta^{2}\\), respectively. Since all moments exist for any positive \\(k\\), the gamma distribution is considered a light tailed distribution, which may not be suitable for modeling risky assets as it will not provide a realistic assessment of the likelihood of severe losses. 3.2.2 The Pareto Distribution The Pareto distribution, named after the Italian economist Vilfredo Pareto (1843-1923), has many economic and financial applications. It is a positively skewed and heavy-tailed distribution which makes it suitable for modeling income, high-risk insurance claims and severity of large casualty losses. The survival function of the Pareto distribution which decays slowly to zero was first used to describe the distribution of income where a small percentage of the population holds a large proportion of the total wealth. For extreme insurance claims, the tail of the severity distribution (losses in excess of a threshold) can be modelled using a Pareto distribution. The continuous variable \\(X\\) is said to have the Pareto distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) if its pdf is given by \\[f_{X}\\left( x \\right) = \\frac{\\alpha\\theta^{\\alpha}}{\\left( x + \\theta \\right)^{\\alpha + 1}} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] Figures 3.3 and 3.4 demonstrate the effect of the scale and shape parameters on the Pareto density function. Figure 3.3: Pareto Density, with shape=3 and Varying Scale Figure 3.4: Pareto Density, with scale=2000 and Varying Shape R Code for Pareto Density Plots # Varying Scale Pareto Densities #install.packages(&quot;VGAM&quot;) library(VGAM) scaleparam &lt;- seq(2000,3500,500) shapeparam &lt;- 1:4 z&lt;- seq(1,3000,by=1) fpareto &lt;- dpareto(z, shape = 3, scale = scaleparam[1]) plot(z, fpareto, ylim=c(0,0.002),type = &quot;l&quot;, ylab = &quot;Pareto Density&quot;) for(k in 2:length(shapeparam)){ fpareto &lt;- dpareto(z,shape = 3, scale = scaleparam[k]) lines(z,fpareto, col = k) } legend(&quot;topright&quot;, c(&quot;scale=2000&quot;, &quot;scale=2500&quot;, &quot;scale=3000&quot;, &quot;scale=3500&quot;), lty=1, col = 1:4) # Varying Shape Pareto Densities fpareto &lt;- dpareto(z, shape = shapeparam[1], scale = 2000) plot(z, fpareto, ylim=c(0,0.002),type = &quot;l&quot;, ylab = &quot;Pareto Density&quot;) for(k in 2:length(shapeparam)){ fpareto &lt;- dpareto(z,shape = shapeparam[k], scale = 2000) lines(z,fpareto, col = k)} legend(&quot;topright&quot;, c(&quot;shape=1&quot;, &quot;shape=2&quot;, &quot;shape=3&quot;, &quot;shape=4&quot;), lty=1, col = 1:4) The distribution function of the Pareto distribution is given by \\[F_{X}\\left( x \\right) = 1 - \\left( \\frac{\\theta}{x + \\theta} \\right)^{\\alpha} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] It can be easily seen that the hazard function of the Pareto distribution is a decreasing function in \\(x\\), another indication that the distribution is heavy tailed. The \\(k\\)-th moment of the Pareto distributed random variable exists, if and only if, \\(\\alpha &gt; k\\). If \\(k\\) is a positive integer then \\[E\\left( X^{k} \\right) = \\frac{k!\\theta^{k}}{\\left( \\alpha - 1 \\right)\\cdots\\left( \\alpha - k \\right)} \\ \\ \\ \\alpha &gt; k.\\] The mean and variance are given by \\[E\\left( X \\right) = \\frac{\\theta}{\\alpha - 1} \\ \\ \\ \\text{for } \\alpha &gt; 1\\] and \\[Var\\left( X \\right) = \\frac{\\alpha\\theta^{2}}{\\left( \\alpha - 1 \\right)^{2}\\left( \\alpha - 2 \\right)} \\ \\ \\ \\text{for } \\alpha &gt; 2,\\]respectively. Example 3.5 The claim size of an insurance portfolio follows the Pareto distribution with mean and variance of 40 and 1800 respectively. Find The shape and scale parameters. The 95-th percentile of this distribution. Solution \\(E\\left( X \\right) = \\frac{\\theta}{\\alpha - 1} = 40\\) and \\(Var\\left( X \\right) = \\frac{\\alpha\\theta^{2}}{\\left( \\alpha - 1 \\right)^{2}\\left( \\alpha - 2 \\right)} = 1800\\). By dividing the square of the first equation by the second we get \\(\\frac{\\alpha - 2}{\\alpha} = \\frac{40^{2}}{1800}\\). Thus, \\(\\alpha = 18.02\\) and \\(\\theta = 680.72\\). The 95-th percentile, \\(\\pi_{0.95}\\), satisfies the equation \\[F_{X}\\left( \\pi_{0.95} \\right) = 1 - \\left( \\frac{680.72}{\\pi_{0.95} + 680.72} \\right)^{18.02} = 0.95.\\] Thus, \\(\\pi_{0.95} = 122.96\\). 3.2.3 The Weibull Distribution The Weibull distribution, named after the Swedish physicist Waloddi Weibull (1887-1979) is widely used in reliability, life data analysis, weather forecasts and general insurance claims. Truncated data arise frequently in insurance studies. The Weibull distribution is particularly useful in modeling left-truncated claim severity distributions. Weibull was used to model excess of loss treaty over automobile insurance as well as earthquake inter-arrival times. The continuous variable \\(X\\) is said to have the Weibull distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\theta\\) if its probability density function is given by \\[f_{X}\\left( x \\right) = \\frac{\\alpha}{\\theta}\\left( \\frac{x}{\\theta} \\right)^{\\alpha - 1} \\exp \\left(- \\left( \\frac{x}{\\theta} \\right)^{\\alpha}\\right) \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] Figures 3.5 and 3.6 demonstrate the effects of the scale and shape parameters on the Weibull density function. Figure 3.5: Weibull Density, with shape=3 and Varying Scale Figure 3.6: Weibull Density, with scale=100 and Varying Shape R Code for Weibull Density Plots # Varying Scale Weibull Densities z&lt;- seq(0,400,by=1) scaleparam &lt;- seq(50,200,50) shapeparam &lt;- seq(1.5,3,0.5) plot(z, dweibull(z, shape = 3, scale = scaleparam[1]), type = &quot;l&quot;, ylab = &quot;Weibull density&quot;) for(k in 2:length(scaleparam)){ lines(z,dweibull(z,shape = 3, scale = scaleparam[k]), col = k)} legend(&quot;topright&quot;, c(&quot;scale=50&quot;, &quot;scale=100&quot;, &quot;scale=150&quot;, &quot;scale=200&quot;), lty=1, col = 1:4) # Varying Shape Weibull Densities plot(z, dweibull(z, shape = shapeparam[1], scale = 100), ylim=c(0,0.012), type = &quot;l&quot;, ylab = &quot;Weibull density&quot;) for(k in 2:length(shapeparam)){ lines(z,dweibull(z,shape = shapeparam[k], scale = 100), col = k)} legend(&quot;topright&quot;, c(&quot;shape=1.5&quot;, &quot;shape=2&quot;, &quot;shape=2.5&quot;, &quot;shape=3&quot;), lty=1, col = 1:4) The distribution function of the Weibull distribution is given by \\[F_{X}\\left( x \\right) = 1 - e^{- \\left( x / \\theta \\right)^{\\alpha}} \\ \\ \\ x &gt; 0,\\ \\alpha &gt; 0,\\ \\theta &gt; 0.\\] It can be easily seen that the shape parameter \\(\\alpha\\) describes the shape of the hazard function of the Weibull distribution. The hazard function is a decreasing function when \\(\\alpha &lt; 1\\), constant when \\(\\alpha = 1\\) and increasing when \\(\\alpha &gt; 1\\). This behavior of the hazard function makes the Weibull distribution a suitable model for a wide variety of phenomena such as weather forecasting, electrical and industrial engineering, insurance modeling and financial risk analysis. The \\(k\\)-th moment of the Weibull distributed random variable is given by \\[E\\left( X^{k} \\right) = \\theta^{k}\\Gamma\\left( 1 + \\frac{k}{\\alpha} \\right) .\\] The mean and variance are given by \\[E\\left( X \\right) = \\theta\\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right)\\] and \\[Var(X)= \\theta^{2}\\left( \\Gamma\\left( 1 + \\frac{2}{\\alpha} \\right) - \\left\\lbrack \\Gamma\\left( 1 + \\frac{1}{\\alpha} \\right) \\right\\rbrack ^{2}\\right),\\] respectively. Example 3.6 Suppose that the probability distribution of the lifetime of AIDS patients (in months) from the time of diagnosis is described by the Weibull distribution with shape parameter 1.2 and scale parameter 33.33. Find the probability that a randomly selected person from this population survives at least 12 months, A random sample of 10 patients will be selected from this population. What is the probability that at most two will die within one year of diagnosis. Find the 99-th percentile of this distribution. Solution Let \\(X\\) be the lifetime of AIDS patients (in months) \\[{\\Pr\\left( X \\geq 12 \\right) = S}_{X}\\left( 12 \\right) = e^{- \\left( \\frac{12}{33.33} \\right)^{1.2}} = 0.746.\\] Let \\(Y\\) be the number of patients who die within one year of diagnosis. Then, \\(Y\\sim Bin\\left( 10,\\ 0.254 \\right)\\) and \\(\\Pr\\left( Y \\leq 2 \\right) = 0.514.\\) Let \\(\\pi_{0.99}\\) denote the 99-th percentile of this distribution. Then, \\[S_{X}\\left( \\pi_{0.99} \\right) = \\exp\\left\\{- \\left( \\frac{\\pi_{0.99}}{33.33} \\right)^{1.2}\\right\\} = 0.01\\] and \\(\\pi_{0.99} = 118.99\\). 3.2.4 The Generalized Beta Distribution of the Second Kind The Generalized Beta Distribution of the Second Kind (GB2) was introduced by Venter (1983) in the context of insurance loss modeling and by McDonald (1984) as an income and wealth distribution. It is a four-parameter very flexible distribution that can model positively as well as negatively skewed distributions. The continuous variable \\(X\\) is said to have the GB2 distribution with parameters \\(a\\), \\(b\\), \\(\\alpha\\) and \\(\\beta\\) if its probability density function is given by \\[f_{X}\\left( x \\right) = \\frac{ax^{a \\alpha - 1}}{b^{a \\alpha}B\\left( \\alpha,\\beta \\right)\\left\\lbrack 1 + \\left( x/b \\right)^{a} \\right\\rbrack^{\\alpha + \\beta}} \\ \\ \\ \\text{for } x &gt; 0,\\] \\(a,b,\\alpha,\\beta &gt; 0\\), and where the beta function \\(B\\left( \\alpha,\\beta \\right)\\) is defined as \\[B\\left( \\alpha,\\beta \\right) = \\int_{0}^{1}{t^{\\alpha - 1}\\left( 1 - t \\right)^{\\beta - 1}}\\text{dt}.\\] The GB2 provides a model for heavy as well as light tailed data. It includes the exponential, gamma, Weibull, Burr, Lomax, F, chi-square, Rayleigh, lognormal and log-logistic as special or limiting cases. For example, by setting the parameters \\(a = \\alpha = \\beta = 1\\), then the GB2 reduces to the log-logistic distribution. When \\(a = 1\\) and \\(\\beta \\rightarrow \\infty\\), it reduces to the gamma distribution and when \\(\\alpha = 1\\) and \\(\\beta \\rightarrow \\infty\\), it reduces to the Weibull distribution. The \\(k\\)-th moment of the GB2 distributed random variable is given by \\[E\\left( X^{k} \\right) = \\frac{b^{k}\\left( \\alpha + \\frac{k}{a},\\beta - \\frac{k}{a} \\right)}{\\left( \\alpha,\\beta \\right)}, \\ \\ \\ k &gt; 0.\\] Earlier applications of the GB2 were on income data and more recently have been used to model long-tailed claims data. GB2 was used to model different types of automobile insurance claims, severity of fire losses as well as medical insurance claim data. 3.3 Methods of Creating New Distributions In this section we understand connections among the distributions; give insights into when a distribution is preferred when compared to alternatives; provide foundations for creating new distributions. 3.3.1 Functions of Random Variables and their Distributions In Section 3.2 we discussed some elementary known distributions. In this section we discuss means of creating new parametric probability distributions from existing ones. Let \\(X\\) be a continuous random variable with a known probability density function \\(f_{X}(x)\\) and distribution function \\(F_{X}(x)\\). Consider the transformation \\(Y = g\\left( X \\right)\\), where \\(g(X)\\) is a one-to-one transformation defining a new random variable \\(Y\\). We can use the distribution function technique, the change-of-variable technique or the moment-generating function technique to find the probability density function of the variable of interest \\(Y\\). In this section we apply the following techniques for creating new families of distributions: (a) multiplication by a constant (b) raising to a power, (c) exponentiation and (d) mixing. 3.3.2 Multiplication by a Constant If claim data show change over time then such transformation can be useful to adjust for inflation. If the level of inflation is positive then claim costs are rising, and if it is negative then costs are falling. To adjust for inflation we multiply the cost \\(X\\) by 1+ inflation rate (negative inflation is deflation). To account for currency impact on claim costs we also use a transformation to apply currency conversion from a base to a counter currency. Consider the transformation \\(Y = cX\\), where \\(c &gt; 0\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( cX \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{c} \\right) = F_{X}\\left( \\frac{y}{c} \\right).\\] Hence, the probability density function of interest \\(f_{Y}(y)\\) can be written as \\[f_{Y}\\left( y \\right) = \\frac{1}{c}f_{X}\\left( \\frac{y}{c} \\right).\\] Suppose that \\(X\\) belongs to a certain set of parametric distributions and define a rescaled version \\(Y\\ = \\ cX\\), \\(c\\ &gt; \\ 0\\). If \\(Y\\) is in the same set of distributions then the distribution is said to be a scale distribution. When a member of a scale distribution is multiplied by a constant \\(c\\) (\\(c &gt; 0\\)), the scale parameter for this scale distribution meets two conditions: The parameter is changed by multiplying by \\(c\\); All other parameter remain unchanged. Example 3.7 (SOA) The aggregate losses of Eiffel Auto Insurance are denoted in Euro currency and follow a Lognormal distribution with \\(\\mu = 8\\) and \\(\\sigma = 2\\). Given that 1 euro \\(=\\) 1.3 dollars, find the set of lognormal parameters, which describe the distribution of Eiffel’s losses in dollars? Solution Let \\(X\\) and \\(Y\\) denote the aggregate losses of Eiffel Auto Insurance in euro currency and dollars respectively. Then, \\(Y = 1.3X\\). \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( 1.3X \\leq y \\right) = \\Pr\\left( X \\leq \\frac{y}{1.3} \\right) = F_{X}\\left( \\frac{y}{1.3} \\right).\\] \\(X\\) follows a lognormal distribution with parameters \\(\\mu = 8\\) and \\(\\sigma = 2\\). The probability density function of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\ln x - \\mu}{\\sigma} \\right)^{2}\\right\\} \\ \\ \\ \\text{for } x &gt; 0.\\] Then, the probability density function of interest \\(f_{Y}(y)\\) is \\[f_{Y}\\left( y \\right) = \\frac{1}{1.3}f_{X}\\left( \\frac{y}{1.3} \\right) \\\\ = \\frac{1}{1.3}\\frac{1.3}{y \\sigma \\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\ln\\left( y/1.3 \\right) - \\mu}{\\sigma} \\right)^{2}\\right\\} \\\\ = \\frac{1}{y \\sigma\\sqrt{2\\pi}}\\exp \\left\\{- \\frac{1}{2}\\left( \\frac{\\ln y - \\left( \\ln 1.3 + \\mu \\right)}{\\sigma} \\right)^{2}\\right\\}.\\] Then \\(Y\\) follows a lognormal distribution with parameters \\(\\ln 1.3 + \\mu = 8.26\\) and \\(\\sigma = 2.00\\). If we let \\(\\mu = ln(m)\\) then it can be easily seen that \\(m\\)=\\(e^{\\mu}\\) is the scale parameter which was multiplied by 1.3 while \\(\\sigma\\) is the shape parameter that remained unchanged. Example 3.8 Demonstrate that the gamma distribution is a scale distribution. Solution Let \\(X\\sim Ga(\\alpha,\\theta)\\) and \\(Y = cX\\), then \\[f_{Y}\\left( y \\right) = \\frac{1}{c}f_{X}\\left( \\frac{y}{c} \\right) = \\frac{\\left( \\frac{y}{c\\theta} \\right)^{\\alpha}}{y\\Gamma\\left( \\alpha \\right)}\\exp \\left( - \\frac{y}{c\\theta} \\right) .\\] We can see that \\(Y\\sim Ga(\\alpha,c\\theta)\\) indicating that gamma is a scale distribution and \\(\\theta\\) is a scale parameter. 3.3.3 Raising to a Power In the previous section we have talked about the flexibility of the Weibull distribution in fitting reliability data. Looking to the origins of the Weibull distribution, we recognize that the Weibull is a power transformation of the exponential distribution. This is an application of another type of transformation which involves raising the random variable to a power. Consider the transformation \\(Y = X^{\\tau}\\), where \\(\\tau &gt; 0\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\leq y^{1/ \\tau} \\right) = F_{X}\\left( y^{1/ \\tau} \\right).\\] Hence, the probability density function of interest \\(f_{Y}(y)\\) can be written as \\[f_{Y}(y) = \\frac{1}{\\tau} y^{1/ \\tau - 1} f_{X}\\left( y^{1/ \\tau} \\right).\\] On the other hand, if \\(\\tau &lt; 0\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( X^{\\tau} \\leq y \\right) = \\Pr\\left( X \\geq y^{1/ \\tau} \\right) = 1 - F_{X}\\left( y^{1/ \\tau} \\right), \\] and \\[f_{Y}(y) = \\left| \\frac{1}{\\tau} \\right|{y^{1/ \\tau - 1}f}_{X}\\left( y^{1/ \\tau} \\right).\\] Example 3.9 We assume that \\(X\\) follows the exponential distribution with mean \\(\\theta\\) and consider the transformed variable \\(Y = X^{\\tau}\\). Show that \\(Y\\) follows the Weibull distribution when \\(\\tau\\) is positive and determine the parameters of the Weibull distribution. Solution \\[f_{X}(x) = \\frac{1}{\\theta}e^{- x/ \\theta} \\ \\ \\ \\, x &gt; 0.\\] \\[f_{Y}\\left( y \\right) = \\frac{1}{\\tau}{y^{\\frac{1}{\\tau} - 1}f}_{X}\\left( y^{\\frac{1}{\\tau}} \\right) \\\\ = \\frac{1}{\\tau \\theta }y^{\\frac{1}{\\tau} - 1}e^{- \\frac{y^{\\frac{1}{\\tau}}}{\\theta}} = \\frac{\\alpha}{\\beta}\\left( \\frac{y}{\\beta} \\right)^{\\alpha - 1}e^{- \\left( y/ \\beta \\right)^{\\alpha}}.\\] where \\(\\alpha = \\frac{1}{\\tau}\\) and \\(\\beta = \\theta^{\\tau}\\). Then, \\(Y\\) follows the Weibull distribution with shape parameter \\(\\alpha\\) and scale parameter \\(\\beta\\). 3.3.4 Exponentiation The normal distribution is a very popular model for a wide number of applications and when the sample size is large, it can serve as an approximate distribution for other models. If the random variable \\(X\\) has a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), then \\(Y = e^{X}\\) has lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^{2}\\). The lognormal random variable has a lower bound of zero, is positively skewed and has a long right tail. A lognormal distribution is commonly used to describe distributions of financial assets such as stock prices. It is also used in fitting claim amounts for automobile as well as health insurance. This is an example of another type of transformation which involves exponentiation. Consider the transformation \\(Y = e^{X}\\), then the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\ln y \\right) = F_{X}\\left( \\ln y \\right).\\] Hence, the probability density function of interest \\(f_{Y}(y)\\) can be written as \\[f_{Y}(y) = \\frac{1}{y}f_{X}\\left( \\ln y \\right).\\] Example 3.10 (SOA) \\(X\\) has a uniform distribution on the interval \\((0,\\ c)\\). \\(Y = e^{X}\\). Find the distribution of \\(Y\\). Solution \\[F_{Y}\\left( y \\right) = \\Pr\\left( Y \\leq y \\right) = \\Pr\\left( e^{X} \\leq y \\right) = \\Pr\\left( X \\leq \\ln y \\right) = F_{X}\\left( \\ln y \\right).\\] Then, \\[f_{Y}\\left( y \\right) = \\frac{1}{y}f_{X}\\left(\\ln y \\right) = \\frac{1}{\\text{cy}}. \\] Since \\(0 &lt; x &lt; c\\), then \\(1 &lt; y &lt; e^{c}\\). 3.3.5 Finite Mixtures Mixture distributions represent a useful way of modelling data that are drawn from a heterogeneous population. This parent population can be thought to be divided into multiple subpopulations with distinct distributions. 3.3.5.1 Two-point Mixture If the underlying phenomenon is diverse and can actually be described as two phenomena representing two subpopulations with different modes, we can construct the two point mixture random variable \\(X\\). Given random variables \\(X_{1}\\) and \\(X_{2}\\), with probability density functions \\(f_{X_{1}}\\left( x \\right)\\) and \\(f_{X_{2}}\\left( x \\right)\\) respectively, the probability density function of \\(X\\) is the weighted average of the component probability density function \\(f_{X_{1}}\\left( x \\right)\\) and \\(f_{X_{2}}\\left( x \\right)\\). The probability density function and distribution function of \\(X\\) are given by \\[f_{X}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right),\\] and \\[F_{X}\\left( x \\right) = aF_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)F_{X_{2}}\\left( x \\right),\\] for \\(0 &lt; a &lt;1\\), where the mixing parameters \\(a\\) and \\((1 - a)\\) represent the proportions of data points that fall under each of the two subpopulations respectively. This weighted average can be applied to a number of other distribution related quantities. The k-th moment and moment generating function of \\(X\\) are given by \\(E\\left( X^{k} \\right) = aE\\left( X_{1}^{K} \\right) + \\left( 1 - a \\right)E\\left( X_{2}^{k} \\right)\\), and \\[M_{X}\\left( t \\right) = aM_{X_{1}}\\left( t \\right) + \\left( 1 - a \\right)M_{X_{2}}\\left( t \\right),\\] respectively. Example 3.11 (SOA) The distribution of the random variable \\(X\\) is an equally weighted mixture of two Poisson distributions with parameters \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\) respectively. The mean and variance of \\(X\\) are 4 and 13, respectively. Determine \\(\\Pr\\left( X &gt; 2 \\right)\\). Solution \\[E\\left( X \\right) = 0.5\\lambda_{1} + 0.5\\lambda_{2} = 4\\] \\[E\\left( X^{2} \\right) = 0.5\\left( \\lambda_{1} + \\lambda_{1}^{2} \\right) + 0.5\\left( \\lambda_{2} + \\lambda_{2}^{2} \\right) = 13 + 16\\] Simplifying the two equations we get \\(\\lambda_{1} + \\lambda_{2} = 8\\) and \\(\\lambda_{1}^{2} + \\lambda_{2}^{2} = 50\\). Then, the parameters of the two Poisson distributions are 1 and 7. \\[\\Pr\\left( X &gt; 2 \\right) = 0.5\\Pr\\left( X_{1} &gt; 2 \\right) + 0.5\\Pr\\left( X_{2} &gt; 2 \\right) = 0.05\\] 3.3.5.2 k-point Mixture In case of finite mixture distributions, the random variable of interest \\(X\\) has a probability \\(p_{i}\\) of being drawn from homogeneous subpopulation \\(i\\), where \\(i = 1,2,\\ldots,k\\) and \\(k\\) is the initially specified number of subpopulations in our mixture. The mixing parameter \\(p_{i}\\) represents the proportion of observations from subpopulation \\(i\\). Consider the random variable \\(X\\) generated from \\(k\\) distinct subpopulations, where subpopulation \\(i\\) is modeled by the continuous distribution \\(f_{X_{i}}\\left( x \\right)\\). The probability distribution of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}f_{X_{i}}\\left( x \\right)},\\] where \\(0 &lt; p_{i} &lt; 1\\) and \\(\\sum_{i = 1}^{k} p_{i} = 1\\). This model is often referred to as a finite mixture or a \\(k\\) point mixture. The distribution function, \\(r\\)-th moment and moment generating functions of the \\(k\\)-th point mixture are given as \\[F_{X}\\left( x \\right) = \\sum_{i = 1}^{k}{p_{i}F_{X_{i}}\\left( x \\right)},\\] \\[E\\left( X^{r} \\right) = \\sum_{i = 1}^{k}{p_{i}E\\left( X_{i}^{r} \\right)}, \\text{and}\\] \\[M_{X}\\left( t \\right) = \\sum_{i = 1}^{k}{p_{i}M_{X_{i}}\\left( t \\right)},\\] respectively. Example 3.12 (SOA) \\(Y_{1}\\) is a mixture of \\(X_{1}\\) and \\(X_{2}\\) with mixing weights \\(a\\) and \\((1 - a)\\). \\(Y_{2}\\) is a mixture of \\(X_{3}\\) and \\(X_{4}\\) with mixing weights \\(b\\) and \\((1 - b)\\). \\(Z\\) is a mixture of \\(Y_{1}\\) and \\(Y_{2}\\) with mixing weights \\(c\\) and \\((1 - c)\\). Show that \\(Z\\) is a mixture of \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\) and \\(X_{4}\\), and find the mixing weights. Solution \\[f_{Y_{1}}\\left( x \\right) = af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right)\\] \\[f_{Y_{2}}\\left( x \\right) = bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right)\\] \\[f_{Z}\\left( x \\right) = cf_{Y_{1}}\\left( x \\right) + \\left( 1 - c \\right)f_{Y_{2}}\\left( x \\right)\\] \\[f_{Z}\\left( x \\right) = c\\left\\lbrack af_{X_{1}}\\left( x \\right) + \\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) \\right\\rbrack + \\left( 1 - c \\right)\\left\\lbrack bf_{X_{3}}\\left( x \\right) + \\left( 1 - b \\right)f_{X_{4}}\\left( x \\right) \\right\\rbrack\\] \\(= caf_{X_{1}}\\left( x \\right) + c\\left( 1 - a \\right)f_{X_{2}}\\left( x \\right) + \\left( 1 - c \\right)bf_{X_{3}}\\left( x \\right) + (1 - c)\\left( 1 - b \\right)f_{X_{4}}\\left( x \\right)\\). Then, \\(Z\\) is a mixture of \\(X_{1}\\), \\(X_{2}\\), \\(X_{3}\\) and \\(X_{4}\\), with mixing weights \\(\\text{ca}\\), \\(c\\left( 1 - a \\right)\\), \\(\\left( 1 - c \\right)b\\) and \\((1 - c)\\left( 1 - b \\right)\\). 3.3.6 Continuous Mixtures A mixture with a very large number of subpopulations (\\(k\\) goes to infinity) is often referred to as a continuous mixture. In a continuous mixture, subpopulations are not distinguished by a discrete mixing parameter but by a continuous variable \\(\\theta\\), where \\(\\theta\\) plays the role of \\(p_{i}\\) in the finite mixture. Consider the random variable \\(X\\) with a distribution depending on a parameter \\(\\theta\\), where \\(\\theta\\) itself is a continuous random variable. This description yields the following model for \\(X\\) \\[f_{X}\\left( x \\right) = \\int_{0}^{\\infty}{f_{X}\\left( x\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)} d \\theta ,\\] where \\(f_{X}\\left( x\\left| \\theta \\right.\\ \\right)\\) is the conditional distribution of \\(X\\) at a particular value of \\(\\theta\\) and \\(g\\left( \\theta \\right)\\) is the probability statement made about the unknown parameter \\(\\theta\\), known as the prior distribution of \\(\\theta\\) (the prior information or expert opinion to be used in the analysis). The distribution function, \\(k\\)-th moment and moment generating functions of the continuous mixture are given as \\[F_{X}\\left( x \\right) = \\int_{-\\infty}^{\\infty}{F_{X}\\left( x\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)} d \\theta,\\] \\[E\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{E\\left( X^{k}\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)}d \\theta,\\] \\[M_{X}\\left( t \\right) = E\\left( e^{t X} \\right) = \\int_{-\\infty}^{\\infty}{E\\left( e^{ tx}\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)}d \\theta, \\] respectively. The \\(k\\)-th moments of the mixture distribution can be rewritten as \\[E\\left( X^{k} \\right) = \\int_{-\\infty}^{\\infty}{E\\left( X^{k}\\left| \\theta \\right.\\ \\right)g\\left( \\theta \\right)}d\\theta = E\\left\\lbrack E\\left( X^{k}\\left| \\theta \\right.\\ \\right) \\right\\rbrack .\\] In particular the mean and variance of \\(X\\) are given by \\[E\\left( X \\right) = E\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack\\] and \\[Var\\left( X \\right) = E\\left\\lbrack Var\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack + Var\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack .\\] Example 3.13 (SOA) \\(X\\) has a binomial distribution with a mean of \\(100q\\) and a variance of \\(100q\\left( 1 - q \\right)\\) and \\(q\\) has a beta distribution with parameters \\(a = 3\\) and \\(b = 2\\). Find the unconditional mean and variance of \\(X\\). Solution \\(E\\left( q \\right) = \\frac{a}{a + b} = \\frac{3}{5}\\) and \\(E\\left( q^{2} \\right) = \\frac{a\\left( a + 1 \\right)}{\\left( a + b \\right)\\left( a + b + 1 \\right)} = \\frac{2}{5}\\). \\(E\\left( X \\right) = E\\left\\lbrack E\\left( X\\left| q \\right.\\ \\right) \\right\\rbrack = E\\left( 100q \\right) = 100E\\left( q \\right) = 60\\), \\[Var\\left( X \\right) = E\\left\\lbrack Var\\left( X\\left| q \\right.\\ \\right) \\right\\rbrack + Var\\left\\lbrack E\\left( X\\left| q \\right.\\ \\right) \\right\\rbrack = E\\left\\lbrack 100q\\left( 1 - q \\right) \\right\\rbrack + Var\\left( 100q \\right)\\] \\(= 100E\\left( q \\right) - 100E\\left( q^{2} \\right) + 100^{2}V\\left( q \\right) = 420\\). Exercise 3.14 (SOA) Claim sizes, \\(X\\), are uniform on for each policyholder. varies by policyholder according to an exponential distribution with mean 5. Find the unconditional distribution, mean and variance of \\(X\\). Solution The conditional distribution of \\(X\\) is \\(f_{X}\\left( \\left. \\ x \\right|\\theta \\right) = \\frac{1}{10}\\) for \\(\\theta &lt; x &lt; \\theta + 10\\). The prior distribution of \\(\\theta\\) is \\(g\\left( \\theta \\right) = \\frac{1}{5}e^{- \\frac{\\theta}{5}}\\) for \\(0 &lt; \\theta &lt; \\infty\\). The conditional mean and variance of \\(X\\) are given by \\[E\\left( \\left. \\ X \\right|\\theta \\right) = \\frac{\\theta + \\theta + 10}{2} = \\theta + 5\\] and \\[Var\\left( \\left. \\ X \\right|\\theta \\right) = \\frac{\\left\\lbrack \\left( \\theta + 10 \\right) - \\theta \\right\\rbrack^{2}}{12} = \\frac{100}{12}, \\] respectively. Hence, the unconditional mean and variance of \\(X\\) are given by \\[E\\left( X \\right) = E\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack = E\\left( \\theta + 5 \\right) = E\\left( \\theta \\right) + 5 = 5 + 5 = 10,\\] and \\[Var\\left( X \\right) = E\\left\\lbrack V\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack + Var\\left\\lbrack E\\left( X\\left| \\theta \\right.\\ \\right) \\right\\rbrack \\\\ = E\\left( \\frac{100}{12} \\right) + Var\\left( \\theta + 5 \\right) = 8.33 + Var\\left( \\theta \\right) = 33.33. \\] The unconditional distribution of \\(X\\) is \\[f_{X}\\left( x \\right) = \\int_{}^{}{f_{X}\\left( x |\\theta \\right) ~g\\left( \\theta \\right) d \\theta} .\\] \\[f_{X}\\left( x \\right) = \\left\\{ \\begin{matrix} \\int_{0}^{x}{\\frac{1}{50}e^{- \\frac{\\theta}{5}}d\\theta = \\frac{1}{10}\\left( 1 - e^{- \\frac{x}{5}} \\right)} &amp; 0 \\leq x \\leq 10, \\\\ \\int_{x - 10}^{x}{\\frac{1}{50}e^{- \\frac{\\theta}{5}} d\\theta} = \\frac{1}{10}\\left( e^{- \\frac{\\left( x - 10 \\right)}{5}} - e^{- \\frac{x}{5}} \\right) &amp; 10 &lt; x &lt; \\infty. \\\\ \\end{matrix} \\right.\\ \\] 3.4 Coverage Modifications In this section we evaluate the impacts of coverage modifications: a) deductibles, b) policy limit, c) coinsurance and inflation on insurer’s costs. 3.4.1 Policy Deductibles Under an ordinary deductible policy, the insured (policyholder) agrees to cover a fixed amount of an insurance claim before the insurer starts to pay. This fixed expense paid out of pocket is called the deductible and often denoted by \\(d\\). The insurer is responsible for covering the loss \\(X\\) less the deductible \\(d\\). Depending on the agreement, the deductible may apply to each covered loss or to a defined benefit period (month, year, etc.) Deductibles eliminate a large number of small claims, reduce costs of handling and processing these claims, reduce premiums for the policyholders and reduce moral hazard. Moral hazard occurs when the insured takes more risks, increasing the chances of loss due to perils insured against, knowing that the insurer will incur the cost (e.g. a policyholder with collision insurance may be encouraged to drive recklessly). The larger the deductible, the less the insured pays in premiums for an insurance policy. Let \\(X\\) denote the loss incurred to the insured and \\(Y\\) denote the amount of paid claim by the insurer. Speaking of the benefit paid to the policyholder, we differentiate between two variables: The payment per loss and the payment per payment. The payment per loss variable, denoted by \\(Y^{L}\\), includes losses for which a payment is made as well as losses less than the deductible and hence is defined as \\[Y^{L} = \\left( X - d \\right)_{+} = \\left\\{ \\begin{array}{cc} 0 &amp; X &lt; d, \\\\ X - d &amp; X &gt; d \\end{array} \\right. .\\] \\(Y^{L}\\) is often referred to as left censored and shifted variable because the values below \\(d\\) are not ignored and all losses are shifted by a value \\(d\\). On the other hand, the payment per payment variable, denoted by \\(Y^{P}\\), is not defined when there is no payment and only includes losses for which a payment is made. The variable is defined as \\[Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\le d \\\\ X - d &amp; X &gt; d \\end{matrix} \\right. \\] \\(Y^{P}\\) is often referred to as left truncated and shifted variable or excess loss variable because the claims smaller than \\(d\\) are not reported and values above \\(d\\) are shifted by \\(d\\). Even when the distribution of \\(X\\) is continuous, the distribution of \\(Y^{L}\\) is partly discrete and partly continuous. The discrete part of the distribution is concentrated at \\(Y = 0\\) (when \\(X \\leq d\\)) and the continuous part is spread over the interval \\(Y &gt; 0\\) (when \\(X &gt; d\\)). For the discrete part, the probability that no payment is made is the probability that losses fall below the deductible; that is, \\[\\Pr\\left( Y^{L} = 0 \\right) = \\Pr\\left( X \\leq d \\right) = F_{X}\\left( d \\right).\\] Using the transformation \\(Y^{L} = X - d\\) for the continuous part of the distribution, we can find the probability density function of \\(Y^{L}\\) given by \\[f_{Y^{L}}\\left( y \\right) = \\left\\{ \\begin{matrix} F_{X}\\left( d \\right) &amp; y = 0, \\\\ f_{X}\\left( y + d \\right) &amp; y &gt; 0 \\end{matrix} \\right. \\] We can see that the payment per payment variable is the payment per loss variable conditioned on the loss exceeding the deductible; that is, \\(Y^{P} = \\left. \\ Y^{L} \\right|X &gt; d\\). Hence, the probability density function of \\(Y^{P}\\) is given by \\[f_{Y^{P}}\\left( y \\right) = \\frac{f_{X}\\left( y + d \\right)}{1 - F_{X}\\left( d \\right)},\\] for \\(y &gt; 0\\). Accordingly, the distribution functions of \\(Y^{L}\\)and \\(Y^{P}\\) are given by \\[F_{Y^{L}}\\left( y \\right) = \\left\\{ \\begin{matrix} F_{X}\\left( d \\right) &amp; y = 0, \\\\ F_{X}\\left( y + d \\right) &amp; y &gt; 0. \\\\ \\end{matrix} \\right.\\ \\] and \\[F_{Y^{P}}\\left( y \\right) = \\frac{F_{X}\\left( y + d \\right) - F_{X}\\left( d \\right)}{1 - F_{X}\\left( d \\right)},\\] for \\(y &gt; 0\\), respectively. The raw moments of \\(Y^{L}\\) and \\(Y^{P}\\) can be found directly using the probability density function of \\(X\\) as follows \\[E\\left\\lbrack \\left( Y^{L} \\right)^{k} \\right\\rbrack = \\int_{d}^{\\infty}\\left( x - d \\right)^{k}f_{X}\\left( x \\right)dx ,\\] and \\[E\\left\\lbrack \\left( Y^{P} \\right)^{k} \\right\\rbrack = \\frac{\\int_{d}^{\\infty}\\left( x - d \\right)^{k}f_{X}\\left( x \\right) dx }{{1 - F}_{X}\\left( d \\right)} = \\frac{E\\left\\lbrack \\left( Y^{L} \\right)^{k} \\right\\rbrack}{{1 - F}_{X}\\left( d \\right)},\\] respectively. We have seen that the deductible \\(d\\) imposed on an insurance policy is the amount of loss that has to be paid out of pocket before the insurer makes any payment. The deductible \\(d\\) imposed on an insurance policy reduces the insurer’s payment. The loss elimination ratio (LER) is the percentage decrease in the expected payment of the insurer as a result of imposing the deductible. LER is defined as \\[LER = \\frac{E\\left( X \\right) - E\\left( Y^{L} \\right)}{E\\left( X \\right)}.\\] A little less common type of policy deductible is the franchise deductible. The Franchise deductible will apply to the policy in the same way as ordinary deductible except that when the loss exceeds the deductible \\(d\\), the full loss is covered by the insurer. The payment per loss and payment per payment variables are defined as \\[Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d, \\\\ X &amp; X &gt; d, \\\\ \\end{matrix} \\right.\\ \\] and \\[Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\leq d, \\\\ X &amp; X &gt; d, \\\\ \\end{matrix} \\right.\\ \\] respectively. Example 3.15 (SOA) A claim severity distribution is exponential with mean 1000. An insurance company will pay the amount of each claim in excess of a deductible of 100. Calculate the variance of the amount paid by the insurance company for one claim, including the possibility that the amount paid is 0. Solution Let \\(Y^{L}\\) denote the amount paid by the insurance company for one claim. \\[Y^{L} = \\left( X - 100 \\right)_{+} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq 100, \\\\ X - 100 &amp; X &gt; 100. \\\\ \\end{matrix} \\right.\\ \\] The first and second moments of \\(Y^{L}\\) are \\[E\\left( Y^{L} \\right) = \\int_{100}^{\\infty}\\left( x - 100 \\right)f_{X}\\left( x \\right)dx \\\\ = {\\int_{100}^{\\infty}{S_{X}\\left( x \\right)}dx = 1000e}^{- \\frac{100}{1000}},\\] and \\[E\\left\\lbrack \\left( Y^{L} \\right)^{2} \\right\\rbrack = \\int_{100}^{\\infty}\\left( x - 100 \\right)^{2}f_{X}\\left( x \\right)dx \\\\ = 2 \\times 1000^{2}e^{- \\frac{100}{1000}}.\\] \\[Var\\left( Y^{L} \\right) = \\left( 2 \\times 1000^{2}e^{- \\frac{100}{1000}} \\right) - \\left( {1000e}^{- \\frac{100}{1000}} \\right)^{2} = 990,944.\\] The solution can be simplified if we make use of the relationship between \\(X\\) and \\(Y^{P}\\). If \\(X\\) is exponentially distributed with mean 1000, then \\(Y^{P}\\) is also exponentially distributed with the same mean. Hence, \\(E\\left( Y^{P} \\right)\\)=1000 and \\[E\\left\\lbrack \\left( Y^{P} \\right)^{2} \\right\\rbrack = 2 \\times 1000^{2}.\\] Using the relationship between \\(Y^{L}\\) and \\(Y^{P}\\) we find \\[E\\left( Y^{L} \\right) = \\ E\\left( Y^{P} \\right)S_{X}\\left( 100 \\right){= 1000e}^{- \\frac{100}{1000}}\\] \\[E\\left\\lbrack \\left( Y^{L} \\right)^{2} \\right\\rbrack = E\\left\\lbrack \\left( Y^{P} \\right)^{2} \\right\\rbrack S_{X}\\left( 100 \\right) = 2 \\times 1000^{2}e^{- \\frac{100}{1000}}.\\] Example 3.16 (SOA) For an insurance: Losses have a density function \\[f_{X}\\left( x \\right) = \\left\\{ \\begin{matrix} 0.02x &amp; 0 &lt; x &lt; 10, \\\\ 0 &amp; \\text{elsewhere.} \\\\ \\end{matrix} \\right. \\] The insurance has an ordinary deductible of 4 per loss. \\(Y^{P}\\) is the claim payment per payment random variable. Calculate \\(E\\left( Y^{P} \\right)\\). Solution \\[Y^{P} = \\left\\{ \\begin{matrix} \\text{Undefined} &amp; X \\leq 4, \\\\ X - 4 &amp; X &gt; 4. \\\\ \\end{matrix} \\right.\\ \\] \\(E\\left( Y^{P} \\right) = \\frac{\\int_{4}^{10}\\left( x - 4 \\right)0.02xdx}{{1 - F}_{X}\\left( 4 \\right)} = \\frac{2.88}{0.84} = 3.43\\). Example 3.17 (SOA) You are given: Losses follow an exponential distribution with the same mean in all years. The loss elimination ratio this year is 70%. The ordinary deductible for the coming year is 4/3 of the current deductible. Compute the loss elimination ratio for the coming year. Solution The LER for the current year is \\[\\frac{E\\left( X \\right) - E\\left( Y^{L} \\right)}{E\\left( X \\right)} = \\frac{\\theta - \\theta e^{- d / \\theta}}{\\theta} = 1 - e^{- d / \\theta} = 0.7.\\] Then, \\(e^{- d / \\theta} = 0.3\\). The LER for the coming year is \\[ \\frac{\\theta - \\theta e^{- \\frac{\\left( \\frac{4}{3}d \\right)}{\\theta}}}{\\theta} = 1 - e^{- \\frac{\\left( \\frac{4}{3} d \\right)}{\\theta}} = 1 - \\left( e^{-d /\\theta} \\right)^{4/3} = 1 - {0.3}^{4/3} = 0.8 .\\] 3.4.2 Policy Limits Under a limited policy, the insurer is responsible for covering the actual loss \\(X\\) up to the limit of its coverage. This fixed limit of coverage is called the policy limit and often denoted by \\(u\\). If the loss exceeds the policy limit, the difference \\(X - u\\) has to be paid by the policyholder. While a higher policy limit means a higher payout to the insured, it is associated with a higher premium. Let \\(X\\) denote the loss incurred to the insured and \\(Y\\) denote the amount of paid claim by the insurer. Then \\(Y\\) is defined as \\[Y = X \\land u = \\left\\{ \\begin{matrix} X &amp; X \\leq u, \\\\ u &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] It can be seen that the distinction between \\(Y^{L}\\) and \\(Y^{P}\\) is not needed under limited policy as the insurer will always make a payment. Even when the distribution of \\(X\\) is continuous, the distribution of \\(Y\\) is partly discrete and partly continuous. The discrete part of the distribution is concentrated at \\(Y = u\\) (when \\(X &gt; u\\)), while the continuous part is spread over the interval \\(Y &lt; u\\) (when \\(X \\leq u\\)). For the discrete part, the probability that the benefit paid is \\(u\\), is the probability that the loss exceeds the policy limit \\(u\\); that is, \\[\\Pr \\left( Y = u \\right) = \\Pr \\left( X &gt; u \\right) = {1 - F}_{X}\\left( u \\right).\\] For the continuous part of the distribution \\(Y = X\\), hence the probability density function of \\(Y\\) is given by \\[f_{Y}\\left( y \\right) = \\left\\{ \\begin{matrix} f_{X}\\left( y \\right) &amp; 0 &lt; y &lt; u, \\\\ 1 - F_{X}\\left( u \\right) &amp; y = u. \\\\ \\end{matrix} \\right.\\ \\] Accordingly, the distribution function of \\(Y\\) is given by \\[F_{Y}\\left( y \\right) = \\left\\{ \\begin{matrix} F_{X}\\left( x \\right) &amp; 0 &lt; y &lt; u, \\\\ 1 &amp; y \\geq u. \\\\ \\end{matrix} \\right.\\ \\] The raw moments of \\(Y\\) can be found directly using the probability density function of \\(X\\) as follows \\[E\\left( Y^{k} \\right) = E\\left\\lbrack \\left( X \\land u \\right)^{k} \\right\\rbrack = \\int_{0}^{u}x^{k}f_{X}\\left( x \\right)dx + \\int_{u}^{\\infty}{u^{k}f_{X}\\left( x \\right)} dx \\\\ \\int_{0}^{u}x^{k}f_{X}\\left( x \\right)dx + u^{k}\\left\\lbrack {1 - F}_{X}\\left( u \\right) \\right\\rbrack dx.\\] Example 3.18 (SOA) Under a group insurance policy, an insurer agrees to pay 100% of the medical bills incurred during the year by employees of a small company, up to a maximum total of one million dollars. The total amount of bills incurred, \\(X\\), has probability density function \\[f_{X}\\left( x \\right) = \\left\\{ \\begin{matrix} \\frac{x\\left( 4 - x \\right)}{9} &amp; 0 &lt; x &lt; 3, \\\\ 0 &amp; \\text{elsewhere.} \\\\ \\end{matrix} \\right.\\ \\] where \\(x\\) is measured in millions. Calculate the total amount, in millions of dollars, the insurer would expect to pay under this policy. Solution \\[Y = X \\land 1 = \\left\\{ \\begin{matrix} X &amp; X \\leq 1, \\\\ 1 &amp; X &gt; 1. \\\\ \\end{matrix} \\right.\\ \\] \\(E\\left( Y \\right) = E\\left( X \\land 1 \\right) = \\int_{0}^{1}\\frac{x^{2}(4 - x)}{9}dx + \\int_{1}^{3}\\frac{x\\left( 4 - x \\right)}{9}dx = 0.935\\). 3.4.3 Coinsurance As we have seen in Section 3.4.1, the amount of loss retained by the policyholder can be losses up to the deductible \\(d\\). The retained loss can also be a percentage of the claim. The percentage \\(\\alpha\\), often referred to as the coinsurance factor, is the percentage of claim the insurance company is required to cover. If the policy is subject to an ordinary deductible and policy limit, coinsurance refers to the percentage of claim the insurer is required to cover, after imposing the ordinary deductible and policy limit. The payment per loss variable, \\(Y^{L}\\), is defined as \\[Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq d, \\\\ \\alpha\\left( X - d \\right) &amp; d &lt; X \\leq u, \\\\ \\alpha\\left( u - d \\right) &amp; X &gt; u. \\\\ \\end{matrix} \\right.\\ \\] The policy limit (the maximum amount paid by the insurer) in this case is \\(\\alpha\\left( u - d \\right)\\), while \\(u\\) is the maximum covered loss. The \\(k\\)-th moment of \\(Y^{L}\\) is given by \\[E\\left\\lbrack \\left( Y^{L} \\right)^{k} \\right\\rbrack = \\int_{d}^{u}\\left\\lbrack \\alpha\\left( x - d \\right) \\right\\rbrack^{k}f_{X}\\left( x \\right)dx + \\int_{u}^{\\infty}\\left\\lbrack \\alpha\\left( u - d \\right) \\right\\rbrack^{k}f_{X}\\left( x \\right) dx .\\] A growth factor \\(\\left( 1 + r \\right)\\) may be applied to \\(X\\) resulting in an inflated loss random variable \\(\\left( 1 + r \\right)X\\) (the prespecified d and u remain unchanged). The resulting per loss variable can be written as \\[Y^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq \\frac{d}{1 + r}, \\\\ \\alpha\\left\\lbrack \\left( 1 + r \\right)X - d \\right\\rbrack &amp; \\frac{d}{1 + r} &lt; X \\leq \\frac{u}{1 + r}, \\\\ \\alpha\\left( u - d \\right) &amp; X &gt; \\frac{u}{1 + r}. \\\\ \\end{matrix} \\right.\\ \\] The first and second moments of \\(Y^{L}\\) can be expressed as \\[E\\left( Y^{L} \\right) = \\alpha\\left( 1 + r \\right)\\left\\lbrack E\\left( X \\land \\frac{u}{1 + r} \\right) - E\\left( X \\land \\frac{d}{1 + r} \\right) \\right\\rbrack,\\] and \\[E\\left\\lbrack \\left( Y^{L} \\right)^{2} \\right\\rbrack = \\alpha^{2}\\left( 1 + r \\right)^{2} \\left\\{ E\\left\\lbrack \\left( X \\land \\frac{u}{1 + r} \\right)^{2} \\right\\rbrack - E\\left\\lbrack \\left( X \\land \\frac{d}{1 + r} \\right)^{2} \\right\\rbrack \\right. \\\\ \\left. \\ \\ \\ \\ \\ - 2\\left( \\frac{d}{1 + r} \\right)\\left\\lbrack E\\left( X \\land \\frac{u}{1 + r} \\right) - E\\left( X \\land \\frac{d}{1 + r} \\right) \\right\\rbrack \\right\\} ,\\] respectively. The formulae given for the first and second moments of \\(Y^{L}\\) are general. Under full coverage, \\(\\alpha = 1\\), \\(r = 0\\), \\(u = \\infty\\), \\(d = 0\\) and \\(E\\left( Y^{L} \\right)\\) reduces to \\(E\\left( X \\right)\\). If only an ordinary deductible is imposed, \\(\\alpha = 1\\), \\(r = 0\\), \\(u = \\infty\\) and \\(E\\left( Y^{L} \\right)\\) reduces to \\(E\\left( X \\right) - E\\left( X \\land d \\right)\\). If only a policy limit is imposed \\(\\alpha = 1\\), \\(r = 0\\), \\(d = 0\\) and \\(E\\left( Y^{L} \\right)\\) reduces to \\(E\\left( X \\land u \\right)\\). Example 3.19 (SOA) The ground up loss random variable for a health insurance policy in 2006 is modeled with X, an exponential distribution with mean 1000. An insurance policy pays the loss above an ordinary deductible of 100, with a maximum annual payment of 500. The ground up loss random variable is expected to be 5% larger in 2007, but the insurance in 2007 has the same deductible and maximum payment as in 2006. Find the percentage increase in the expected cost per payment from 2006 to 2007. Solution \\[Y_{2006}^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq 100, \\\\ X - 100 &amp; 100 &lt; X \\leq 600, \\\\ 500 &amp; X &gt; 600. \\\\ \\end{matrix} \\right.\\ \\] \\[Y_{2007}^{L} = \\left\\{ \\begin{matrix} 0 &amp; X \\leq 95.24, \\\\ 1.05X - 100 &amp; 95.24 &lt; X \\leq 571.43, \\\\ 500 &amp; X &gt; 571.43. \\\\ \\end{matrix} \\right.\\ \\] \\[E\\left( Y_{2006}^{L} \\right) = E\\left( X \\land 600 \\right) - E\\left( X \\land 100 \\right) = 1000\\left( {1 - e}^{- \\frac{600}{1000}} \\right) - 1000\\left( {1 - e}^{- \\frac{100}{1000}} \\right)\\] \\(= 356.026\\). \\[E\\left( Y_{2007}^{L} \\right) = 1.05\\left\\lbrack E\\left( X \\land 571.43 \\right) - E\\left( X \\land 95.24 \\right) \\right\\rbrack\\] \\(= 1.05\\left\\lbrack 1000\\left( {1 - e}^{- \\frac{571.43}{1000}} \\right) - 1000\\left( {1 - e}^{- \\frac{95.24}{1000}} \\right) \\right\\rbrack\\) \\(\\mathbf{=}361.659\\). \\(E\\left( Y_{2006}^{P} \\right) = \\frac{356.026}{e^{- \\frac{100}{1000}} = 393.469}\\). \\(E\\left( Y_{2007}^{P} \\right) = \\frac{361.659}{e^{- \\frac{95.24}{1000}} = 397.797}\\). There is an increase of 1.1% from 2006 to 2007. 3.4.4 Reinsurance In Section 3.4.1 we introduced the policy deductible, which is a contractual arrangement under which an insured transfers part of the risk by securing coverage from an insurer in return for an insurance premium. Under that policy, when the loss exceeds the deductible, the insurer is not required to pay until the insured has paid the fixed deductible. We now introduce reinsurance, a mechanism of insurance for insurance companies. Reinsurance is a contractual arrangement under which an insurer transfers part of the underlying insured risk by securing coverage from another insurer (referred to as a reinsurer) in return for a reinsurance premium. Although reinsurance involves a relationship between three parties: the original insured, the insurer (often referred to as cedent or cedant) and the reinsurer, the parties of the reinsurance agreement are only the primary insurer and the reinsurer. There is no contractual agreement between the original insured and the reinsurer. The reinsurer is not required to pay under the reinsurance contract until the insurer has paid a loss to its original insured. The amount retained by the primary insurer in the reinsurance agreement (the reinsurance deductible) is called retention. Reinsurance arrangements allow insurers with limited financial resources to increase the capacity to write insurance and meet client requests for larger insurance coverage while reducing the impact of potential losses and protecting the insurance company against catastrophic losses. Reinsurance also allows the primary insurer to benefit from underwriting skills, expertize and proficient complex claim file handling of the larger reinsurance companies. Example 3.20 (SOA) In 2005 a risk has a two-parameter Pareto distribution with \\(\\alpha = 2\\) and \\(\\theta = 3000\\). In 2006 losses inflate by 20%. Insurance on the risk has a deductible of 600 in each year. \\(P_{i}\\), the premium in year \\(i\\), equals 1.2 times expected claims. The risk is reinsured with a deductible that stays the same in each year. \\(R_{i}\\), the reinsurance premium in year \\(i\\), equals 1.1 times the expected reinsured claims. \\(\\frac{R_{2005}}{P_{2005} = 0.55}\\). Calculate \\(\\frac{R_{2006}}{P_{2006}}\\). Solution Let us use the following notation: \\(X_{i}:\\) The risk in year \\(i\\) \\(Y_{i}:\\) The insured claim in year \\(i\\) \\(P_{i}:\\) The insurance premium in year \\(i\\) \\(Y_{i}^{R}:\\) The reinsured claim in year \\(i\\) \\(R_{i}:\\) The reinsurance premium in year \\(i\\) \\(d:\\) The insurance deductible in year \\(i\\) (the insurance deductible is fixed each year, equal to 600) \\(d^{R}:\\) The reinsurance deductible or retention in year \\(i\\) (the reinsurance deductible is fixed each year, but unknown) where \\(i = 2005,\\ 2006\\) \\[Y_{i} = \\left\\{ \\begin{matrix} 0 &amp; X_{i} \\leq 600 \\\\ X_{i} - 600 &amp; X_{i} &gt; 600 \\\\ \\end{matrix} \\right.\\ \\] where \\(i = 2005,\\ 2006\\) \\[X_{2005}\\sim Pa\\left( 2,3000 \\right)\\] \\[E\\left( Y_{2005} \\right) = E\\left( X_{2005} - 600 \\right)_{+} = E\\left( X_{2005} \\right) - E\\left( X_{2005} \\land 600 \\right)\\] \\(= 3000 - 3000\\left( 1 - \\frac{3000}{3600} \\right) = 2500\\) \\[P_{2005} = 1.2E\\left( Y_{2005} \\right) = 3000\\] Since \\(X_{2006} = 1.2X_{2005}\\) and Pareto is a scale distribution with scale parameter \\(\\theta\\), then \\(X_{2006}\\sim Pa\\left( 2,3600 \\right)\\) \\[E\\left( Y_{2006} \\right) = E\\left( X_{2006} - 600 \\right)_{+} = E\\left( X_{2006} \\right) - E\\left( X_{2006} \\land 600 \\right)\\] \\(= 3600 - 3600\\left( 1 - \\frac{3600}{4200} \\right) = 3085.714\\) \\[P_{2006} = 1.2E\\left( Y_{2006} \\right) = 3702.857\\] \\[Y_{i}^{R} = \\left\\{ \\begin{matrix} 0 &amp; X_{i} - 600 \\leq d^{R} \\\\ X_{i} - 600 - d^{R} &amp; X_{i} - 600 &gt; d^{R} \\\\ \\end{matrix} \\right.\\ \\] Since \\(\\frac{R_{2005}}{P_{2005}} = 0.55\\), then \\(R_{2005} = 3000 \\times 0.55 = 1650\\) Since \\(R_{2005} = 1.1E\\left( Y_{2005}^{R} \\right)\\), then \\(E\\left( Y_{2005}^{R} \\right) = \\frac{1650}{1.1} = 1500\\) \\[E\\left( Y_{2005}^{R} \\right) = E\\left( X_{2005} - 600 - d^{R} \\right)_{+} = E\\left( X_{2005} \\right) - E\\left( X_{2005} \\land \\left( 600 + d^{R} \\right) \\right)\\] \\(= 3000 - 3000\\left( 1 - \\frac{3000}{3600 + d^{R}} \\right) = 1500 \\Rightarrow d^{R} = 2400\\) \\[E\\left( Y_{2006}^{R} \\right) = E\\left( X_{2006} - 600 - d^{R} \\right)_{+} = E\\left( X_{2006} - 3000 \\right)_{+} = E\\left( X_{2006} \\right) - E\\left( X_{2006} \\land 3000 \\right)\\] \\(= 3600 - 3600\\left( 1 - \\frac{3600}{6600} \\right) = 1963.636\\) \\[R_{2006} = 1.1E\\left( Y_{2006}^{R} \\right) = 1.1 \\times 1963.636 = 2160\\] Therefore \\(\\frac{R_{2006}}{P_{2006}} = \\frac{2160}{3702.857} = 0.583\\) 3.5 Maximum Likelihood Estimation In this section we estimate statistical parameters using the method of maximum likelihood. Maximum likelihood estimates in the presence of grouping, truncation or censoring are calculated. 3.5.1 Maximum Likelihood Estimators for Complete Data Pricing of insurance premiums and estimation of claim reserving are among many actuarial problems that involve modeling the severity of loss (claim size). The principles for using maximum likelihood to estimate model parameters were introduced in Chapter xxx. In this section, we present a few examples to illustrate how actuaries fit a parametric distribution model to a set of claim data using maximum likelihood. In these examples we derive the asymptotic variance of maximum-likelihood estimators of the model parameters. We use the delta method to derive the asymptotic variances of functions of these parameters. Example 3.21 Consider a random sample of claim amounts: 8,000 10,000 12,000 15,000. You assume that claim amounts follow an inverse exponential distribution, with parameter \\(\\theta\\). Calculate the maximum likelihood estimator for \\(\\theta\\). Approximate the variance of the maximum likelihood estimator. Determine an approximate 95% confidence interval for \\(\\theta\\). Determine an approximate 95% confidence interval for \\(\\Pr \\left( X \\leq 9,000 \\right).\\) Solution The probability density function is \\[f_{X}\\left( x \\right) = \\frac{\\theta e^{- \\frac{\\theta}{x}}}{x^{2}}, \\] where \\(x &gt; 0\\). The likelihood function, \\(L\\left( \\theta \\right)\\), can be viewed as the probability of the observed data, written as a function of the model’s parameter \\(\\theta\\) \\[L\\left( \\theta \\right) = \\prod_{i = 1}^{4}{f_{X_{i}}\\left( x_{i} \\right)} = \\frac{\\theta^{4}e^{- \\theta\\sum_{i = 1}^{4}\\frac{1}{x_{i}}}}{\\prod_{i = 1}^{4}x_{i}^{2}}.\\] The loglikelihood function, \\(\\ln L \\left( \\theta \\right)\\), is the sum of the individual logarithms. \\[\\ln L \\left( \\theta \\right) = 4ln\\theta - \\theta\\sum_{i = 1}^{4}\\frac{1}{x_{i}} - 2\\sum_{i = 1}^{4}\\ln x_{i} .\\] \\[\\frac{d \\ln L \\left( \\theta \\right)}{d \\theta} = \\frac{4}{\\theta} - \\sum_{i = 1}^{4}\\frac{1}{x_{i}}.\\] The maximum likelihood estimator of \\(\\theta\\), denoted by \\(\\hat{\\theta}\\), is the solution to the equation \\[\\frac{4}{\\hat{\\theta}} - \\sum_{i = 1}^{4}{\\frac{1}{x_{i}} = 0}.\\] Thus, \\(\\hat{\\theta} = \\frac{4}{\\sum_{i = 1}^{4}\\frac{1}{x_{i}}} = 10,667\\) The second derivative of \\(\\ln L \\left( \\theta \\right)\\) is given by \\[\\frac{d^{2}\\ln L\\left( \\theta \\right)}{d\\theta^{2}} = \\frac{- 4}{\\theta^{2}}.\\] Evaluating the second derivative of the loglikelihood function at \\(\\hat{\\theta} = 10,667\\) gives a negative value, indicating \\(\\hat{\\theta}\\) as the value that maximizes the loglikelihood function. Taking reciprocal of negative expectation of the second derivative of \\(\\ln L \\left( \\theta \\right)\\), we obtain an estimate of the variance of \\(\\hat{\\theta}\\) \\(\\widehat{Var}\\left( \\hat{\\theta} \\right) = \\left. \\ \\left\\lbrack E\\left( \\frac{d^{2}\\ln L \\left( \\theta \\right)}{d\\theta^{2}} \\right) \\right\\rbrack^{- 1} \\right|_{\\theta = \\hat{\\theta}} = \\frac{{\\hat{\\theta}}^{2}}{4} = 28,446,222\\). It should be noted that as the sample size \\(n \\rightarrow \\infty\\), the distribution of the maximum likelihood estimator \\(\\hat{\\theta}\\) converges to a normal distribution with mean \\(\\theta\\) and variance \\(\\hat{V}\\left( \\hat{\\theta} \\right)\\). The approximate confidence interval in this example is based on the assumption of normality, despite the small sample size, only for the purpose of illustration. The 95% confidence interval for \\(\\theta\\) is given by \\[10,667 \\pm 1.96\\sqrt{28,446,222} = \\left( 213.34,\\ 21,120.66 \\right).\\] The distribution function of \\(X\\) is \\(F\\left( x \\right) = 1 - e^{- \\frac{x}{\\theta}}\\). Then, the maximum likelihood estimate of \\(g\\left( \\theta \\right) = F\\left( 9,000 \\right)\\) is \\[g\\left( \\hat{\\theta} \\right) = 1 - e^{- \\frac{9,000}{10,667}} = 0.57.\\] We use the delta method to approximate the variance of \\(g\\left( \\hat{\\theta} \\right)\\). \\[\\frac{\\text{dg}\\left( \\theta \\right)}{d \\theta} = {- \\frac{9,000}{\\theta^{2}}e}^{- \\frac{9,000}{\\theta}}.\\] \\(\\widehat{Var}\\left\\lbrack g\\left( \\hat{\\theta} \\right) \\right\\rbrack = \\left( - {\\frac{9,000}{{\\hat{\\theta}}^{2}}e}^{- \\frac{9,000}{\\hat{\\theta}}} \\right)^{2}\\hat{V}\\left( \\hat{\\theta} \\right) = 0.0329\\). The 95% confidence interval for \\(F\\left( 9,000 \\right)\\) is given by \\[0.57 \\pm 1.96\\sqrt{0.0329} = \\left( 0.214,\\ 0.926 \\right).\\] Example 3.22 A random sample of size 6 is from a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). The sample values are 200, 3,000, 8,000, 60,000, 60,000, 160,000. Calculate the maximum likelihood estimator for \\(\\mu\\) and \\(\\sigma\\). Estimate the covariance matrix of the maximum likelihood estimator. Determine approximate 95% confidence intervals for \\(\\mu\\) and \\(\\sigma\\). Determine an approximate 95% confidence interval for the mean of the lognormal distribution. Solution The probability density function is \\[f_{X}\\left( x \\right) = \\frac{1}{x \\sigma \\sqrt{2\\pi}}\\exp - \\frac{1}{2}\\left( \\frac{\\ln x - \\mu}{\\sigma} \\right)^{2},\\] where \\(x &gt; 0\\). The likelihood function, \\(L\\left( \\mu,\\sigma \\right)\\), is the product of the pdf for each data point. \\[L\\left( \\mu,\\sigma \\right) = \\prod_{i = 1}^{6}{f_{X_{i}}\\left( x_{i} \\right)} = \\frac{1}{\\sigma^{6}\\left( 2\\pi \\right)^{3}\\prod_{i = 1}^{6}x_{i}}exp - \\frac{1}{2}\\sum_{i = 1}^{6}\\left( \\frac{\\ln x_{i} - \\mu}{\\sigma} \\right)^{2}.\\] The loglikelihood function, \\(\\ln L \\left( \\mu,\\sigma \\right)\\), is the sum of the individual logarithms. \\[\\ln \\left( \\mu,\\sigma \\right) = - 6ln\\sigma - 3ln\\left( 2\\pi \\right) - \\sum_{i = 1}^{6}\\ln x_{i} - \\frac{1}{2}\\sum_{i = 1}^{6}\\left( \\frac{\\ln x_{i} - \\mu}{\\sigma} \\right)^{2}.\\] The first partial derivatives are \\[\\frac{\\partial lnL\\left( \\mu,\\sigma \\right)}{\\partial\\mu} = \\frac{1}{\\sigma^{2}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right).\\] \\[\\frac{\\partial lnL\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} = \\frac{- 6}{\\sigma} + \\frac{1}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right)^{2}.\\] The maximum likelihood estimators of \\(\\mu\\) and \\(\\sigma\\), denoted by \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}\\), are the solutions to the equations \\[\\frac{1}{{\\hat{\\sigma}}^{2}}\\sum_{i = 1}^{6}\\left( lnx_{i} - \\hat{\\mu} \\right) = 0.\\] \\[\\frac{- 6}{\\hat{\\sigma}} + \\frac{1}{{\\hat{\\sigma}}^{3}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\hat{\\mu} \\right)^{2} = 0.\\] These yield the estimates \\(\\hat{\\mu} = \\frac{\\sum_{i = 1}^{6}{\\ln x_{i}}}{6} = 9.38\\) and \\({\\hat{\\sigma}}^{2} = \\frac{\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\hat{\\mu} \\right)^{2}}{6} = 5.12\\). The second partial derivatives are \\(\\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu^{2}} = \\frac{- 6}{\\sigma^{2}}\\), \\(\\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu\\partial\\sigma} = \\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right)\\) and \\(\\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\sigma^{2}} = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}\\left( \\ln x_{i} - \\mu \\right)^{2}\\). To derive the covariance matrix of the mle we need to find the expectations of the second derivatives. Since the random variable \\(X\\) is from a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma\\), then \\(\\text{lnX}\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). \\(E\\left( \\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu^{2}} \\right) = E\\left( \\frac{- 6}{\\sigma^{2}} \\right) = \\frac{- 6}{\\sigma^{2}}\\), \\(E\\left( \\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\mu\\partial\\sigma} \\right) = \\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}{E\\left( \\ln x_{i} - \\mu \\right)} = \\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left\\lbrack E\\left( \\ln x_{i} \\right) - \\mu \\right\\rbrack\\)=\\(\\frac{- 2}{\\sigma^{3}}\\sum_{i = 1}^{6}\\left( \\mu - \\mu \\right) = 0\\), and \\(E\\left( \\frac{\\partial^{2}\\text{lnL}\\left( \\mu,\\sigma \\right)}{\\partial\\sigma^{2}} \\right) = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}{E\\left( \\ln x_{i} - \\mu \\right)}^{2} = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}{V\\left( \\ln x_{i} \\right) = \\frac{6}{\\sigma^{2}} - \\frac{3}{\\sigma^{4}}\\sum_{i = 1}^{6}{\\sigma^{2} = \\frac{- 12}{\\sigma^{2}}}}\\). Using the negatives of these expectations we obtain the Fisher information matrix \\[\\begin{bmatrix} \\frac{6}{\\sigma^{2}} &amp; 0 \\\\ 0 &amp; \\frac{12}{\\sigma^{2}} \\\\ \\end{bmatrix}\\]. The covariance matrix, \\(\\Sigma\\), is the inverse of the Fisher information matrix \\[\\Sigma = \\begin{bmatrix} \\frac{\\sigma^{2}}{6} &amp; 0 \\\\ 0 &amp; \\frac{\\sigma^{2}}{12} \\\\ \\end{bmatrix}\\]. The estimated matrix is given by \\[\\hat{\\Sigma} = \\begin{bmatrix} 0.8533 &amp; 0 \\\\ 0 &amp; 0.4267 \\\\ \\end{bmatrix}\\]. The 95% confidence interval for \\(\\mu\\) is given by \\(9.38 \\pm 1.96\\sqrt{0.8533} = \\left( 7.57,\\ 11.19 \\right)\\). The 95% confidence interval for \\(\\sigma^{2}\\) is given by \\(5.12 \\pm 1.96\\sqrt{0.4267} = \\left( 3.84,\\ 6.40 \\right)\\). The mean of X is \\(\\exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\). Then, the maximum likelihood estimate of \\[g\\left( \\mu,\\sigma \\right) = \\exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\] is \\[g\\left( \\hat{\\mu},\\hat{\\sigma} \\right) = \\exp\\left( \\hat{\\mu} + \\frac{{\\hat{\\sigma}}^{2}}{2} \\right) = 153,277.\\] We use the delta method to approximate the variance of the mle \\(g\\left( \\hat{\\mu},\\hat{\\sigma} \\right)\\). \\(\\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\mu} = exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\) and \\(\\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} = \\sigma exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\). Using the delta method, the approximate variance of \\(g\\left( \\hat{\\mu},\\hat{\\sigma} \\right)\\) is given by \\[\\left. \\ \\hat{V}\\left( g\\left( \\hat{\\mu},\\hat{\\sigma} \\right) \\right) = \\begin{bmatrix} \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\mu} &amp; \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} \\\\ \\end{bmatrix}\\Sigma\\begin{bmatrix} \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\mu} \\\\ \\frac{\\partial g\\left( \\mu,\\sigma \\right)}{\\partial\\sigma} \\\\ \\end{bmatrix} \\right|_{\\mu = \\hat{\\mu},\\sigma = \\hat{\\sigma}}\\] \\[= \\begin{bmatrix} 153,277 &amp; 346,826 \\\\ \\end{bmatrix}\\begin{bmatrix} 0.8533 &amp; 0 \\\\ 0 &amp; 0.4267 \\\\ \\end{bmatrix}\\begin{bmatrix} 153,277 \\\\ 346,826 \\\\ \\end{bmatrix} =\\]71,374,380,000 The 95% confidence interval for \\(\\exp\\left( \\mu + \\frac{\\sigma^{2}}{2} \\right)\\) is given by \\(153,277 \\pm 1.96\\sqrt{71,374,380,000} = \\left( - 370,356,\\ 676,910 \\right)\\). Since the mean of the lognormal distribution cannot be negative, we should replace the negative lower limit in the previous interval by a zero. 3.5.2 Maximum Likelihood Estimators for Grouped Data In the previous section we considered the maximum likelihood estimation of continuous models from complete (individual) data. Each individual observation is recorded, and its contribution to the likelihood function is the density at that value. In this section we consider the problem of obtaining maximum likelihood estimates of parameters from grouped data. The observations are only available in grouped form, and the contribution of each observation to the likelihood function is the probability of falling in a specific group (interval). Let \\(n_{j}\\) represent the number of observations in the interval \\(\\left( \\left. \\ c_{j - 1},c_{j} \\right\\rbrack \\right.\\ \\) The grouped data likelihood function is thus given by \\[L\\left( \\theta \\right) = \\prod_{j = 1}^{k}\\left\\lbrack F\\left( \\left. \\ c_{j} \\right|\\theta \\right) - F\\left( \\left. \\ c_{j - 1} \\right|\\theta \\right) \\right\\rbrack^{n_{j}},\\] where \\(c_{0}\\) is the smallest possible observation (often set to zero) and \\(c_{k}\\) is the largest possible observation (often set to infinity). Example 3.23 (SOA) For a group of policies, you are given that losses follow the distribution function \\(F\\left( x \\right) = 1 - \\frac{\\theta}{x}\\), for \\(\\theta &lt; x &lt; \\infty.\\) Further, a sample of 20 losses resulted in the following: \\[ {\\small \\begin{matrix}\\hline \\text{Interval} &amp; \\text{Number of Losses} \\\\ \\hline (\\theta, 10] &amp; 9 \\\\ (10, 25] &amp; 6 \\\\ (25, \\infty) &amp; 5 \\\\ \\hline \\end{matrix} } \\] Calculate the maximum likelihood estimate of \\(\\theta\\). Solution The contribution of each of the 9 observations in the first interval to the likelihood function is the probability of \\(X \\leq 10\\); that is, \\(\\Pr\\left( X \\leq 10 \\right) = F\\left( 10 \\right)\\). Similarly, the contributions of each of 6 and 5 observations in the second and third intervals are \\(\\Pr\\left( 10 &lt; X \\leq 25 \\right) = F\\left( 25 \\right) - F(10)\\) and \\(P\\left( X &gt; 25 \\right) = 1 - F(25)\\), respectively. The likelihood function is thus given by \\[L\\left( \\theta \\right) = \\left\\lbrack F\\left( 10 \\right) \\right\\rbrack^{9}\\left\\lbrack F\\left( 25 \\right) - F(10) \\right\\rbrack^{6}\\left\\lbrack 1 - F(25) \\right\\rbrack^{5}\\] \\[{= \\left( 1 - \\frac{\\theta}{10} \\right)}^{9}\\left( \\frac{\\theta}{10} - \\frac{\\theta}{25} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5}\\] \\[{= \\left( \\frac{10 - \\theta}{10} \\right)}^{9}\\left( \\frac{15\\theta}{250} \\right)^{6}\\left( \\frac{\\theta}{25} \\right)^{5}.\\] Then, \\(\\ln L \\left( \\theta \\right) = 9ln\\left( 10 - \\theta \\right) + 6ln\\theta + 5ln\\theta - 9ln10 + 6ln15 - 6ln250 - 5ln25\\). \\[\\frac{d \\ln L \\left( \\theta \\right)}{d \\theta} = \\frac{- 9}{\\left( 10 - \\theta \\right)} + \\frac{6}{\\theta} + \\frac{5}{\\theta}.\\] The maximum likelihood estimator, \\(\\hat{\\theta}\\), is the solution to the equation \\[\\frac{- 9}{\\left( 10 - \\hat{\\theta} \\right)} + \\frac{11}{\\hat{\\theta}} = 0\\] and \\(\\hat{\\theta} = 5.5\\). 3.5.3 Maximum Likelihood Estimators for Censored Data Another distinguishing feature of data gathering mechanism is censoring. While for some event of interest (losses, claims, lifetimes, etc.) the complete data maybe available, for others only partial information is available; information that the observation exceeds a specific value. The limited policy introduced in Section 3.4.2 is an example of right censoring. Any loss greater than or equal to the policy limit is recorded at the limit. The contribution of the censored observation to the likelihood function is the probability of the random variable exceeding this specific limit. Note that contributions of both complete and censored data share the survivor function, for a complete point this survivor function is multiplied by the hazard function, but for a censored observation it is not. Example 3.24 (SOA) The random variable has survival function: \\[S_{X}\\left( x \\right) = \\frac{\\theta^{4}}{\\left( \\theta^{2} + x^{2} \\right)^{2}}.\\] Two values of \\(X\\) are observed to be 2 and 4. One other value exceeds 4. Calculate the maximum likelihood estimate of \\(\\theta\\). Solution The contributions of the two observations 2 and 4 are \\(f_{X}\\left( 2 \\right)\\) and \\(f_{X}\\left( 4 \\right)\\) respectively. The contribution of the third observation, which is only known to exceed 4 is \\(S_{X}\\left( 4 \\right)\\). The likelihood function is thus given by \\[L\\left( \\theta \\right) = f_{X}\\left( 2 \\right)f_{X}\\left( 4 \\right)S_{X}\\left( 4 \\right).\\] The probability density function of \\(X\\) is given by \\[f_{X}\\left( x \\right) = \\frac{4x\\theta^{4}}{\\left( \\theta^{2} + x^{2} \\right)^{3}}.\\] Thus, \\[L\\left( \\theta \\right) = \\frac{8\\theta^{4}}{\\left( \\theta^{2} + 4 \\right)^{3}}\\frac{16\\theta^{4}}{\\left( \\theta^{2} + 16 \\right)^{3}}\\frac{\\theta^{4}}{\\left( \\theta^{2} + 16 \\right)^{2}} = \\\\ \\frac{128\\theta^{12}}{\\left( \\theta^{2} + 4 \\right)^{3}\\left( \\theta^{2} + 16 \\right)^{5}},\\] \\(\\ln L\\left( \\theta \\right) = ln128 + 12ln\\theta - 3ln\\left( \\theta^{2} + 4 \\right) - 5ln\\left( \\theta^{2} + 16 \\right)\\), and \\(\\frac{\\text{dlnL}\\left( \\theta \\right)}{d \\theta} = \\frac{12}{\\theta} - \\frac{6\\theta}{\\left( \\theta^{2} + 4 \\right)} - \\frac{10\\theta}{\\left( \\theta^{2} + 16 \\right)}\\). The maximum likelihood estimator, \\(\\hat{\\theta}\\), is the solution to the equation \\[\\frac{12}{\\hat{\\theta}} - \\frac{6\\hat{\\theta}}{\\left( {\\hat{\\theta}}^{2} + 4 \\right)} - \\frac{10\\hat{\\theta}}{\\left( {\\hat{\\theta}}^{2} + 16 \\right)} = 0\\] or \\[12\\left( {\\hat{\\theta}}^{2} + 4 \\right)\\left( {\\hat{\\theta}}^{2} + 16 \\right) - 6{\\hat{\\theta}}^{2}\\left( {\\hat{\\theta}}^{2} + 16 \\right) - 10{\\hat{\\theta}}^{2}\\left( {\\hat{\\theta}}^{2} + 4 \\right) = \\\\ - 4{\\hat{\\theta}}^{4} + 104{\\hat{\\theta}}^{2} + 768 = 0,\\] which yields \\({\\hat{\\theta}}^{2} = 32\\) and \\(\\hat{\\theta} = 5.7\\). 3.5.4 Maximum Likelihood Estimators for Truncated Data This section is concerned with the maximum likelihood estimation of the continuous distribution of the random variable \\(X\\) when the data is incomplete due to truncation. If the values of \\(X\\) are truncated at \\(d\\), then it should be noted that we would not have been aware of the existence of these values had they not exceeded \\(d\\). The policy deductible introduced in Section 3.4.1 is an example of left truncation. Any loss less than or equal to the deductible is not recorded. The contribution to the likelihood function of an observation \\(x\\) truncated at \\(d\\) will be a conditional probability and the \\(f_{X}\\left( x \\right)\\) will be replaced by \\(\\frac{f_{X}\\left( x \\right)}{S_{X}\\left( d \\right)}\\). Example 3.25 (SOA) For the single parameter Pareto distribution with \\(\\theta = 2\\), maximum likelihood estimation is applied to estimate the parameter \\(\\alpha\\). Find the estimated mean of the ground up loss distribution based on the maximum likelihood estimate of \\(\\alpha\\) for the following data set: Ordinary policy deductible of 5, maximum covered loss of 25 (policy limit 20) 8 insurance payment amounts: 2, 4, 5, 5, 8, 10, 12, 15 2 limit payments: 20, 20. Solution The contributions of the different observations can be summarized as follows: For the exact loss: \\(f_{X}\\left( x \\right)\\) For censored observations: \\(S_{X}\\left( 25 \\right)\\). For truncated observations: \\(\\frac{f_{X}\\left( x \\right)}{S_{X}\\left( 5 \\right)}\\). Given that ground up losses smaller than 5 are omitted from the data set, the contribution of all observations should be conditional on exceeding 5. The likelihood function becomes \\[L\\left( \\alpha \\right) = \\frac{\\prod_{i = 1}^{8}{f_{X}\\left( x_{i} \\right)}}{\\left\\lbrack S_{X}\\left( 5 \\right) \\right\\rbrack^{8}}\\left\\lbrack \\frac{S_{X}\\left( 25 \\right)}{S_{X}\\left( 5 \\right)} \\right\\rbrack^{2}.\\] For the single parameter Pareto the probability density and distribution functions are given by \\[f_{X}\\left( x \\right) = \\frac{\\alpha\\theta^{\\alpha}}{x^{\\alpha + 1}} \\ \\ \\text{and} \\ \\ F_{X}\\left( x \\right) = 1 - \\left( \\frac{\\theta}{x} \\right)^{\\alpha},\\] for \\(x &gt; \\theta\\), respectively. Then, the likelihood and loglikelihood functions are given by \\[L\\left( \\alpha \\right) = \\frac{\\alpha^{8}}{\\prod_{i = 1}^{8}x_{i}^{\\alpha + 1}}\\frac{5^{10\\alpha}}{25^{2\\alpha}},\\] \\[\\ln L \\left( \\alpha \\right) = 8ln\\alpha - \\left( \\alpha + 1 \\right)\\sum_{i = 1}^{8}{\\ln x_{i}} + 10\\alpha ln5 - 2\\alpha ln25.\\] \\(\\frac{\\text{dlnL}\\left( \\alpha \\right)}{d \\theta} = \\frac{8}{\\alpha} - \\sum_{i = 1}^{8}{\\ln x_{i}} + 10ln5 - 2ln25\\). The maximum likelihood estimator, \\(\\hat{\\alpha}\\), is the solution to the equation \\[\\frac{8}{\\hat{\\alpha}} - \\sum_{i = 1}^{8}{\\ln x_{i}} + 10ln5 - 2ln25 = 0,\\]which yields \\[\\hat{\\alpha} = \\frac{8}{\\sum_{i = 1}^{8}{\\ln x_{i}} - 10ln5 + 2ln25} = \\frac{8}{(ln7 + ln9 + \\ldots + ln20) - 10ln5 + 2ln25} = 0.785.\\] The mean of the Pareto only exists for \\(\\alpha &gt; 1\\). Since \\(\\hat{\\alpha} = 0.785 &lt; 1\\). Then, the mean does not exist. 3.6 Further Resources and Contributors In describing losses, actuaries fit appropriate parametric distribution models for the frequency and severity of loss. This involves finding appropriate statistical distributions that could efficiently model the data in hand. After fitting a distribution model to a data set, the model should be validated. Model validation is a crucial step in the model building sequence. It assesses how well these statistical distributions fit the data in hand and how well can we expect this model to perform in the future. If the selected model does not fit the data, another distribution is to be chosen. If more than one model seems to be a good fit for the data, we then have to make the choice on which model to use. It should be noted though that the same data should not serve for both purposes (fitting and validating the model). Additional data should be used to assess the performance of the model. There are many statistical tools for model validation. Alternative goodness of fit tests used to determine whether sample data are consistent with the candidate model, will be presented in a separate chapter. Further Readings and References Cummins, J. D. and Derrig, R. A. 1991. Managing the Insolvency Risk of Insurance Companies, Springer Science+ Business Media, LLC. Frees, E. W. and Valdez, E. A. 2008. Hierarchical insurance claims modeling, Journal of the American Statistical Association, 103, 1457-1469. Klugman, S. A., Panjer, H. H. and Willmot, G. E. 2008. Loss Models from Data to Decisions, Wiley. Kreer, M., Kizilers, A., Thomas, A. W. and Eg?dio dos Reis, A. D. 2015. Goodness-of-fit tests and applications for left-truncated Weibull distributions to non-life insurance, European Actuarial Journal, 5, 139-163. McDonald, J. B. 1984. Some generalized functions for the size distribution of income, Econometrica 52, 647-663. McDonald, J. B. and Xu, Y. J. 1995. A generalization of the beta distribution with applications, Journal of Econometrics 66, 133-52. Tevet, D. 2016. Applying generalized linear models to insurance data: Frequency/severity versus premium modeling in: Frees, E. W., Derrig, A. R. and Meyers G. (Eds.) Predictive Modeling Applications in Actuarial Science Vol. II Case Studies in Insurance. Cambridge University Press. Venter, G. 1983. Transformed beta and gamma distributions and aggregate losses. Proceedings of the Casualty Actuarial Society 70: 156-193. Contributors Zeinab Amin, The American University in Cairo, is the principal author of this chapter. Date: October 27, 2016. Email: zeinabha@aucegypt.edu for chapter comments and suggested improvements. Many helpful comments have been provided by Hirokazu (Iwahiro) Iwasawa, iwahiro@bb.mbn.or.jp . 3.7 Exercises Here are a set of exercises that guide the viewer through some of the theoretical foundations of Loss Data Analytics. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C. Severity Distribution Guided Tutorials Bibliography "],
["C-ModelSelection.html", "Chapter 4 Model Selection, Validation, and Inference 4.1 Nonparametric Inference 4.2 Model Validation 4.3 Modified Data 4.4 Bayesian Inference 4.5 Exercises Technical Supplement A. Gini Statistic", " Chapter 4 Model Selection, Validation, and Inference Chapter Preview. Chapters 2 and 3 have described how to fit parametric models to frequency and severity data, respectively. This chapter describes selection of models. To compare alternative parametric models, it is helpful to introduce models that summarize data without reference to a specific parametric distribution. Section 4.1 describes nonparametric estimation, how we can use it for model comparisons and how it can be used to provide starting values for parametric procedures. The process of model selection is then summarized in Section 4.2. Although our focus is on continuous data, the same process can be used for discrete data or data that is a hybrid combination of discrete and continuous data. Further, Section 4.3 introduces for alternative sampling schemes, included grouped, censored and truncated data. The chapter closes with Section 4.4 on Bayesian inference, an alternative procedure where the (typically unknown) parameters are treated as random variables. 4.1 Nonparametric Inference In this section, you learn how to: Estimate moments, quantiles, and distributions without reference to a parametric distribution Summarize the data graphically without reference to a parametric distribution Determine measures that summarize deviations of a parametric from a nonparametric fit Use nonparametric estimators to approximate parameters that can be used to start a parametric estimation procedure Consider \\(X_1, \\ldots, X_n\\), a random sample (with replacement) from an unknown underlying population distribution \\(F(\\cdot)\\). As independent draws from the same distribution, we say that \\(X_1, \\ldots, X_n\\) are independently and identically distributed (iid) random variables. Now say we have a data sample, \\(x_1, \\dots, x_n\\), which represents a realization of \\(X_1, \\ldots, X_n\\). Note that \\(x_1, \\ldots, x_n\\) is non-random; it is simply a particular set of data values, i.e. an observation of the random variables \\(X_1, \\ldots, X_n\\). Using this sample, we will try to estimate the population distribution function \\(F(\\cdot)\\). We first proceed with a nonparametric analysis, in which we do not assume or rely on any explicit parametric distributional forms for \\(F(\\cdot)\\). 4.1.1 Nonparametric Estimation The population distribution \\(F(\\cdot)\\) can be summarized in various ways. These include moments, the distribution function \\(F(\\cdot)\\) itself, the quantiles or percentiles associated with the distribution, and the corresponding mass or density function \\(f(\\cdot)\\). Summary statistics based on the sample, \\(X_1, \\ldots, X_n\\), are known as nonparametric estimators of the corresponding summary measures of the distribution. We will examine moment estimators, distribution function estimators, quantile estimators, and density estimators, as well as their statistical properties such as expected value and variance. Using our data observations \\(x_1, \\ldots, x_n\\), we can put numerical values to these estimators and compute nonparametric estimates. 4.1.1.1 Moment Estimators The \\(k\\)-th moment, \\(\\mathrm{E~}[X^k] = \\mu^{\\prime}_k\\), is our first example of a population summary measure. It is estimated with the corresponding sample statistic \\[\\frac{1}{n} \\sum_{i=1}^n X_i^k .\\] In typical applications, \\(k\\) is a positive integer, although it need not be. For the first moment (\\(k=1\\)), the prime symbol (\\(\\prime\\)) and the \\(1\\) subscript are usually dropped, using \\(\\mu=\\mu^{\\prime}_1\\) to denote the mean. The corresponding sample estimator for \\(\\mu\\) is called the sample mean, denoted with a bar on top of the random variable: \\[\\bar{X} =\\frac{1}{n} \\sum_{i=1}^n X_i .\\] Sometimes, \\(\\mu^{\\prime}_k\\) is called the \\(k\\)-th raw moment to distinguish it from the \\(k\\)-th central moment, \\(\\mathrm{E~} [(X-\\mu)^k] = \\mu_k\\), which is estimated as \\[\\frac{1}{n} \\sum_{i=1}^n \\left(X_i - \\bar{X}\\right)^k .\\] The second central moment (\\(k=2\\)) is an important case for which we typically assign a new symbol, \\(\\sigma^2 = \\mathrm{E~} [(X-\\mu)^2]\\), known as the variance. The corresponding sample estimator for \\(\\sigma^2\\) is called the sample variance. 4.1.1.2 Empirical Distribution Function To estimate the distribution function nonparametrically, we define the empirical distribution function to be \\[\\begin{aligned} F_n(x) &amp;= \\frac{\\text{number of observations less than or equal to }x}{n} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n I\\left(X_i \\le x\\right). \\end{aligned}\\] Here, the notation \\(I(\\cdot)\\) is the indicator function; it returns 1 if the event \\((\\cdot)\\) is true and 0 otherwise. Example – Toy Data Set. To illustrate, consider a fictitious, or “toy,” data set of \\(n=10\\) observations. Determine the empirical distribution function. \\[\\begin{array}{c|cccccccccc} \\hline i &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\\\ X_i&amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\\\ \\hline \\end{array}\\] Show Example Solution You should check that the sample mean is \\(\\bar{x} = 19.7\\) and that the sample variance is \\(34.45556\\). The corresponding empirical distribution function is \\[\\begin{aligned} F_n(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; \\text{ for }\\ x&lt;10 \\\\ 0.1 &amp; \\text{ for }\\ 10 \\leq x&lt;15 \\\\ 0.4 &amp; \\text{ for }\\ 15 \\leq x&lt;20 \\\\ 0.5 &amp; \\text{ for }\\ 20 \\leq x&lt;23 \\\\ 0.9 &amp; \\text{ for }\\ 23 \\leq x&lt;30 \\\\ 1 &amp; \\text{ for }\\ x \\geq 30, \\end{array} \\right.\\end{aligned}\\] which is shown in the following graph in Figure 4.1. Figure 4.1: Empirical Distribution Function of a Toy Example Show R Code (xExample &lt;- c(10,rep(15,3),20,rep(23,4),30)) PercentilesxExample &lt;- ecdf(xExample) plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;) 4.1.1.3 Quantiles We have already seen the median, which is the number such that approximately half of a data set is below (or above) it. The first quartile is the number such that approximately 25% of the data is below it and the third quartile is the number such that approximately 75% of the data is below it. A \\(100p\\) percentile is the number such that \\(100 \\times p\\) percent of the data is below it. To generalize this concept, consider a distribution function \\(F(\\cdot)\\), which may or may not be from a continuous variable, and let \\(q\\) be a fraction so that \\(0&lt;q&lt;1\\). We want to define a quantile, say \\(q_F\\), to be a number such that \\(F(q_F) \\approx q\\). Notice that when \\(q = 0.5\\), \\(q_F\\) is the median; when \\(q = 0.25\\), \\(q_F\\) is the first quartile, and so on. To be precise, for a given \\(0&lt;q&lt;1\\), define the \\(q\\)th quantile \\(q_F\\) to be any number that satisfies \\[\\begin{equation} F(q_F-) \\le q \\le F(q_F) \\tag{4.1} \\end{equation}\\] Here, the notation \\(F(x-)\\) means to evaluate the function \\(F(\\cdot)\\) as a left-hand limit. To get a better understanding of this definition, let us look at a few special cases. First, consider the case where \\(X\\) is a continuous random variable so that the distribution function \\(F(\\cdot)\\) has no jump points, as illustrated in Figure 4.2. In this figure, a few fractions, \\(q_1\\), \\(q_2\\), and \\(q_3\\) are shown with their corresponding quantiles \\(q_{F,1}\\), \\(q_{F,2}\\), and \\(q_{F,3}\\). In each case, it can be seen that \\(F(q_F-)= F(q_F)\\) so that there is a unique quantile. Because we can find a unique inverse of the distribution function at any \\(0&lt;q&lt;1\\), we can write \\(q_F= F^{-1}(q)\\). Figure 4.2: Continuous Quantile Case Figure 4.3 shows three cases for distribution functions. The left panel corresponds to the continuous case just discussed. The middle panel displays a jump point similar to those we already saw in the empirical distribution function of Figure 4.1. For the value of \\(q\\) shown in this panel, we still have a unique value of the quantile \\(q_F\\). Even though there are many values of \\(q\\) such that \\(F(q_F-) \\le q \\le F(q_F)\\), for a particular value of \\(q\\), there is only one solution to equation (4.1). The right panel depicts a situation in which the quantile can not be uniquely determined for the \\(q\\) shown as there is a range of \\(q_F\\)’s satisfying equation (4.1). Figure 4.3: Three Quantile Cases Example – Toy Data Set: Continued. Determine quantiles corresponding to the 20th, 50th, and 95th percentiles. Show Example Solution Solution. Consider Figure 4.1. The case of \\(q=0.20\\) corresponds to the middle panel, so the 20th percentile is 15. The case of \\(q=0.50\\) corresponds to the right panel, so the median is any number between 20 and 23 inclusive. Many software packages use the average 21.5 (e.g. R, as seen below). For the 95th percentile, the solution is 30. We can see from the graph that 30 also corresponds to the 99th and the 99.99th percentiles. quantile(xExample, probs=c(0.2, 0.5, 0.95), type=6) ## 20% 50% 95% ## 15.0 21.5 30.0 By taking a weighted average between data observations, smoothed empirical quantiles can handle cases such as the right panel in Figure 4.3. The \\(q\\)th smoothed empirical quantile is defined as \\[\\hat{\\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\\] where \\(j=\\lfloor(n+1)q\\rfloor\\), \\(h=(n+1)q-j\\), and \\(X_{(1)}, \\ldots, X_{(n)}\\) are the ordered values (the order statistics) corresponding to \\(X_1, \\ldots, X_n\\). Note that this is a linear interpolation between \\(X_{(j)}\\) and \\(X_{(j+1)}\\). Example – Toy Data Set: Continued. Determine the 50th and 20th smoothed percentiles. Show Example Solution Solution: Take \\(n=10\\) and \\(q=0.5\\). Then, \\(j=\\lfloor(11)0.5 \\rfloor= \\lfloor5.5 \\rfloor=5\\) and \\(h=(11)(0.5)-5=0.5\\). Then the 0.5-th smoothed empirical quantile is \\[\\hat{\\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\\] Now take \\(n=10\\) and \\(q=0.2\\). In this case, \\(j=\\lfloor(11)0.2\\rfloor=\\lfloor 2.2 \\rfloor=2\\) and \\(h=(11)(0.2)-2=0.2\\). Then the 0.2-th smoothed empirical quantile is \\[\\hat{\\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.2 (15) + (0.8)(15) = 15.\\] 4.1.1.4 Density Estimators When the random variable is discrete, estimating the probability mass function \\(f(x) = \\Pr(X=x)\\) is straightforward. We simply use the empirical average, defined to be \\[f_n(x) = \\frac{1}{n} \\sum_{i=1}^n I(X_i = x).\\] For a continuous random variable, consider a discretized formulation in which the domain of \\(F(\\cdot)\\) is partitioned by constants \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) into intervals of the form \\([c_{j-1}, c_j)\\), for \\(j=1, \\ldots, k\\). The data observations are thus “grouped” by the intervals into which they fall. Then, we might use the basic definition of the empirical mass function, or a variation such as \\[f_n(x) = \\frac{n_j}{n \\times (c_j - c_{j-1})} \\ \\ \\ \\ \\ \\ c_{j-1} \\le x &lt; c_j,\\] where \\(n_j\\) is the number of observations (\\(X_i\\)) that fall into the interval \\([c_{j-1}, c_j)\\). Extending this notion to instances where we observe individual data, note that we can always create arbitrary groupings and use this formula. More formally, let \\(b&gt;0\\) be a small positive constant, known as a bandwidth, and define a density estimator to be \\[\\begin{equation} f_n(x) = \\frac{1}{2nb} \\sum_{i=1}^n I(x-b &lt; X_i \\le x + b) \\tag{4.2} \\end{equation}\\] Show A Snippet of Theory The idea is that the estimator \\(f_n(x)\\) in equation (4.2) is the average over \\(n\\) iid realizations of a random variable with mean \\[\\begin{aligned} \\mathrm{E~ } \\frac{1}{2b} I(x-b &lt; X \\le x + b) &amp;= \\frac{1}{2b}\\left(F(x+b)-F(x-b)\\right) \\\\ &amp;= \\frac{1}{2b} \\left( \\left\\{ F(x) + b F^{\\prime}(x) + b^2 C_1\\right\\} \\left\\{ F(x) - b F^{\\prime}(x) + b^2 C_2\\right\\} \\right) \\\\ &amp;= F^{\\prime}(x) + b \\frac{C_1-C_2}{2} \\rightarrow F^{\\prime}(x) = f(x), \\end{aligned}\\] as \\(b\\rightarrow 0\\). That is, \\(f_n(x)\\) is an asymptotically unbiased estimator of \\(f(x)\\) (its expectation approaches the true value as sample size increases to infinity). This development assumes some smoothness of \\(F(\\cdot)\\), in particular, twice differentiability at \\(x\\), but makes no assumptions on the form of the distribution function \\(F\\). Because of this, the density estimator \\(f_n\\) is said to be nonparametric. More generally, define the kernel density estimator as \\[\\begin{equation} f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right) \\tag{4.3} \\end{equation}\\] where \\(w\\) is a probability density function centered about 0. Note that equation (4.2) simply becomes the kernel density estimator where \\(w(x) = \\frac{1}{2}I(-1 &lt; x \\le 1)\\), also known as the uniform kernel. Other popular choices are shown in the table below. \\[\\text{Table 4.1: Popular Choices for the Kernel Density Estimator} \\\\ \\begin{array}{l|cc} \\hline \\text{Kernel} &amp; w(x) \\\\ \\hline \\text{Uniform } &amp; \\frac{1}{2}I(-1 &lt; x \\le 1) \\\\ \\text{Triangle} &amp; (1-|x|)\\times I(|x| \\le 1) \\\\ \\text{Epanechnikov} &amp; \\frac{3}{4}(1-x^2) \\times I(|x| \\le 1) \\\\ \\text{Gaussian} &amp; \\phi(x) \\\\ \\hline \\end{array}\\] Here, \\(\\phi(\\cdot)\\) is the standard normal density function. As we will see in the following example, the choice of bandwidth \\(b\\) comes with a bias-variance tradeoff between matching local distributional features and reducing the volatility. Example – Property Fund. Figure 4.4 shows a histogram (with shaded gray rectangles) of logarithmic property claims from 2010. The (blue) thick curve represents a Gaussian kernel density where the bandwidth was selected automatically using an ad hoc rule based on the sample size and volatility of the data. For this dataset, the bandwidth turned out to be \\(b=0.3255\\). For comparison, the (red) dashed curve represents the density estimator with a bandwidth equal to 0.1 and the green smooth curve uses a bandwidth of 1. As anticipated, the smaller bandwidth (0.1) indicates taking local averages over less data so that we get a better idea of the local average, but at the price of higher volatility. In contrast, the larger bandwidth (1) smooths out local fluctuations, yielding a smoother curve that may miss perturbations in the local average. For actuarial applications, we mainly use the kernel density estimator to get a quick visual impression of the data. From this perspective, you can simply use the default ad hoc rule for bandwidth selection, knowing that you have the ability to change it depending on the situation at hand. Figure 4.4: Histogram of Logarithmic Property Claims with Superimposed Kernel Density Estimators Show R Code #Density Comparison hist(log(ClaimData$Claim), main=&quot;&quot;, ylim=c(0,.35),xlab=&quot;Log Expenditures&quot;, freq=FALSE, col=&quot;lightgray&quot;) lines(density(log(ClaimData$Claim)), col=&quot;blue&quot;,lwd=2.5) lines(density(log(ClaimData$Claim), bw=1), col=&quot;green&quot;) lines(density(log(ClaimData$Claim), bw=.1), col=&quot;red&quot;, lty=3) legend(&quot;topright&quot;, c(&quot;b=0.3255 (default)&quot;, &quot;b=0.1&quot;, &quot;b=1.0&quot;), lty=c(1,3,1), lwd=c(2.5,1,1), col=c(&quot;blue&quot;, &quot;red&quot;, &quot;green&quot;), cex=1) Nonparametric density estimators, such as the kernel estimator, are regularly used in practice. The concept can also be extended to give smooth versions of an empirical distribution function. Given the definition of the kernel density estimator, the kernel estimator of the distribution function can be found as \\[\\begin{aligned} \\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n W\\left(\\frac{x-X_i}{b}\\right).\\end{aligned}\\] where \\(W\\) is the distribution function associated with the kernel density \\(w\\). To illustrate, for the uniform kernel, we have \\(w(y) = \\frac{1}{2}I(-1 &lt; y \\le 1)\\), so \\[\\begin{aligned} W(y) = \\begin{cases} 0 &amp; y&lt;-1\\\\ \\frac{y+1}{2}&amp; -1 \\le y &lt; 1 \\\\ 1 &amp; y \\ge 1 \\\\ \\end{cases}\\end{aligned}\\] Exercise – Exam C Question 3. You study five lives to estimate the time from the onset of a disease to death. The times to death are: \\[\\begin{array}{ccccc} 2 &amp; 3 &amp; 3 &amp; 3 &amp; 7 \\\\ \\end{array}\\] Using a triangular kernel with bandwith \\(2\\), calculate the density function estimate at 2.5. Show Solution Solution: For the kernel density estimate, we have \\[f_n(x) = \\frac{1}{nb} \\sum_{i=1}^n w\\left(\\frac{x-X_i}{b}\\right),\\] where \\(n=5\\), \\(b=2\\), and \\(x=2.5\\). For the triangular kernel, \\(w(x) = (1-|x|)\\times I(|x| \\le 1)\\). Thus, \\[\\begin{array}{c|c|c} \\hline X_i &amp; \\frac{x-X_i}{b} &amp; w\\left(\\frac{x-X_i}{b} \\right) \\\\ \\hline 2 &amp; \\frac{2.5-2}{2}=\\frac{1}{4} &amp; (1-\\frac{1}{4})(1) = \\frac{3}{4} \\\\ \\hline 3 &amp; &amp; \\\\ 3 &amp; \\frac{2.5-3}{2}=\\frac{-1}{4} &amp; \\left(1-\\left| \\frac{-1}{4} \\right| \\right)(1) = \\frac{3}{4} \\\\ 3 &amp; &amp; \\\\ \\hline 7 &amp; \\frac{2.5-7}{2}=-2.25 &amp; (1-|-2.25|)(0) = 0\\\\ \\hline \\end{array}\\] Then the kernel density estimate is \\[f_n(x) = \\frac{1}{5(2)}\\left( \\frac{3}{4} + (3) \\frac{3}{4} + 0 \\right) = \\frac{3}{10}\\] 4.1.2 Tools for Model Selection The previous section introduced nonparametric estimators in which there was no parametric form assumed about the underlying distributions. However, in many actuarial applications, analysts seek to employ a parametric fit of a distribution for ease of explanation and the ability to readily extend it to more complex situations such as including explanatory variables in a regression setting. When fitting a parametric distribution, one analyst might try to use a gamma distribution to represent a set of loss data. However, another analyst may prefer to use a Pareto distribution. How does one know which model to select? Nonparametric tools can be used to corroborate the selection of parametric models. Essentially, the approach is to compute selected summary measures under a fitted parametric model and to compare it to the corresponding quantity under the nonparametric model. As the nonparametric does not assume a specific distribution and is merely a function of the data, it is used as a benchmark to assess how well the parametric distribution/model represents the data. This comparison may alert the analyst to deficiencies in the parametric model and sometimes point ways to improving the parametric specification. 4.1.2.1 Graphical Comparison of Distributions We have already seen the technique of overlaying graphs for comparison purposes. To reinforce the application of this technique, Figure 4.5 compares the empirical distribution to two parametric fitted distributions. The left panel shows the distribution functions of claims distributions. The dots forming an “S-shaped” curve represent the empirical distribution function at each observation. The thick blue curve gives corresponding values for the fitted gamma distribution and the light purple is for the fitted Pareto distribution. Because the Pareto is much closer to the empirical distribution function than the gamma, this provides evidence that the Pareto is the better model for this data set. The right panel gives similar information for the density function and provides a consistent message. Based on these figures, the Pareto distribution is the clear choice for the analyst. Figure 4.5: Nonparametric Versus Fitted Parametric Distribution and Density Functions. The left-hand panel compares distribution functions, with the dots corresponding to the empirical distribution, the thick blue curve corresponding to the fitted gamma and the light purple curve corresponding to the fitted Pareto. The right hand panel compares these three distributions summarized using probability density functions. For another way to compare the appropriateness of two fitted models, consider the probability-probability (\\(pp\\)) plot. A \\(pp\\) plot compares cumulative probabilities under two models. For our purposes, these two models are the nonparametric empirical distribution function and the parametric fitted model. Figure 4.6 shows \\(pp\\) plots for the Property Fund data. The fitted gamma is on the left and the fitted Pareto is on the right, compared to the same empirical distribution function of the data. The straight line represents equality between the two distributions being compared, so points close to the line are desirable. As seen in earlier demonstrations, the Pareto is much closer to the empirical distribution than the gamma, providing additional evidence that the Pareto is the better model. Figure 4.6: Probability-Probability (\\(pp\\)) Plots. The horizontal axes gives the empirical distribution function at each observation. In the left-hand panel, the corresponding distribution function for the gamma is shown in the vertical axis. The right-hand panel shows the fitted Pareto distribution. Lines of \\(y=x\\) are superimposed. A \\(pp\\) plot is useful in part because no artificial scaling is required, such as with the overlaying of densities in Figure 4.5, in which we switched to the log scale to better visualize the data. Furthermore, \\(pp\\) plots are available in multivariate settings where more than one outcome variable is available. However, a limitation of the \\(pp\\) plot is that, because they plot cumulative distribution functions, it can sometimes be difficult to detect where a fitted parametric distribution is deficient. As an alternative, it is common to use a quantile-quantile (\\(qq\\)) plot, as demonstrated in Figure 4.7. The \\(qq\\) plot compares two fitted models through their quantiles. As with \\(pp\\) plots, we compare the nonparametric to a parametric fitted model. Quantiles may be evaluated at each point of the data set, or on a grid (e.g., at \\(0, 0.001, 0.002, \\ldots, 0.999, 1.000\\)), depending on the application. In Figure 4.7, for each point on the aforementioned grid, the horizontal axis displays the empirical quantile and the vertical axis displays the corresponding fitted parametric quantile (gamma for the upper two panels, Pareto for the lower two). Quantiles are plotted on the original scale in the left panels and on the log scale in the right panels to allow us to see where a fitted distribution is deficient. The straight line represents equality between the empirical distribution and fitted distribution. From these plots, we again see that the Pareto is an overall better fit than the gamma. Furthermore, the lower-right panel suggests that the Pareto distribution does a good job with large observations, but provides a poorer fit for small observations. Figure 4.7: Quantile-Quantile (\\(qq\\)) Plots. The horizontal axes gives the empirical quantiles at each observation. The right-hand panels they are graphed on a logarithmic basis. The vertical axis gives the quantiles from the fitted distributions; Gamma quantiles are in the upper panels, Pareto quantiles are in the lower panels. Exercise – Exam C Question 59. The graph below shows a \\(pp\\) plot of a fitted distribution compared to a sample. Comment on the two distributions with respect to left tail, right tail, and median probabilities. Show Solution Solution: The tail of the fitted distribution is too thick on the left, too thin on the right, and the fitted distribution has less probability around the median than the sample. To see this, recall that the \\(pp\\) plot graphs the cumulative distribution of two distributions on its axes (empirical on the x-axis and fitted on the y-axis in this case). For small values of \\(x\\), the fitted model assigns greater probability to being below that value than occurred in the sample (i.e. \\(F(x) &gt; F_n(x)\\)). This indicates that the model has a heavier left tail than the data. For large values of \\(x\\), the model again assigns greater probability to being below that value and thus less probability to being above that value (i.e. \\(S(x) &lt; S_n(x)\\). This indicates that the model has a lighter right tail than the data. In addition, as we go from 0.4 to 0.6 on the horizontal axis (thus looking at the middle 20% of the data), the \\(pp\\) plot increases from about 0.3 to 0.4. This indicates that the model puts only about 10% of the probability in this range. 4.1.2.2 Statistical Comparison of Distributions When selecting a model, it is helpful to make the graphical displays presented. However, for reporting results, it can be effective to supplement the graphical displays with selected statistics that summarize model goodness of fit. Table 4.2 provides three commonly used goodness of fit statistics. Here, \\(F_n\\) is the empirical distribution and \\(F\\) is the fitted distribution. \\[\\text{Table 4.2: Three Goodness of Fit Statistics} \\\\ \\begin{array}{l|cc} \\hline \\text{Statistic} &amp; \\text{Definition} &amp; \\text{Computational Expression} \\\\ \\hline \\text{Kolmogorov-Smirnov} &amp; \\max_x |F_n(x) - F(x)| &amp; \\max(D^+, D^-) \\text{ where } \\\\ &amp;&amp; D^+ = \\max_{i=1, \\ldots, n} \\left|\\frac{i}{n} - F_i\\right| \\\\ &amp;&amp; D^- = \\max_{i=1, \\ldots, n} \\left| F_i - \\frac{i-1}{n} \\right| \\\\ \\text{Cramer-von Mises} &amp; n \\int (F_n(x) - F(x))^2 f(x) dx &amp; \\frac{1}{12n} + \\sum_{i=1}^n \\left(F_i - (2i-1)/n\\right)^2 \\\\ \\text{Anderson-Darling} &amp; n \\int \\frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} f(x) dx &amp; -n-\\frac{1}{n} \\sum_{i=1}^n (2i-1) \\log\\left(F_i(1-F_{n+1-i})\\right)^2 \\\\ \\hline \\end{array} \\\\ \\text{where } F_i \\text{ is defined to be } F(x_i).\\] The Kolmogorov-Smirnov statistic is the maximum absolute difference between the fitted distribution function and the empirical distribution function. Instead of comparing differences between single points, the Cramer-von Mises statistic integrates the difference between the empirical and fitted distribution functions over the entire range of values. The Anderson-Darling statistic also integrates this difference over the range of values, although weighted by the inverse of the variance. It therefore places greater emphasis on the tails of the distribution (i.e when \\(F(x)\\) or \\(1-F(x)=S(x)\\) is small). Exercise – Exam C Question 40 (modified). A sample of claim payments is: \\[\\begin{array}{ccccc} 29 &amp; 64 &amp; 90 &amp; 135 &amp; 182 \\\\ \\end{array}\\] Compare the empirical claims distribution to an exponential distribution with mean \\(100\\) by calculating the value of the Kolmogorov-Smirnov test statistic. Show Solution Solution: For an exponential distribution with mean \\(100\\), the cumulative distribution function is \\(F(x)=1-e^{-x/100}\\). Thus, \\[\\begin{array}{ccccc} \\hline x &amp; F(x) &amp; F_n(x) &amp; F_n(x-) &amp; \\max(|F(x)-F_n(x)|,|F(x)-F_n(x-)|) \\\\ \\hline 29 &amp; 0.2517 &amp; 0.2 &amp; 0 &amp; \\max(0.0517, 0.2517) = 0.2517 \\\\ 64 &amp; 0.4727 &amp; 0.4 &amp; 0.2 &amp; \\max(0.0727, 0.2727) = 0.2727 \\\\ 90 &amp; 0.5934 &amp; 0.6 &amp; 0.4 &amp; \\max(0.0066, 0.1934) = 0.1934 \\\\ 135 &amp; 0.7408 &amp; 0.8 &amp; 0.6 &amp; \\max(0.0592, 0.1408) = 0.1408 \\\\ 182 &amp; 0.8380 &amp; 1 &amp; 0.8 &amp; \\max(0.1620, 0.0380) = 0.1620 \\\\ \\hline \\end{array}\\] The Kolmogorov-Smirnov test statistic is therefore \\(KS = \\max(0.2517, 0.2727, 0.1934, 0.1408, 0.1620) = 0.2727\\). 4.1.3 Starting Values The method of moments and percentile matching are nonparametric estimation methods that provide alternatives to maximum likelihood. Generally, maximum likelihood is the preferred technique because it employs data more efficiently. However, methods of moments and percentile matching are useful because they are easier to interpret and therefore allow the actuary or analyst to explain procedures to others. Additionally, the numerical estimation procedure (e.g. if performed in R) for the maximum likelihood is iterative and requires starting values to begin the recursive process. Although many problems are robust to the choice of the starting values, for some complex situations, it can be important to have a starting value that is close to the (unknown) optimal value. Method of moments and percentile matching are techniques that can produce desirable estimates without a serious computational investment and can thus be used as a starting value for computing maximum likelihood. 4.1.3.1 Method of Moments Under the method of moments, we approximate the moments of the parametric distribution using the empirical (nonparametric) moments described in Section 4.1.1.1. We can then algebraically solve for the parameter estimates. Example – Property Fund. For the 2010 property fund, there are \\(n=1,377\\) individual claims (in thousands of dollars) with \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = 26.62259 \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = 136154.6 .\\] Fit the parameters of the gamma and Pareto distributions using the method of moments. Show Example Solution To fit a gamma distribution, we have \\(\\mu_1 = \\alpha \\theta\\) and \\(\\mu_2^{\\prime} = \\alpha(\\alpha+1) \\theta^2\\). Equating the two yields the method of moments estimators, easy algebra shows that \\[\\alpha = \\frac{\\mu_1^2}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\text{and} \\ \\ \\ \\theta = \\frac{\\mu_2^{\\prime}-\\mu_1^2}{\\mu_1}.\\] Thus, the method of moment estimators are \\[\\begin{aligned} \\hat{\\alpha} &amp;= \\frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809 \\\\ \\hat{\\theta} &amp;= \\frac{136154.6-26.62259^2}{26.62259} = 5,087.629. \\end{aligned}\\] For comparison, the maximum likelihood values turn out to be \\(\\hat{\\alpha}_{MLE} = 0.2905959\\) and \\(\\hat{\\theta}_{MLE} = 91.61378\\), so there are big discrepancies between the two estimation procedures. This is one indication, as we have seen before, that the gamma model fits poorly. In contrast, now assume a Pareto distribution so that \\(\\mu_1 = \\theta/(\\alpha -1)\\) and \\(\\mu_2^{\\prime} = 2\\theta^2/((\\alpha-1)(\\alpha-2) )\\). Easy algebra shows \\[\\alpha = 1+ \\frac{\\mu_2^{\\prime}}{\\mu_2^{\\prime}-\\mu_1^2} \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\ \\theta = (\\alpha-1)\\mu_1.\\] Thus, the method of moment estimators are \\[\\begin{aligned} \\hat{\\alpha} &amp;= 1+ \\frac{136154.6}{136154.6-26,62259^2} = 2.005233 \\\\ \\hat{\\theta} &amp;= (2.005233-1) \\cdot 26.62259 = 26.7619 \\end{aligned}\\] The maximum likelihood values turn out to be \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) and \\(\\hat{\\theta}_{MLE} = 2.2821147\\). It is interesting that \\(\\hat{\\alpha}_{MLE}&lt;1\\); for the Pareto distribution, recall that \\(\\alpha &lt;1\\) means that the mean is infinite. This is another indication that the property claims data set is a long tail distribution. As the above example suggests, there is flexibility with the method of moments. For example, we could have matched the second and third moments instead of the first and second, yielding different estimators. Furthermore, there is no guarantee that a solution will exist for each problem. You will also find that matching moments is possible for a few problems where the data are censored or truncated, but in general, this is a more difficult scenario. Finally, for distributions where the moments do not exist or are infinite, method of moments is not available. As an alternative for the infinite moment situation, one can use the percentile matching technique. 4.1.3.2 Percentile Matching Under percentile matching, we approximate the quantiles or percentiles of the parametric distribution using the empirical (nonparametric) quantiles or percentiles described in Section 4.1.1.3. Show Example Example – Property Fund. For the 2010 property fund, we illustrate matching on quantiles. In particular, the Pareto distribution is intuitively pleasing because of the closed-form solution for the quantiles. Recall that the distribution function for the Pareto distribution is \\[F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta}\\right)^{\\alpha}\\] Easy algebra shows that we can express the quantile as \\[F^{-1}(q) = \\theta \\left( (1-q)^{-1/\\alpha} -1 \\right)\\] for a fraction \\(q\\), \\(0&lt;q&lt;1\\). The 25th percentile (the first quartile) turns out to be \\(0.78853\\) and the 95th percentile is \\(50.98293\\) (both in thousands of dollars). With two equations \\[0.78853 = \\theta \\left( 1- (1-.25)^{-1/\\alpha} \\right) \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ 50.98293 = \\theta \\left( 1- (1-.75)^{-1/\\alpha} \\right)\\] and two unknowns, the solution is \\[\\hat{\\alpha} = 0.9412076 \\ \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\hat{\\theta} = 2.205617 .\\] We remark here that a numerical routine is required for these solutions as no analytic solution is available. Furthermore, recall that the maximum likelihood estimates are \\(\\hat{\\alpha}_{MLE} = 0.9990936\\) and \\(\\hat{\\theta}_{MLE} = 2.2821147\\), so the percentile matching provides a better approximation for the Pareto distribution than the method of moments. Exercise – Exam C Question 1. You are given: Losses follow a loglogistic distribution with cumulative distribution function: \\[F(x) = \\frac{\\left(x/\\theta\\right)^{\\gamma}}{1+\\left(x/\\theta\\right)^{\\gamma}}\\] The sample of losses is: \\[\\begin{array}{ccccccccccc} 10 &amp;35 &amp;80 &amp;86 &amp;90 &amp;120 &amp;158 &amp;180 &amp;200 &amp;210 &amp;1500 \\\\ \\end{array}\\] Calculate the estimate of \\(\\theta\\) by percentile matching, using the 40th and 80th empirically smoothed percentile estimates. Show Solution Solution: With 11 observations, we have \\(j=\\lfloor(n+1)q\\rfloor = \\lfloor 12(0.4) \\rfloor = \\lfloor 4.8\\rfloor=4\\) and \\(h=(n+1)q-j = 12(0.4)-4=0.8\\). By interpolation, the 40th empirically smoothed percentile estimate is \\(\\hat{\\pi}_{0.4} = (1-h) X_{(j)} + h X_{(j+1)} = 0.2(86)+0.8(90)=89.2\\). Similarly, for the 80th empirically smoothed percentile estimate, we have \\(12(0.8)=9.6\\) so the estimate is \\(\\hat{\\pi}_{0.8} = 0.4(200)+0.6(210)=206\\). Using the loglogistic cumulative distribution, we need to solve the following two equations for parameters \\(\\theta\\) and \\(gamma\\): \\[0.4=\\frac{(89.2/\\theta)^\\gamma}{1+(89.2/\\theta)^\\gamma} \\ \\ \\ \\text{and} \\ \\ \\ \\ 0.8=\\frac{(206/\\theta)^\\gamma}{1+(206+\\theta)^\\gamma}\\] Solving for each parenthetical expression gives \\(\\frac{2}{3}=(89.2/\\theta)^\\gamma\\) and \\(4=(206/\\theta)^\\gamma\\). Taking the ratio of the second equation to the first gives \\(6=(206/89.2)^\\gamma \\Rightarrow \\gamma=\\frac{\\ln(6)}{\\ln(206/89.2)} = 2.1407\\). Then \\(4^{1/2.1407}=206/\\theta \\Rightarrow \\theta=107.8\\) 4.2 Model Validation In this section, you learn how to: Describe the iterative model selection specification process Outline steps needed to select a parametric model Describe pitfalls of model selection based purely on insample data when compared to the advantages of out-of-sample model validation Describe the Gini statistic for model selection This section revisits the idea that model selection is an iterative process in which models are cyclically (re)formulated and tested for appropriateness before using them for inference. After summarizing the process of selecting a model based on the dataset at hand, we will focus on the process of validating the selected model by applying it to a different dataset. 4.2.1 Iterative Model Selection In our development, we examine the data graphically, hypothesize a model structure, and compare the data to a candidate model in order to formulate an improved model. Box (1980) describes this as an iterative process which is shown in Figure 4.8. Figure 4.8: The iterative model specification process. This iterative process provides a useful recipe for structuring the task of specifying a model to represent a set of data. The first step, the model formulation stage, is accomplished by examining the data graphically and using prior knowledge of relationships, such as from economic theory or industry practice. The second step in the iteration is based on the assumptions of the specified model. These assumptions must be consistent with the data to make valid use of the model. The third step, diagnostic checking, is also known as data and model criticism; the data and model must be consistent with one another before additional inferences can be made. Diagnostic checking is an important part of the model formulation; it can reveal mistakes made in previous steps and provide ways to correct these mistakes. The iterative process also emphasizes the skills you need to make analytics work. First, you need a willingness to summarize information numerically and portray this information graphically. Second, it is important to develop an understanding of model properties. You should understand how a probabilistic model behaves in order to match a set of data to it. Third, theoretical properties of the model are also important for inferring general relationships based on the behavior of the data. 4.2.2 Summarizing Model Selection Techniques available for selecting a model depend upon whether the outcomes \\(X\\) are discrete, continuous, or a hybrid of the two, although the principles are the same. Begin by summarizing the data graphically and with statistics that do not rely on a specific parametric form, as summarized in Section 4.1. Specifically, you will want to graph both the empirical distribution and density functions. Particularly for loss data that contain many zeros and that can be skewed, deciding on the appropriate scale (e.g., logarithmic) may present some difficulties. For discrete data, tables are often preferred. Determine sample moments, such as the mean and variance, as well as selected quantiles, including the minimum, maximum, and the median. For discrete data, the mode (or most frequently occurring value) is usually helpful. These summaries, as well as your familiarity of industry practice, will suggest one or more candidate parametric models. Generally, start with the simpler parametric models (for example, one parameter exponential before a two parameter gamma), gradually introducing more complexity into the modeling process. Critique the candidate parametric model numerically and graphically. For the graphs, utilize the tools introduced in Section 4.1.2 such as \\(pp\\) and \\(qq\\) plots. For the numerical assessments, examine the statistical significance of parameters and try to eliminate parameters that do not provide additional information. For comparing model fits, if one model is a subset of another, then a likelihood ratio test may be employed. Generally, models are not proper subsets of one another so overall goodness of fit statistics, summarized in Section 1.2 \\ref{S:NonparametricModelSelection}, are useful for model comparison. For discrete data, a chi-square goodness of fit statistic is generally preferred as it is more intuitive and simpler to explain. Information criteria, such as Akaike’s Information Criterion (AIC) and the Schwarz Bayesian Criterion (BIC) are widely cited because they can be readily generalized to multivariate settings. Finally, a likelihood statistic that we have not yet considered is Vuong’s test. This statistic is gaining popularity among analysts because it is a likelihood based statistic that has the ability to compare models that are non-nested. 4.2.3 Out of Sample Validation Model validation is the process of confirming that the proposed model is appropriate, especially in light of the purposes of the investigation. An important criticism of the model selection process is that it can be susceptible to data-snooping, that is, fitting a great number of models to a single set of data. By looking at a large number of models, we may overfit the data and understate the natural variation in our representation. We can respond to this criticism by using a technique called out-of-sample validation. The ideal situation is to have available two sets of data, one for model development and one for model validation. We initially develop one or several models on a first data set. The models developed from the first set of data are called our candidate models. Then, the relative performance of the candidate models could be measured on a second set of data. In this way, the data used to validate the model is unaffected by the procedures used to formulate the model. Unfortunately, rarely will two sets of data be available to the investigator. However, we can implement the validation process by splitting the data set into two subsamples. We call these the model development subsample and validation subsample, respectively. Figure 4.9 illustrates this splitting of the data. Figure 4.9: Model Validation. A data set of size n is randomly split into two subsamples. Various researchers recommend different proportions for the allocation. Snee (1977) suggests that data-splitting not be done unless the sample size is moderately large. The guidelines of Picard and Berk (1990) show that the greater the number of parameters to be estimated, the greater the proportion of observations needed for the model development subsample. As a rule of thumb, for data sets with 100 or fewer observations, use about 25-35% of the sample for out-of-sample validation. For data sets with 500 or more observations, use 50% of the sample for out-of-sample validation. Because of these criticisms, several variants of the basic out-of-sample validation process are used by analysts. Although there is no theoretically best procedure, it is widely agreed that model validation is an important part of confirming the usefulness and appropriateness of a model. 4.3 Modified Data In this section, you learn how to: Describe grouped, censored, and truncated data Estimate parametric distributions based on grouped, censored, and truncated data Estimate distributions nonparametrically based on grouped, censored, and truncated data 4.3.1 Parametric Estimation using Modified Data Basic theory and many applications are based on individual observations that are “complete” and “unmodified,” as we have seen in the previous section. Chapter 3 introduced the concept of observations that are “modified” due to two common types of limitations: censoring and truncation. This section will address parametric estmation methods for three alternatives to individual, complete, and unmodified data: interval-censored data available only in groups, data that are limited or censored, and data that may not be observed due to truncation. 4.3.1.1 Parametric Estimation using Grouped Data Consider a sample of size \\(n\\) observed from the distribution \\(F(\\cdot)\\), but in groups so that we only know the group into which each observation fell, but not the exact value. This is referred to as grouped or interval-censored data. For example, we may be looking at two successive years of annual employee records. People employed in the first year but not the second have left sometime during the year. With an exact departure date (individual data), we could compute the amount of time that they were with the firm. Without the departure date (grouped data), we only know that they departed sometime during a year-long interval. Formalizing this idea, suppose there are \\(k\\) groups or intervals delimited by boundaries \\(c_0 &lt; c_1&lt; \\cdots &lt; c_k\\). For each observation, we only observe the interval into which it fell (e.g. \\((c_{j-1}, c_j)\\)), not the exact value. Thus, we only know the number of observations in each interval. The constants \\(\\{c_0 &lt; c_1 &lt; \\cdots &lt; c_k\\}\\) form some partition of the domain of \\(F(\\cdot)\\). Then the probability of an observation \\(X_i\\) falling in the \\(j\\)th interval is \\[\\Pr\\left(X _i \\in (c_{j-1}, c_j]\\right) = F(c_j) - F(c_{j-1}).\\] The corresponding probability mass function for an observation is \\[\\begin{aligned} f(x) &amp;= \\begin{cases} F(c_1) - F(c_{0}) &amp; \\text{if }\\ x \\in (c_{0}, c_1]\\\\ \\vdots &amp; \\vdots \\\\ F(c_k) - F(c_{k-1}) &amp; \\text{if }\\ x \\in (c_{k-1}, c_k]\\\\ \\end{cases} \\\\ &amp;= \\prod_{j=1}^k \\left\\{F(c_j) - F(c_{j-1})\\right\\}^{I(x \\in (c_{j-1}, c_j])} \\end{aligned}\\] Now, define \\(n_j\\) to be the number of observations that fall in the \\(j\\)th interval, \\((c_{j-1}, c_j]\\). Thus, the likelihood function (with respect to the parameter(s) \\(\\theta\\)) is \\[\\begin{aligned} \\mathcal{L}(\\theta) = \\prod_{j=1}^n f(x_i) = \\prod_{j=1}^k \\left\\{F(c_j) - F(c_{j-1})\\right\\}^{n_j} \\end{aligned}\\] And the log-likelihood function is \\[\\begin{aligned} L(\\theta) = \\ln \\mathcal{L}(\\theta) = \\ln \\prod_{j=1}^n f(x_i) = \\sum_{j=1}^k n_j \\ln \\left\\{F(c_j) - F(c_{j-1})\\right\\} \\end{aligned}\\] Maximizing the likelihood function (or equivalently, maximizing the log-likelihood function) would then produce the maximum likelihood estimates for grouped data. 4.3.1.2 Censored Data Censoring occurs when we observe only a limited value of an observation. The most common form is right-censoring, in which we record the smaller of the “true” dependent variable and a censoring variable. Using notation, let \\(X\\) represent an outcome of interest, such as the loss due to an insured event. Let \\(C_U\\) denote the censoring time, such as \\(C_U=5\\). With right-censored observations, we observe \\(X\\) if it is below censoring point \\(C_U\\); otherwise if \\(X\\) is higher than the censoring point, we only observe the censored \\(C_U\\). Therefore, we record \\(X_U^{\\ast}= \\min(X, C_U)\\). We also observe whether or not censoring has occurred. Let \\(\\delta_U= \\mathrm{I}(X \\geq C_U)\\) be a binary variable that is 1 if censoring occurs, \\(y \\geq C_U\\), and 0 otherwise. For example, \\(C_U\\) may represent the upper limit of coverage of an insurance policy. The loss may exceed the amount \\(C_U\\), but the insurer only has \\(C_U\\) in its records as the amount paid out and does not have the amount of the actual loss \\(X\\) in its records. Similarly, with left-censoring, we only observe \\(X\\) if \\(X\\) is above censoring point (e.g. time or loss amount) \\(C_L\\); otherwise we observe \\(C_L\\). Thus, we record \\(X_L^{\\ast}= \\max(X, C_L)\\) along with the censoring indicator \\(\\delta_L= \\mathrm{I}(X \\leq C_L)\\). For example, suppose a reinsurer will cover insurer losses greater than \\(C_L\\). Let \\(Y = X_L^{\\ast} - C_L\\) represent the amount that the reinsurer is responsible for. If the policyholder loss \\(X &lt; C_L\\), then the insurer will pay the entire claim and \\(Y =0\\), no loss for the reinsurer. If the loss \\(X \\ge C_L\\), then \\(Y = X-C_L\\) represents the reinsurer’s retained claims. If a loss occurs, the reinsurer knows the actual amount if it exceeds the limit \\(C_L\\), otherwise it only knows that it had a loss of \\(0\\). As another example of a left-censored observation, suppose we are conducting a study and interviewing a person about an event in the past. The subject may recall that the event occurred before \\(C_L\\), but not the exact date. 4.3.1.3 Truncated Data We just saw that censored observations are still available for study, although in a limited form. In contrast, truncated outcomes are a type of missing data. An outcome is potentially truncated when the availability of an observation depends on the outcome. In insurance, it is common for observations to be left-truncated at \\(C_L\\) when tfhe amount is \\[\\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{ll} \\text{we do not observe }X &amp; X &lt; C_L \\\\ X- C_L &amp; X \\geq C_L. \\end{array} \\right.\\end{aligned}\\] In other words, if \\(X\\) is less than the threshold \\(C_L\\), then it is not observed. FOr example, \\(C_L\\) may represent the deductible associated with an insurance coverage. If the insured loss is less than the deductible, then the insurer does not observe or record the loss at all. If the loss exceeds the deductible, then the excess \\(X-C_L\\) is the claim that the insurer covers. Similarly for right-truncated data, if \\(X\\) exceeds a threshold \\(C_U\\), then it is not observed. In this case, the amount is \\[\\begin{aligned} Y &amp;= \\left\\{ \\begin{array}{ll} X &amp; X &lt; C_U \\\\ \\text{we do not observe }X &amp; X \\geq C_U. \\end{array} \\right.\\end{aligned}\\] Classic examples of truncation from the right include \\(X\\) as a measure of distance to a star. When the distance exceeds a certain level \\(C_U\\), the star is no longer observable. Figure 4.10 compares truncated and censored observations. Values of \\(X\\) that are greater than the “upper” censoring limit \\(C_U\\) are not observed at all (right-censored), while values of \\(X\\) that are smaller than the “lower” truncation limit \\(C_L\\) are observed, but observed as \\(C_L\\) rather than the actual value of \\(X\\) (left-truncated). Figure 4.10: Censoring and Truncation Show Example Example – Mortality Study. Suppose that you are conducting a two-year study of mortality of high-risk subjects, beginning January 1, 2010 and finishing January 1, 2012. Figure 4.11 graphically portrays the six types of subjects recruited. For each subject, the beginning of the arrow represents that the the subject was recruited and the arrow end represents the event time. Thus, the arrow represents exposure time. Figure 4.11: Timeline for Several Subjects on Test in a Mortality Study Type A - Right-censored. This subject is alive at the beginning and the end of the study. Because the time of death is not known by the end of the study, it is right-censored. Most subjects are Type A. Type B - Complete information is available for a type B subject. The subject is alive at the beginning of the study and the death occurs within the observation period. Type C - Right-censored and left-truncated. A type C subject is right-censored, in that death occurs after the observation period. However, the subject entered after the start of the study and is said to have a delayed entry time. Because the subject would not have been observed had death occurred before entry, it is left-truncated. Type D - Left-truncated. A type D subject also has delayed entry. Because death occurs within the observation period, this subject is not right censored. Type E - Left-truncated. A type E subject is not included in the study because death occurs prior to the observation period. Type F - Right-truncated. Similarly, a type F subject is not included because the entry time occurs after the observation period. To summarize, for outcome \\(X\\) and constants \\(C_L\\) and \\(C_U\\), Limitation Type Limited Variable Censoring Information right censoring \\(X_U^{\\ast}= \\min(X, C_U)\\) \\(\\delta_U= \\mathrm{I}(X \\geq C_U)\\) left censoring \\(X_L^{\\ast}= \\max(y, C_L)\\) \\(\\delta_L= \\mathrm{I}(X \\leq C_L)\\) interval censoring right truncation \\(X\\) observe \\(X\\) if \\(X &lt; C_U\\) left truncation \\(X\\) observe \\(X\\) if \\(X &lt; C_L\\) 4.3.1.4 Parametric Estimation using Censored and Truncated Data For simplicity, we assume fixed censoring times and a continuous outcome \\(X\\). To begin, consider the case of right-censored data where we record \\(X_U^{\\ast}= \\min(X, C_U)\\) and censoring indicator \\(\\delta_U= \\mathrm{I}(X \\geq C_U)\\). If censoring occurs so that \\(\\delta_U=1\\), then \\(X \\geq C_U\\) and the likelihood is \\(\\Pr(X \\geq C_U) = 1-F(C_U)\\). If censoring does not occur so that \\(\\delta_U=0\\), then \\(X &lt; C_U\\) and the likelihood is \\(f(x)\\). Summarizing, we have the likelihood of a single observation as \\[\\begin{aligned} \\left\\{ \\begin{array}{ll} f(x) &amp; \\text{if } \\delta = 0 \\\\ 1-F(C_U) &amp; \\text{if }\\delta=1 \\end{array} \\right. = \\left( f(x)\\right)^{1-\\delta} \\left(1-F(C_U)\\right)^{\\delta} . \\end{aligned}\\] The right-hand expression allows us to present the likelihood more compactly. Now, for an iid sample of size \\(n\\), \\(\\{ (x_{U1},\\delta_1), \\ldots,(x_{Un}, \\delta_n) \\}\\), the likelihood is \\[\\mathcal{L}(\\theta) = \\prod_{i=1}^n \\left( f(x_i)\\right)^{1-\\delta_i} \\left(1-F(C_{Ui})\\right)^{\\delta_i} = \\prod_{\\delta_i=0} f(x_i) \\prod_{\\delta_i=1} \\{1-F(C_{Ui})\\},\\] with potential censoring times \\(\\{ C_{U1}, \\ldots,C_{Un} \\}\\). Here, the notation “\\(\\prod_{\\delta_i=0}\\)” means to take the product over uncensored observations, and similarly for “\\(\\prod_{\\delta_i=1}\\).” On the other hand, truncated data are handled in likelihood inference via conditional probabilities. Specifically, we adjust the likelihood contribution by dividing by the probability that the variable was observed. To summarize, we have the following contributions to the likelihood function for six types of outcomes: \\[\\begin{array}{lc} \\hline \\text{Outcome} &amp; \\text{Likelihood Contribution} \\\\ \\hline \\text{exact value} &amp; f(x) \\\\ \\text{right-censoring} &amp; 1-F(C_U) \\\\ \\text{left-censoring} &amp; F(C_L) \\\\ \\text{right-truncation} &amp; f(x)/F(C_U) \\\\ \\text{left-truncation} &amp; f(x)/(1-F(C_L)) \\\\ \\text{interval-censoring} &amp; F(C_U)-F(C_L) \\\\ \\hline \\end{array}\\] For known outcomes and censored data, the likelihood is \\[\\mathcal{L}(\\theta) = \\prod_{E} f(x_i) \\prod_{R} \\{1-F(C_{Ui})\\} \\prod_{L} F(C_{Li}) \\prod_{I} (F(C_{Ui})-F(C_{Li})),\\] where “\\(\\prod_{E}\\)” is the product over observations with Exact values, and similarly for Right-, Left- and Interval-censoring. For right-censored and left-truncated data, the likelihood is \\[\\mathcal{L}(\\theta) = \\prod_{E} \\frac{f(x_i)}{1-F(C_{Li})} \\prod_{R} \\frac{1-F(C_{Ui})}{1-F(C_{Li})},\\] and similarly for other combinations. To get further insights, consider the following. Show Example Special Case: Exponential Distribution. Consider data that are right-censored and left-truncated, with random variables \\(X_i\\) that are exponentially distributed with mean \\(\\theta\\). With these specifications, recall that \\(f(x) = \\theta^{-1} \\exp(-x/\\theta)\\) and \\(F(x) = 1-\\exp(-x/\\theta)\\). For this special case, the log-likelihood is \\[\\begin{aligned} L(\\theta) &amp;= \\sum_{E} \\left\\{ \\ln f(x_i) - \\ln (1-F(C_{Li})) \\right\\} + \\sum_{R}\\left\\{ \\ln (1-F(C_{Ui}))- \\ln (1-\\mathrm{F}(C_{Li})) \\right\\}\\\\ &amp;= \\sum_{E} (-\\ln \\theta -(x_i-C_{Li})/\\theta ) -\\sum_{R} (C_{Ui}-C_{Li})/\\theta . \\end{aligned}\\] To simplify the notation, define \\(\\delta_i = \\mathrm{I}(X_i \\geq C_{Ui})\\) to be a binary variable that indicates right-censoring. Let \\(X_i^{\\ast \\ast} = \\min(X_i, C_{Ui}) - C_{Li}\\) be the amount that the observed variable exceeds the lower truncation limit. With this, the log-likelihood is \\[\\begin{equation} L(\\theta) = - \\sum_{i=1}^n ((1-\\delta_i) \\ln \\theta + \\frac{x_i^{\\ast \\ast}}{\\theta}) \\tag{4.4} \\end{equation}\\] Taking derivatives with respect to the parameter \\(\\theta\\) and setting it equal to zero yields the maximum likelihood estimator \\[\\widehat{\\theta} = \\frac{1}{n_u} \\sum_{i=1}^n x_i^{\\ast \\ast},\\] where \\(n_u = \\sum_i (1-\\delta_i)\\) is the number of uncensored observations. Exercise – Exam C Question 44. You are given: Losses follow an exponential distribution with mean \\(\\theta\\). A random sample of 20 losses is distributed as follows: \\[\\begin{array}{l|c} \\hline \\text{Loss Range} &amp; \\text{Frequency} \\\\ \\hline [0,1000] &amp; 7 \\\\ (1000,2000] &amp; 6 \\\\ (2000,\\infty) &amp; 7 \\\\ \\hline \\end{array}\\] Calculate the maximum likelihood estimate of \\(\\theta\\). Show Solution Solution: \\[\\begin{aligned} \\mathcal{L}(\\theta) &amp;= F(1000)^7[F(2000)-F(1000)]^6[1-F(2000)]^7 \\\\ &amp;= (1-e^{-1000/\\theta})^7(e^{-1000/\\theta} - e^{-2000/\\theta})^6(e^{-2000/\\theta})^7 \\\\ &amp;= (1-p)^7(p-p^2)^6(p^2)^7 \\\\ &amp;= p^{20}(1-p)^{13} \\end{aligned}\\] where \\(p=e^{-1000/\\theta}\\). Maximizing this expression with respect to \\(p\\) is equivalent to maximizing the likelihood with respect to \\(\\theta\\). The maximum occurs at \\(p=\\frac{20}{33}\\) and so \\(\\hat{\\theta}=\\frac{-1000}{\\ln(20/33)}= 1996.90\\). Exercise – Exam C Question 152. You are given: A sample of losses is: 600 700 900 No information is available about losses of 500 or less. Losses are assumed to follow an exponential distribution with mean \\(\\theta\\). Calculate the maximum likelihood estimate of \\(\\theta\\). Show Solution Solution: These observations are truncated at 500. The contribution of each observation to the likelihood function is \\[\\frac{f(x)}{1-F(500)} = \\frac{\\theta^{-1}e^{-x/\\theta}}{e^{-500/\\theta}}\\] Then the likelihood function is \\[\\mathcal{L}(\\theta)= \\frac{\\theta^{-1} e^{-600/\\theta} \\theta^{-1} e^{-700/\\theta} \\theta^{-1} e^{-900/\\theta}}{(e^{-500/\\theta})^3} = \\theta^{-3}e^{-700/\\theta}\\] The log-likelihood is \\[L(\\theta) = \\ln\\mathcal{L}(\\theta) = -3\\ln \\theta - 700\\theta^{-1}\\] Maximizing this expression by setting the derivative with respect to \\(\\theta\\) equal to 0, we have \\[L&#39;(\\theta) = -3\\theta^{-1} + 700\\theta^{-2} = 0 \\ \\Rightarrow \\ \\hat{\\theta} = \\frac{700}{3} = 233.33\\] Course 4: Fall 2000, Question 22. You are given the following information about a random sample: The sample size equals five. The sample is from a Weibull distribution with \\(\\tau=2\\). Two of the sample observations are known to exceed 50, and the remaining three observations are 20, 30, and 45. Calculate the maximum likelihood estimate of \\(\\theta\\). Show Solution Solution: The likelihood function is \\[\\begin{aligned} \\mathcal{L}(\\theta) &amp;= f(20) f(30) f(45) [1-F(50)]^2 \\\\ &amp;= \\frac{2(20/\\theta)^2 e^{-(20/\\theta)^2}}{20} \\frac{2(30/\\theta)^2 e^{-(30/\\theta)^2}}{30} \\frac{2(45/\\theta)^2 e^{-(45/\\theta)^2}}{45}(e^{-(50/\\theta)^2})^2 \\\\ &amp;\\propto \\frac{1}{\\theta^6} e^{-8325/\\theta^2} \\end{aligned}\\] The natural logarithm of the above expression is \\(-6\\ln\\theta - \\frac{8325}{\\theta^2}\\). Maximizing this expression by setting its derivative to 0, we get \\[\\frac{-6}{\\theta} + \\frac{16650}{\\theta^3} = 0 \\ \\Rightarrow \\ \\hat{\\theta} = \\left(\\frac{16650}{6}\\right)^{\\frac{1}{2}} = 52.6783\\] 4.3.2 Nonparametric Estimation using Modified Data Nonparametric estimators provide useful benchmarks, so it is helpful to understand the estimation procedures for grouped, censored, and truncated data. 4.3.2.1 Grouped Data As we have seen in Section 4.3.1.1, observations may be grouped (also referred to as interval censored) in the sense that we only observe them as belonging in one of \\(k\\) intervals of the form \\((c_{j-1}, c_j]\\), for \\(j=1, \\ldots, k\\). At the boundaries, the empirical distribution function is defined in the usual way: \\[F_n(c_j) = \\frac{\\text{number of observations } \\le c_j}{n}\\] For other values of \\(x \\in (c_{j-1}, c_j)\\), we can estimate the distribution function with the ogive estimator, which linearly interpolates between \\(F_n(c_{j-1})\\) and \\(F_n(c_j)\\), i.e. the values of the boundaries \\(F_n(c_{j-1}\\) and \\(F_n(c_j)\\) are connected with a straight line. This can formally be expressed as \\[F_n(x) = \\frac{c_j-x}{c_j-c_{j-1}} F_n(c_{j-1}) + \\frac{x-c_{j-1}}{c_j-c_{j-1}} F_n(c_j) \\ \\ \\ \\text{for } c_{j-1} \\le x &lt; c_j\\] The corresponding density is \\[f_n(x) = F^{\\prime}_n(x) = \\frac{F_n(c_j)-F_n(c_{j-1})}{c_j - c_{j-1}} \\ \\ \\ \\text{for } c_{j-1} \\le x &lt; c_j .\\] Exercise – Exam C Question 195. You are given the following information regarding claim sizes for 100 claims: \\[ \\begin{array}{r|c} \\hline \\text{Claim Size} &amp; \\text{Number of Claims} \\\\ \\hline 0 - 1,000 &amp; 16 \\\\ 1,000 - 3,000 &amp; 22 \\\\ 3,000 - 5,000 &amp; 25 \\\\ 5,000 - 10,000 &amp; 18 \\\\ 10,000 - 25,000 &amp; 10 \\\\ 25,000 - 50,000 &amp; 5 \\\\ 50,000 - 100,000 &amp; 3 \\\\ \\text{over } 100,000 &amp; 1 \\\\ \\hline \\end{array}\\] Using the ogive, calculate the estimate of the probability that a randomly chosen claim is between 2000 and 6000. Show Solution Solution: At the boundaries, the empirical distribution function is defined in the usual way, so we have \\[F_{100}(1000) = 0.16, \\ F_{100}(3000)=0.38, \\ F_{100}(5000)=0.63, \\ F_{100}(10000)=0.81\\] For other claim sizes, the ogive estimator linearly interpolates between these values: \\[F_{100}(2000) = 0.5F_{100}(1000) + 0.5F_{100}(3000) = 0.5(0.16)+0.5(0.38)=0.27\\] \\[F_{100}(6000)=0.8F_{100}(5000)+0.2F_{100}(10000) = 0.8(0.63)+0.2(0.81)=0.666\\] Thus, the probability that a claim is between 2000 and 6000 is \\(F_{100}(6000) - F_{100}(2000) = 0.666-0.27 = 0.396\\). 4.3.2.2 Right-Censored Empirical Distribution Function It can be useful to calibrate parametric likelihood methods with nonparametric methods that do not rely on a parametric form of the distribution. The product-limit estimator due to (Kaplan and Meier 1958) is a well-known estimator of the distribution in the presence of censoring. To begin, first note that the empirical distribution function \\(F_n(x)\\) is an unbiased estimator of the distribution function \\(F(x)\\) (in the ``usual’’ case in the absence of censoring). This is because \\(F_n(x)\\) is the average of indicator variables that are also unbiased, that is, \\(\\mathrm{E~} I(X \\le x) = \\Pr(X \\le x) = F(x)\\). Now suppose the the random outcome is censored on the right by a limiting amount, say, \\(C_U\\), so that we record the smaller of the two, \\(X^* = \\min(X, C_U)\\). For values of \\(x\\) that are smaller than \\(C_U\\), the indicator variable still provides an unbiased estimator of the distribution function before we reach the censoring limit. That is, \\(\\mathrm{E~} I(X^* \\le x) = F(x)\\) because \\(I(X^* \\le x) = I(X \\le x)\\) for \\(x &lt; C_U\\). In the same way, \\(\\mathrm{E~} I(X^* &gt; x) = 1 -F(x) = S(x)\\). Now consider two random variables that have different censoring limits. For illustration, suppose that we observe \\(X_1^* = \\min(X_1, 5)\\) and \\(X_2^* = \\min(X_2, 10)\\) where \\(X_1\\) and \\(X_2\\) are independent draws from the same distribution. For \\(x \\le 5\\), the empirical distribution function \\(F_2(x)\\) is an unbiased estimator of \\(F(x)\\). However, for \\(5 &lt; x \\le 10\\), the first observation cannot be used for the distribution function because of the censoring limitation. Instead, the strategy developed by (Kaplan and Meier 1958) is to use \\(S_n(5)\\) as an estimator of \\(S(5)\\) and then to use the second observation to estimate the conditional survivor function \\(\\Pr(X &gt; x | X &gt;5) = \\frac{S(x)}{S(5)}\\). Specifically, for \\(5 &lt; x \\le 10\\), the estimator of the survival function is \\[ \\hat{S}(x) = S_2(5) \\times I(X_2^* &gt; x ) . \\] Extending this idea, for each observation \\(i\\), let \\(u_i\\) be the upper censoring limit (\\(=\\infty\\) if no censoring). Thus, the recorded value is \\(x_i\\) in the case of no censoring and \\(u_i\\) if there is censoring. Let \\(t_{1} &lt;\\cdots&lt; t_{k}\\) be \\(k\\) distinct points at which an uncensored loss occurs, and let \\(s_j\\) be the number of uncensored losses \\(x_i\\)’s at \\(t_{j}\\). The corresponding risk set is the number of observations that are active (not censored) at a value less than \\(t_{j}\\), denoted as \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j})\\). With this notation, the product-limit estimator of the distribution function is \\[\\begin{equation} \\hat{F}(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1-\\prod_{j:t_{j} \\leq x}\\left( 1-\\frac{s_j}{R_{j}}\\right) &amp; x \\geq t_{1} . \\end{array} \\right. \\tag{4.5} \\end{equation}\\] As usual, the corresponding estimate of the survival function is \\(\\hat{S}(x) = 1 - \\hat{F}(x)\\). Exercise – Exam C Question 252. The following is a sample of 10 payments: \\[\\begin{array}{cccccccccc} 4 &amp;4 &amp;5+ &amp;5+ &amp;5+ &amp;8 &amp;10+ &amp;10+ &amp;12 &amp;15 \\\\ \\end{array}\\] where \\(+\\) indicates that a loss has exceeded the policy limit. Using the Kaplan-Meier product-limit estimator, calculate the probability that the loss on a policy exceeds 11, \\(\\hat{S}(11)\\). Show Solution Solution: There are four event times (non-censored observations). For each time \\(t_j\\), we can calcuate the number of events \\(s_j\\) and the risk set \\(R_j\\) as the following: \\[\\begin{array}{cccc} \\hline j &amp; t_j &amp; s_j &amp; R_j \\\\ \\hline 1 &amp; 4 &amp; 2 &amp; 10 \\\\ 2 &amp; 8 &amp; 1 &amp; 5 \\\\ 3 &amp; 12 &amp; 1 &amp; 2 \\\\ 4 &amp; 15 &amp; 1 &amp; 1 \\\\ \\hline \\end{array}\\] Thus, the Kaplan-Meier estimate of \\(S(11)\\) is \\[\\begin{aligned} \\hat{S}(11) &amp;= \\prod_{j:t_j\\leq 11} \\left( 1- \\frac{s_j}{R_j} \\right) = \\prod_{j=1}^{2} \\left( 1- \\frac{s_j}{R_j} \\right)\\\\ &amp;= \\left(1-\\frac{2}{10} \\right) \\left(1-\\frac{1}{5} \\right) = (0.8)(0.8)= 0.64. \\\\ \\end{aligned}\\] 4.3.2.3 Right-Censored, Left-Truncated Empirical Distribution Function In addition to right-censoring, we now extend the framework to allow for left-truncated data. As before, for each observation \\(i\\), let \\(u_i\\) be the upper censoring limit (\\(=\\infty\\) if no censoring). Further, let \\(d_i\\) be the lower truncation limit (0 if no truncation). Thus, the recorded value (if it is greater than \\(d_i\\)) is \\(x_i\\) in the case of no censoring and \\(u_i\\) if there is censoring. Let \\(t_{1} &lt;\\cdots&lt; t_{k}\\) be \\(k\\) distinct points at which an event of interest occurs, and let \\(s_j\\) be the number of recorded events \\(x_i\\)’s at time point \\(t_{j}\\). The corresponding risk set is \\(R_j = \\sum_{i=1}^n I(x_i \\geq t_{j}) + \\sum_{i=1}^n I(u_i \\geq t_{j}) - \\sum_{i=1}^n I(d_i \\geq t_{j})\\). With this new definition of the risk set, the product-limit estimator of the distribution function is as in equation (4.5). (Greenwood 1926) derived the formula for the estimated variance of the product-limit estimator to be \\[\\widehat{Var}(\\hat{F}(x)) = (1-\\hat{F}(x))^{2} \\sum _{j:t_{j} \\leq x} \\dfrac{s_j}{R_{j}(R_{j}-s_j)}.\\] R‘s survfit method takes a survival data object and creates a new object containing the Kaplan-Meier estimate of the survival function along with confidence intervals. The Kaplan-Meier method (type='kaplan-meier') is used by default to construct an estimate of the survival curve. The resulting discrete survival function has point masses at the observed event times (discharge dates) \\(t_j\\), where the probability of an event given survival to that duration is estimated as the number of observed events at the duration \\(s_j\\) divided by the number of subjects exposed or ’at-risk’ just prior to the event duration \\(R_j\\). Two alternate types of estimation are also available for the survfit method. The alternative (type='fh2') handles ties, in essence, by assuming that multiple events at the same duration occur in some arbitrary order. Another alternative (type='fleming-harrington') uses the Nelson-Äalen (see (Aalen 1978)) estimate of the cumulative hazard function to obtain an estimate of the survival function. The estimated cumulative hazard \\(\\hat{H}(x)\\) starts at zero and is incremented at each observed event duration \\(t_j\\) by the number of events \\(s_j\\) divided by the number at risk \\(R_j\\). With the same notation as above, the Nelson-Äalen estimator of the distribution function is \\[\\begin{aligned} \\hat{F}_{NA}(x) &amp;= \\left\\{ \\begin{array}{ll} 0 &amp; x&lt;t_{1} \\\\ 1- \\exp \\left(-\\sum_{j:t_{j} \\leq x}\\frac{s_j}{R_j} \\right) &amp; x \\geq t_{1} . \\end{array} \\right.\\end{aligned}\\] Note that the above expression is a result of the Nelson-Äalen estimator of the cumulative hazard function \\[\\hat{H}(x)=\\sum_{j:t_j\\leq x} \\frac{s_j}{R_j} \\] and the relationship between the survival function and cumulative hazard function, \\(\\hat{S}_{NA}(x)=e^{-\\hat{H}(x)}\\). Exercise – Exam C Question 135. For observation \\(i\\) of a survival study: \\(d_i\\) is the left truncation point \\(x_i\\) is the observed value if not right censored \\(u_i\\) is the observed value if right censored You are given: \\[\\begin{array}{c|cccccccccc} \\hline \\text{Observation } (i) &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10\\\\ \\hline d_i &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.3 &amp; 1.5 &amp; 1.6\\\\ x_i &amp; 0.9 &amp; - &amp; 1.5 &amp; - &amp; - &amp; 1.7 &amp; - &amp; 2.1 &amp; 2.1 &amp; - \\\\ u_i &amp; - &amp; 1.2 &amp; - &amp; 1.5 &amp; 1.6 &amp; - &amp; 1.7 &amp; - &amp; - &amp; 2.3 \\\\ \\hline \\end{array}\\] Calculate the Kaplan-Meier product-limit estimate, \\(\\hat{S}(1.6)\\) Show Solution Solution: Recall the risk set \\(R_j = \\sum_{i=1}^n \\left\\{ I(x_i \\geq t_{j}) + I(u_i \\geq t_{j}) - I(d_i \\geq t_{j}) \\right\\}\\). Then \\[\\begin{array}{ccccc} \\hline j &amp; t_j &amp; s_j &amp; R_j &amp; \\hat{S}(t_j) \\\\ \\hline 1 &amp; 0.9 &amp; 1 &amp; 10-3 = 7 &amp; 1-\\frac{1}{7} = \\frac{6}{7} \\\\ 2 &amp; 1.5 &amp; 1 &amp; 8-2 = 6 &amp; \\frac{6}{7}\\left( 1 - \\frac{1}{6} \\right) = \\frac{5}{7}\\\\ 3 &amp; 1.7 &amp; 1 &amp; 5-0 = 5 &amp; \\frac{5}{7}\\left( 1 - \\frac{1}{5} \\right) = \\frac{4}{7}\\\\ 4 &amp; 2.1 &amp; 2 &amp; 3 &amp; \\frac{4}{7}\\left( 1 - \\frac{2}{3}\\right) = \\frac{4}{21}\\\\ \\hline \\end{array}\\] The Kaplan-Meier estimate is therefore \\(\\hat{S}(1.6) = \\frac{5}{7}\\). Exercise – Exam C Question 252. - Continued. Using the Nelson-Äalen estimator, calculate the probability that the loss on a policy exceeds 11, \\(\\hat{S}_{NA}(11)\\). Calculate Greenwood’s approximation to the variance of the product-limit estimate \\(\\hat{S}(11)\\). Show Solution Solution: As before, there are four event times (non-censored observations). For each time \\(t_j\\), we can calcuate the number of events \\(s_j\\) and the risk set \\(R_j\\) as the following: \\[\\begin{array}{cccc} \\hline j &amp; t_j &amp; s_j &amp; R_j \\\\ \\hline 1 &amp; 4 &amp; 2 &amp; 10 \\\\ 2 &amp; 8 &amp; 1 &amp; 5 \\\\ 3 &amp; 12 &amp; 1 &amp; 2 \\\\ 4 &amp; 15 &amp; 1 &amp; 1 \\\\ \\hline \\end{array}\\] The Nelson-Äalen estimate of \\(S(11)\\) is \\(\\hat{S}_{NA}(11)=e^{-\\hat{H}(11)} = e^{-0.4} = 0.67\\), since \\[\\begin{aligned} \\hat{H}(11) &amp;= \\sum_{j:t_j\\leq 11} \\frac{s_j}{R_j} = \\sum_{j=1}^{2} \\frac{s_j}{R_j} \\\\ &amp;= \\frac{2}{10} + \\frac{1}{5} = 0.2 + 0.2 = 0.4 .\\\\ \\end{aligned}\\] From earlier work, the Kaplan-Meier estimate of \\(S(11)\\) is \\(\\hat{S}(11) = 0.64\\). Then Greenwood’s estimate of the variance of the product-limit estimate of \\(S(11)\\) is \\[\\begin{aligned} \\widehat{Var}(\\hat{S}(11)) &amp;= (\\hat{S}(11))^2 \\sum_{j:t_j\\leq 11} \\frac{s_j}{R_j(R_j-s_j)} &amp;= (0.64)^2 \\left(\\frac{2}{10(8)} + \\frac{1}{5(4)} \\right) = 0.0307. \\\\ \\end{aligned}\\] 4.4 Bayesian Inference In this section, you learn how to: Describe the Bayes model as an alternative to the frequentist approach and summarize the five components of this modeling approach Describe the Bayesian decision framework and its role in determining Bayesian predictions Determine posterior predictions Up to this point, our inferential methods have focused on the frequentist setting, in which samples are repeatedly drawn from a population. The vector of parameters \\(\\boldsymbol \\theta\\) is fixed yet unknown, whereas the outcomes \\(X\\) are realizations of random variables. In contrast, under the Bayesian framework, we view both the model parameters and the data as random variables. We are uncertain about the parameters \\(\\boldsymbol \\theta\\) and use probability tools to reflect this uncertainty. There are several advantages of the Bayesian approach. First, we can describe the entire distribution of parameters conditional on the data. This allows us, for example, to provide probability statements regarding the likelihood of parameters. Second, this approach allows analysts to blend prior information known from other sources with the data in a coherent manner. This topic is developed in detail in the credibility chapter. Third, the Bayesian approach provides a unified approach for estimating parameters. Some non-Bayesian methods, such as least squares, require a separate approach to estimate variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. This is convenient for explaining results to consumers of the data analysis. Fourth, Bayesian analysis is particularly useful for forecasting future responses. 4.4.1 Bayesian Model As stated earlier, under the Bayesian perspective, the model parameters and data are both viewed as random. Our uncertainty about the parameters of the underlying data generating process is reflected in the use of probability tools. Prior Distribution. Specifically, think about \\(\\boldsymbol \\theta\\) as a random vector and let \\(\\pi(\\boldsymbol \\theta)\\) denote the distribution of possible outcomes. This is knowledge that we have before outcomes are observed and is called the prior distribution. Typically, the prior distribution is a regular distribution and so integrates or sums to one, depending on whether \\(\\boldsymbol \\theta\\) is continuous or discrete. However, we may be very uncertain (or have no clue) about the distribution of \\(\\boldsymbol \\theta\\); the Bayesian machinery allows the following situation \\[\\int \\pi(\\theta) d\\theta = \\infty,\\] in which case \\(\\pi(\\cdot)\\) is called an improper prior. Model Distribution. The distribution of outcomes given an assumed value of \\(\\boldsymbol \\theta\\) is known as the model distribution and denoted as \\(f(x | \\boldsymbol \\theta) = f_{X|\\boldsymbol \\theta} (x|\\boldsymbol \\theta )\\). This is the usual frequentist mass or density function. Joint Distribution. The distribution of outcomes and model parameters is, unsurprisingly, known as the joint distribution and denoted as \\(f(x , \\boldsymbol \\theta) = f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)\\). Marginal Outcome Distribution. The distribution of outcomes can be expressed as \\[f(x) = f(x | \\boldsymbol \\theta)\\pi(\\boldsymbol \\theta) d\\boldsymbol \\theta.\\] This is analogous to a frequentist mixture distribution. Posterior Distribution of Parameters. After outcomes have been observed (hence the terminology “posterior”), one can use Bayes theorem to write the distribution as \\[\\pi(\\boldsymbol \\theta | x) =\\frac{f(x , \\boldsymbol \\theta)}{f(x)} =\\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)}\\] The idea is to update your knowledge of the distribution of \\(\\boldsymbol \\theta\\) (\\(\\pi(\\boldsymbol \\theta)\\)) with the data \\(x\\). We can summarize the distribution using a confidence interval type statement. Definition. \\([a,b]\\) is said to be a \\(100(1-\\alpha)\\%\\) credibility interval for \\(\\boldsymbol \\theta\\) if \\[\\Pr (a \\le \\theta \\le b | \\mathbf{x}) \\ge 1- \\alpha.\\] Exercise – Exam C Question 157. You are given: In a portfolio of risks, each policyholder can have at most one claim per year. The probability of a claim for a policyholder during a year is \\(q\\). The prior density is \\[\\pi(q) = q^3/0.07, \\ \\ \\ 0.6 &lt; q &lt; 0.8\\] A randomly selected policyholder has one claim in Year 1 and zero claims in Year 2. For this policyholder, calculate the posterior probability that \\(0.7 &lt; q &lt; 0.8\\). Show Solution Solution: The posterior density is proportional to the product of the likelihood function and prior density. Thus, \\[\\pi(q|1,0) \\propto f(1|q)\\ f(0|q)\\ \\pi(q) \\propto q(1-q)q^3 = q^4-q^5\\] To get the exact posterior density, we integrate the above function over its range \\((0.6, 0.8)\\) \\[\\int_{0.6}^{0.8} q^4-q^5 dq = \\frac{q^5}{5} - \\left. \\frac{q^6}{6} \\right|_{0.6}^{0.8} = 0.014069 \\ \\Rightarrow \\ \\pi(q|1,0)=\\frac{q^4-q^5}{0.014069}\\] Then \\[P(0.7&lt;q&lt;0.8|1,0)= \\int_{0.7}^{0.8} \\frac{q^4-q^5}{0.014069}dq = 0.5572\\] Exercise – Exam C Question 43. You are given: The prior distribution of the parameter \\(\\Theta\\) has probability density function: \\[\\pi(\\theta) = \\frac{1}{\\theta^2}, \\ \\ 1 &lt; \\theta &lt; \\infty\\] Given \\(\\Theta = \\theta\\), claim sizes follow a Pareto distribution with parameters \\(\\alpha=2\\) and \\(\\theta\\). A claim of 3 is observed. Calculate the posterior probability that \\(\\Theta\\) exceeds 2. Show Solution Solution: The posterior density, given an observation of 3 is \\[\\pi(\\theta|3) = \\frac{f(3|\\theta)\\pi(\\theta)}{\\int_1^\\infty f(3|\\theta)\\pi(\\theta)d\\theta} = \\frac{\\frac{2\\theta^2}{(3+\\theta)^3}\\frac{1}{\\theta^2}}{\\int_1^\\infty 2(3+\\theta)^{-3} d\\theta} = \\frac{2(3+\\theta)^{-3}}{\\left. -(3+\\theta)^{-2}\\right|_1^\\infty} = 32(3+\\theta)^{-3}, \\ \\ \\theta &gt; 1\\] Then \\[P(\\Theta&gt;2|3) = \\int_2^\\infty 32(3+\\theta)^{-3}d\\theta = \\left. -16(3+\\theta)^{-2} \\right|_2^\\infty = \\frac{16}{25} = 0.64\\] 4.4.2 Decision Analysis In classical decision analysis, the loss function \\(l(\\hat{\\theta}, \\theta)\\) determines the penalty paid for using the estimate \\(\\hat{\\theta}\\) instead of the true \\(\\theta\\). The Bayes estimate is that value that minimizes the expected loss \\(\\mathrm{E~}[ l(\\hat{\\theta}, \\theta)]\\). Some important special cases include: \\[\\begin{array}{cll} \\hline \\text{Loss function } l(\\hat{\\theta}, \\theta) &amp; \\text{Descriptor} &amp; \\text{Bayes Estimate} \\\\ \\hline (\\hat{\\theta}- \\theta)^2 &amp; \\text{squared error loss} &amp; \\mathrm{E}(\\theta|X) \\\\ |\\hat{\\theta}- \\theta| &amp; \\text{absolute deviation loss} &amp; \\text{median of } \\pi(\\theta|x) \\\\ I(\\hat{\\theta} =\\theta) &amp; \\text{zero-one loss (for discrete probabilities)} &amp; \\text{mode of } \\pi(\\theta|x) \\\\ \\hline \\end{array}\\] For new data \\(y\\), the predictive distribution is \\[f(y|x) = \\int f(y|\\theta) \\pi(\\theta|x) d\\theta .\\] With this, the Bayesian prediction of \\(y\\) is \\[\\begin{aligned} \\mathrm{E}(y|x) &amp;= \\int y f(y|x) dy = \\int y \\left(\\int f(y|\\theta) \\pi(\\theta|x) d\\theta \\right) dy \\\\ &amp;= \\int \\mathrm{E}(y|\\theta) \\pi(\\theta|x) d\\theta . \\end{aligned}\\] Exercise – Exam C Question 190. For a particular policy, the conditional probability of the annual number of claims given \\(\\Theta = \\theta\\), and the probability distribution of \\(\\Theta\\) are as follows: \\[\\begin{array}{l|ccc} \\hline \\text{Number of Claims} &amp; 0 &amp; 1 &amp; 2 \\\\ \\text{Probability} &amp; 2\\theta &amp; \\theta &amp; 1-3\\theta \\\\ \\hline \\end{array}\\] \\[\\begin{array}{l|cc} \\hline \\theta &amp; 0.05 &amp; 0.30 \\\\ \\text{Probability} &amp; 0.80 &amp; 0.20 \\\\ \\hline \\end{array}\\] Two claims are observed in Year 1. Calculate the Bayesian estimate (Bühlmann credibility estimate) of the number of claims in Year 2. Show Solution Solution: Note that \\(E(\\theta) = 0.05(0.8) + 0.3(0.2) = 0.1\\) and \\(E(\\theta^2) = 0.05^2(0.8) + 0.3^2(0.2)=0.02\\) We also have \\(\\mu(\\theta) = 0(2\\theta) + 1(\\theta) + 2(1-3\\theta) = 2-5\\theta\\) and \\(v(\\theta) = 0^2(2\\theta) + 1^2(\\theta) + 2^2(1-3\\theta) - (2-5\\theta)^2 = 9\\theta-25\\theta^2\\). Thus \\[\\begin{aligned} \\mu &amp;= E(2-5\\theta) = 2-5(0.1) = 1.5 \\\\ v &amp;= EVPV = E(9\\theta - 25\\theta^2)=9(0.1)-25(0.02)=0.4 \\\\ a &amp;= VHM = Var(2-5\\theta) = 25Var(\\theta) = 25(0.02-0.1^2) = 0.25 \\\\ \\Rightarrow k &amp;= \\frac{v}{a} = \\frac{0.4}{0.25} = 1.6 \\\\ \\Rightarrow Z &amp;= \\frac{1}{1+1.6} = \\frac{5}{13} \\end{aligned}\\] Therefore, \\(P=\\frac{5}{13}2 + \\frac{8}{13}1.5 = 1.6923\\). Exercise – Exam C Question 11. You are given: Losses on a company’s insurance policies follow a Pareto distribution with probability density function: \\[f(x|\\theta) = \\frac{\\theta}{(x+\\theta)^2}, \\ \\ 0 &lt; x &lt; \\infty\\] For half of the company’s policies \\(\\theta=1\\) , while for the other half \\(\\theta=3\\). For a randomly selected policy, losses in Year 1 were 5. Calculate the posterior probability that losses for this policy in Year 2 will exceed 8. Show Solution Solution: We are given the prior distribution of \\(\\theta\\) as \\(P(\\theta=1)=P(\\theta=3)=\\frac{1}{2}\\), the conditional distribution \\(f(x|\\theta)\\), and the fact that we observed \\(X_1=5\\). The goal is to find the predictive probability \\(P(X_2&gt;8|X_1=5)\\). The posterior probabilities are \\[\\begin{aligned} P(\\theta=1|X_1=5) &amp;= \\frac{f(5|\\theta=1)P(\\theta=1)}{f(5|\\theta=1)P(\\theta=1) + f(5|\\theta=3)P(\\theta=3)} \\\\ &amp;= \\frac{\\frac{1}{36}(\\frac{1}{2})}{\\frac{1}{36}(\\frac{1}{2})+\\frac{3}{64}(\\frac{1}{2})} = \\frac{\\frac{1}{72}}{\\frac{1}{72} + \\frac{3}{128}} = \\frac{16}{43} \\end{aligned}\\] \\[\\begin{aligned} P(\\theta=3|X_1=5) &amp;= \\frac{f(5|\\theta=3)P(\\theta=3)}{f(5|\\theta=1)P(\\theta=1) + f(5|\\theta=3)P(\\theta=3)} \\\\ &amp;= 1-P(\\theta=1|X_1=5) = \\frac{27}{43} \\end{aligned}\\] Note that the conditional probability that losses exceed 8 is \\[\\begin{aligned} P(X_2&gt;8|\\theta) &amp;= \\int_8^\\infty f(x|\\theta)dx \\\\ &amp;= \\int_8^\\infty \\frac{\\theta}{(x+\\theta)^2}dx = \\left. -\\frac{\\theta}{x+\\theta} \\right|_8^\\infty = \\frac{\\theta}{8 + \\theta} \\end{aligned}\\] The predictive probability is therefore \\[\\begin{aligned} P(X_2&gt;8|X_1=5) &amp;= P(X_2&gt;8|\\theta=1) P(\\theta=1|X_1=5) + P(X_2&gt;8|\\theta=3) P(\\theta=3 | X_1=5) \\\\ &amp;= \\frac{1}{8+1}\\left( \\frac{16}{43}\\right) + \\frac{3}{8+3} \\left( \\frac{27}{43}\\right) = 0.2126 \\end{aligned}\\] Exercise – Exam C Question 15. You are given: The probability that an insured will have at least one loss during any year is \\(p\\). The prior distribution for \\(p\\) is uniform on \\([0, 0.5]\\). An insured is observed for 8 years and has at least one loss every year. Calculate the posterior probability that the insured will have at least one loss during Year 9. Show Solution Solution: The posterior probability density is \\[\\begin{aligned} \\pi(p|1,1,1,1,1,1,1,1) &amp;\\propto Pr(1,1,1,1,1,1,1,1|p)\\ \\pi(p) = p^8(2) \\propto p^8 \\\\ \\Rightarrow \\pi(p|1,1,1,1,1,1,1,1) &amp;= \\frac{p^8}{\\int_0^5 p^8 dp} = \\frac{p^8}{(0.5^9)/9} = 9(0.5^{-9})p^8 \\end{aligned}\\] Thus, the posterior probability that the insured will have at least one loss during Year 9 is \\[\\begin{aligned} P(X_9=1|1,1,1,1,1,1,1,1) &amp;= \\int_0^5 P(X_9=1|p) \\pi(p|1,1,1,1,1,1,1,1) dp \\\\ &amp;= \\int_0^5 p(9)(0.5^{-9})p^8 dp = 9(0.5^{-9})(0.5^{10})/10 = 0.45 \\end{aligned}\\] Exercise – Exam C Question 29. You are given: Each risk has at most one claim each year. \\[\\begin{array}{ccc} \\hline \\text{Type of Risk} &amp; \\text{Prior Probability} &amp; \\text{Annual Claim Probability} \\\\ \\hline \\text{I} &amp; 0.7 &amp; 0.1 \\\\ \\text{II} &amp; 0.2 &amp; 0.2 \\\\ \\text{III} &amp; 0.1 &amp; 0.4 \\\\ \\hline \\end{array}\\] One randomly chosen risk has three claims during Years 1-6. Calculate the posterior probability of a claim for this risk in Year 7. Show Solution Solution: The probabilities are from a binomial distribution with 6 trials in which 3 successes were observed. \\[\\begin{aligned} P(3|\\text{I}) &amp;= {6 \\choose 3} (0.1^3)(0.9^3) = 0.01458 \\\\ P(3|\\text{II}) &amp;= {6 \\choose 3} (0.2^3)(0.8^3) = 0.08192 \\\\ P(3|\\text{III}) &amp;= {6 \\choose 3} (0.4^3)(0.6^3) = 0.27648 \\end{aligned}\\] The probability of observing three successes is \\[\\begin{aligned} P(3) &amp;= P(3|\\text{I})P(\\text{I}) + P(3|\\text{II})P(\\text{II}) + P(3|\\text{III})P(\\text{III}) \\\\ &amp;= 0.7(0.01458) + 0.2(0.08192) + 0.1(0.27648) = 0.054238 \\end{aligned}\\] The three posterior probabilities are \\[\\begin{aligned} P(\\text{I}|3) &amp;= \\frac{P(3|\\text{I})P(\\text{I})}{P(3)} = \\frac{0.7(0.01458)}{0.054238} = 0.18817 \\\\ P(\\text{II}|3) &amp;= \\frac{P(3|\\text{II})P(\\text{II})}{P(3)} = \\frac{0.2(0.08192)}{0.054238} = 0.30208 \\\\ P(\\text{III}|3) &amp;= \\frac{P(3|\\text{III})P(\\text{III})}{P(3)} = \\frac{0.1(0.27648)}{0.054238} = 0.50975 \\end{aligned}\\] The posterior probability of a claim is then \\[\\begin{aligned} P(\\text{claim} | 3) &amp;= P(\\text{claim}|\\text{I})P(\\text{I} | 3) + P(\\text{claim} | \\text{II})P(\\text{II} | 3) + P(\\text{claim} | \\text{III}) P(\\text{III} | 3) \\\\ &amp;= 0.1(0.18817) + 0.2(0.30208) + 0.4(0.50975) = 0.28313 \\end{aligned}\\] 4.4.3 Posterior Distribution How can we calculate the posterior distribution \\(\\pi(\\boldsymbol \\theta | x) =\\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)}\\)? By hand: we can do this in special cases Simulation: use modern computational techniques such as Markov Chain Monte Carlo (MCMC) simulation Normal approximation: !!! Theorem 12.39 of KPW provides a justification Conjugate distributions: classical approach. Although this approach is available only for a limited number of distributions, it has the appeal that it provides closed-form expressions for the distributions, allowing for easy interpretations of results. We focus on this approach. To relate the prior and posterior distributions of the parameters, we have \\[\\begin{array}{ccc} \\pi(\\boldsymbol \\theta | x) &amp; = &amp; \\frac{f(x|\\boldsymbol \\theta )\\pi(\\boldsymbol \\theta)}{f(x)} \\\\ &amp; \\propto &amp; f(x|\\boldsymbol \\theta ) \\pi(\\boldsymbol \\theta) \\\\ \\text{Posterior} &amp; \\text{is proportional to} &amp; \\text{likelihood} \\times \\text{prior} \\\\ \\end{array}\\] For conjugate distributions, the posterior and the prior come from the same family of distributions. Show Example Example – Poisson-Gamma Assume a Poisson(\\(\\lambda\\)) model distribution so that \\[f(\\mathbf{x} | \\lambda) = \\prod_{i=1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\\] Assume \\(\\lambda\\) follows a gamma(\\(\\alpha, \\theta\\)) prior distribution so that \\[\\pi(\\lambda) = \\frac{\\left(\\lambda/\\theta\\right)^{\\alpha} \\exp(-\\lambda/\\theta)}{\\lambda \\Gamma(\\alpha)}.\\] The posterior distribution is proportional to \\[\\begin{aligned} \\pi(\\lambda | \\mathbf{x}) &amp;\\propto f(\\mathbf{x}|\\theta ) \\pi(\\lambda) \\\\ &amp;= C \\lambda^{\\sum_i x_i + \\alpha +1} \\exp(-\\lambda(n+1/\\theta)) \\end{aligned}\\] where \\(C\\) is a constant. We recognize this to be a gamma distribution with new parameters \\(\\alpha_{new} = \\sum_i x_i + \\alpha\\) and \\(\\theta_{new} = 1/(n + 1/\\theta)\\). Thus, the gamma distribution is a conjugate prior for the Poisson model distribution. Exercise – Exam C Question 215. You are given: The conditional distribution of the number of claims per policyholder is Poisson with mean \\(\\lambda\\). The variable \\(\\lambda\\) has a gamma distribution with parameters \\(\\alpha\\) and \\(\\theta\\). For policyholders with 1 claim in Year 1, the credibility estimate for the number of claims in Year 2 is 0.15. For policyholders with an average of 2 claims per year in Year 1 and Year 2, the credibility estimate for the number of claims in Year 3 is 0.20. Calculate \\(\\theta\\). Show Solution Solution: Since the conditional distribution of the number of claims per policyholder, \\(E(X|\\lambda)=Var(X|\\lambda)=\\lambda\\) Thus, \\[\\begin{aligned} \\mu &amp;= v = E(\\lambda) = \\alpha\\theta \\\\ a &amp;= Var(\\lambda) = \\alpha\\theta^2 \\\\ k &amp;= \\frac{v}{a} = \\frac{1}{\\theta} \\\\ \\Rightarrow Z &amp;= \\frac{n}{n+1/\\theta} = \\frac{n\\theta}{n\\theta+1} \\end{aligned}\\] Using the credibility estimates given, \\[\\begin{aligned} 0.15 &amp;= \\frac{\\theta}{\\theta + 1}(1) + \\frac{1}{\\theta + 1}\\mu = \\frac{\\theta + \\mu}{\\theta + 1} \\\\ 0.20 &amp;= \\frac{2\\theta}{2\\theta+1}(2) + \\frac{1}{2\\theta+1}\\mu = \\frac{4\\theta+\\mu}{2\\theta+1} \\end{aligned}\\] From the first equation, \\(0.15\\theta + 0.15 = \\theta + \\mu \\ \\Rightarrow \\ \\mu = 0.15- 0.85\\theta\\). Then the second equation becomes \\(0.4\\theta + 0.2 = 4\\theta + 0.15 - 0.85\\theta \\ \\Rightarrow \\ \\theta=0.01818\\) 4.5 Exercises Here are a set of exercises that guide the viewer through some of the theoretical foundations of Loss Data Analytics. Each tutorial is based on one or more questions from the professional actuarial examinations, typically the Society of Actuaries Exam C. Model Selection Guided Tutorials Contributors Lisa Gao and Edward W. (Jed) Frees, University of Wisconsin-Madison, are the principal authors of the initital version of this chapter. Email: jfrees@bus.wisc.edu for chapter comments and suggested improvements. Technical Supplement A. Gini Statistic TS A.1. The Classic Lorenz Curve In welfare economics, it is common to compare distributions via the Lorenz curve, developed by Max Otto Lorenz (Lorenz 1905). A Lorenz curve is a graph of the proportion of a population on the horizontal axis and a distribution function of interest on the vertical axis. It is typically used to represent income distributions. When the income distribution is perfectly aligned with the population distribution, the Lorenz curve results in a 45 degree line that is known as the line of equality. The area between the Lorenz curve and the line of equality is a measure of the discrepancy between the income and population distributions. Two times this area is known as the Gini index, introduced by Corrado Gini in 1912. Example – Classic Lorenz Curve. For an insurance example, Figure 4.12 shows a distribution of insurance losses. This figure is based on a random sample of 2000 losses. The left-hand panel shows a right-skewed histogram of losses. The right-hand panel provides the corresponding Lorenz curve, showing again a skewed distribution. For example, the arrow marks the point where 60 percent of the policyholders have 30 percent of losses. The 45 degree line is the line of equality; if each policyholder has the same loss, then the loss distribution would be at this line. The Gini index, twice the area between the Lorenz curve and the 45 degree line, is 37.6 percent for this data set. Figure 4.12: Distribution of insurance losses. TS A.2. Ordered Lorenz Curve and the Gini Index We now introduce a modification of the classic Lorenz curve and Gini statistic that is useful in insurance applications. Specifically, we introduce an ordered Lorenz curve which is a graph of the distribution of losses versus premiums, where both losses and premiums are ordered by relativities. Intuitively, the relativities point towards aspects of the comparison where there is a mismatch between losses and premiums. To make the ideas concrete, we first provide some notation. We will consider \\(i=1, \\ldots, n\\) policies. For the \\(i\\)th policy, let \\(y_i\\) denote the insurance loss, \\(\\mathbf{x}_i\\) be the set of policyholder characteristics known to the analyst, \\(P_i=P(\\mathbf{x}_i)\\) be the associated premium that is a function of \\(\\mathbf{x}_i\\), \\(S_i = S(\\mathbf{x}_i)\\) be an insurance score under consideration for rate changes, and \\(R_i = R(\\mathbf{x}_i)=S(\\mathbf{x}_i)/P(\\mathbf{x}_i)\\) is the relativity, or relative premium. Thus, the set of information used to calculate the ordered Lorenz curve for the \\(i\\)th policy is \\((y_i, P_i, S_i, R_i)\\). Ordered Lorenz Curve We now sort the set of policies based on relativities (from smallest to largest) and compute the premium and loss distributions. Using notation, the premium distribution is \\[\\begin{equation} \\hat{F}_P(s) = \\frac{ \\sum_{i=1}^n P(\\mathbf{x}_i) \\mathrm{I}(R_i \\leq s) }{\\sum_{i=1}^n P(\\mathbf{x}_i)} ,\\tag{4.6} \\end{equation}\\] and the loss distribution is \\[\\begin{equation} \\hat{F}_{L}(s) = \\frac{ \\sum_{i=1}^n y_i \\mathrm{I}(R_i \\leq s) }{\\sum_{i=1}^n y_i} ,\\tag{4.7} \\end{equation}\\] where \\(\\mathrm{I}(\\cdot)\\) is the indicator function, returning a 1 if the event is true and zero otherwise. The graph \\(\\left(\\hat{F}_P(s),\\hat{F}_{L}(s) \\right)\\) is an ordered Lorenz curve. The classic Lorenz curve shows the proportion of policyholders on the horizontal axis and the loss distribution function on the vertical axis. The ordered Lorenz curve extends the classical Lorenz curve in two ways, (1) through the ordering of risks and prices by relativities and (2) by allowing prices to vary by observation. We summarize the ordered Lorenz curve in the same way as the classic Lorenz curve using a Gini index, defined as twice the area between the curve and a 45 degree line. The analyst seeks ordered Lorenz curves that approach passing through the southeast corner (1,0); these have greater separation between the loss and premium distributions and therefore larger Gini indices. Example – Loss Distribution. Suppose we have \\(n=5\\) policyholders with experience as: Variable \\(i\\) 1 2 3 4 5 Sum Loss \\(y_i\\) 5 5 5 4 6 25 Premium \\(P(\\mathbf{x}_i)\\) 4 2 6 5 8 25 Relativity \\(R(\\mathbf{x}_i)\\) 5 4 3 2 1 Determine the Lorenz curve and the ordered Lorenz curve. Show Example Solution Figure 4.13: Lorenz versus Ordered Lorenz Curve Figure 4.13 compares the Lorenz curve to the ordered version based on this data. The left-hand panel shows the Lorenz curve. The horizontal axis is the cumulative proportion of policyholders (0, 0.2, 0.4, and so forth) and the vertical axis is the cumulative proportion of losses (0, 4/25, 9/25, and so forth). This figure shows little separation between the distributions of losses and policyholders. The right-hand panel shows the ordered Lorenz curve. Because observations are sorted by relativities, the first point after the origin (reading from left to right) is (8/25, 6/25). The second point is (13/25, 10/25), with the pattern continuing. For the ordered Lorenz curve, the horizontal axis uses premium weights, the vertical axis uses loss weights, and both axes are ordered by relativities. From the figure, we see that there is greater separation between losses and premiums when viewed through this relativity. Gini Index Specifically, the Gini index can be calculated as follows. Suppose that the empirical ordered Lorenz curve is given by \\(\\{ (a_0=0, b_0=0), (a_1, b_1), \\ldots,\\) \\((a_n=1, b_n=1) \\}\\) for a sample of \\(n\\) observations. Here, we use \\(a_j = \\hat{F}_P(R_j)\\) and \\(b_j = \\hat{F}_{L}(R_j)\\). Then, the empirical Gini index is \\[\\begin{eqnarray} \\widehat{Gini} &amp;=&amp; 2\\sum_{j=0}^{n-1} (a_{j+1} - a_j) \\left \\{ \\frac{a_{j+1}+a_j}{2} - \\frac{b_{j+1}+b_j}{2} \\right\\} \\nonumber \\\\ &amp;=&amp; 1 - \\sum_{j=0}^{n-1} (a_{j+1} - a_j) (b_{j+1}+b_j) .\\tag{4.8} \\end{eqnarray}\\] Example – Loss Distribution: Continued. In the figure, the Gini index for the left-hand panel is 5.6%. In contrast, the Gini index for the right-hand panel is 14.9%. \\(~~\\Box\\) TS A.3. Out-of-Sample Validation The Gini statistics based on an ordered Lorenz curve can be used for out-of-sample validation. The procedure follows: Use an in-sample data set to estimate several competing models. Designate an out-of-sample, or validation, data set of the form \\(\\{(y_i, \\mathbf{x}_i), i=1,\\ldots,n\\}\\). Establish one of the models as the base model. Use this estimated model and explanatory variables from the validation sample to form premiums of the form \\(P(\\mathbf{x}_i))\\). Use an estimated competing model and validation sample explanatory variables to form scores of the form \\(S(\\mathbf{x}_i))\\). From the premiums and scores, develop relativities \\(R_i =S(\\mathbf{x}_i)/P(\\mathbf{x}_i)\\). Use the validation sample outcomes \\(y_i\\) to compute the Gini statistic. Example – Out-of-Sample Validation. Suppose that we have experience from 25 states. For each state, we have available 500 observations that can be used to predict future losses. For this illustration, we have generated losses using a gamma distrbution with common shape parameter equal to 5 and a scale parameter that varies by state, from a low of 20 to 66. Determine the ordered Lorenz curve and the corresponding Gini statistic to compare the two rate procedures. Show Example Solution For our base premium, we simply use the maximum likelihood estimate assuming a common distribution among all states. For the gamma distribution, this turns out to be simply the average which for our simulation is P=219.96. You can think of this common premium as based on a community rating principle. As an alternative, we use averages that are state-specific. Because this illustration uses means that vary by states, we anticipate this alternative rating procedure to be preferred to the community rating procedure. (Recall for the gamma distribution that the mean equals the shape times the scale or, 5 times the scale parameter, for our example.) Out of sample claims were generated from the same gamma distribution, with 200 observations for each state. In the following, we have the ordered Lorenz curve. For these data, the Gini index is 0.187 with a standard error equal to 0.00381. Discussion In insurance claims modeling, standard out-of-sample validation measures are not the most informative due to the high proportions of zeros (corresponding to no claim) and the skewed fat-tailed distribution of the positive values. The Gini index can be motivated by the economics of insurance. Intuitively, the Gini index measures the negative covariance between a policy’s “profit” (\\(P-y\\), premium minus loss) and the rank of the relativity (R, score divided by premium). That is, the close approximation \\[\\widehat{Gini} \\approx - \\frac{2}{n} \\widehat{Cov} \\left( (P-y), rank(R) \\right) .\\] This observation leads an insurer to seek an ordering that produces to a large Gini index. Thus, the Gini index and associated ordered Lorenz curve are useful for identifying profitable blocks of insurance business. Unlike classical measures of association, the Gini index assumes that a premium base P is currently in place and seeks to assess vulnerabilities of this structure. This approach is more akin to hypothesis testing (when compared to goodness of fit) where one identifies a “null hypothesis” as the current state of the world and uses decision-making criteria/statistics to compare this with an “alternative hypothesis.” The insurance version of the Gini statistic was developed by (E. W. Frees, Meyers, and Cummings 2011) and (E. W. Frees, Meyers, and Cummings 2014) where you can find formulas for the standard errors and other additional background information. Bibliography "],
["aggregate-loss-models.html", "Chapter 5 Aggregate Loss Models 5.1 Introduction 5.2 Individual Risk Model 5.3 Collective Risk Model 5.4 Computing the Aggregate Claims Distribution 5.5 Effects of Coverage Modifications", " Chapter 5 Aggregate Loss Models Chapter Preview. This chapter introduces probability models for describing the aggregate claims that arise from a portfolio of insurance contracts. We presents two standard modeling approaches, the individual risk model and the collective risk model. Further, we discuss strategies for computing the distribution of the aggregate claims. Finally, we examine the effects of individual policy modifications on the aggregate loss distribution. 5.1 Introduction The objective of this chapter is to build a probability model to describe the aggregate claims by an insurance system occurring in a fixed time period. The insurance system could be a single policy, a group insurance contract, a business line, or an entire book of an insurance’s business. In the chapter, aggregate claims refers to either the number or the amount of claims from a portfolio of insurance contracts. However, the modeling framework is readily to apply in the more general setup. Consider an insurance portfolio of \\(n\\) individual contracts, and let \\(S\\) denote the aggregate losses of the portfolio in a given time period. There are two approaches to modeling the aggregate losses \\(S\\), the individual risk model and the collective risk model. The individual risk model emphasizes the loss from each individual contract and represents the aggregate losses as: \\[\\begin{aligned} S=X_1 +X_2 +\\cdots+X_n, \\end{aligned}\\] where \\(X_i~(i=1,\\ldots,n)\\) is interpreted as the loss amount from the \\(i\\)th contract. It is worth stressing that \\(n\\) denotes the number of contracts in the portfolio and thus is a fixed number rather than a random variable. For the individual risk model, one usually assumes \\(X_{i}\\)’s are independent, i.e., \\(X_{i}\\perp X_{j}\\) \\(\\forall\\) \\(i,j\\). Because of different contract features such as coverage and exposure, \\(X_{i}\\)’s are not necessarily identically distributed. A notable feature of the distribution of each \\(X_i\\) is the probability mass at zero corresponding to the event of no claims. The collective risk model represents the aggregate losses in terms of a frequency distribution and a severity distribution: \\[\\begin{aligned} S=X_1 +X_2 +\\cdots+X_N. \\end{aligned}\\] Here one thinks of a random number of claims \\(N\\) that may represent either the number of losses or the number of payments. In contrast, in the individual risk model, we use a fixed number of contracts \\(n\\). We think of \\(X_1, X_2, \\ldots, X_N\\) as representing the amount of each loss. Each loss may or may not corresponding to a unique contract. For instance, there may be multiple claims arising from a single contract. It is natural to think about \\(X_i&gt;0\\) because if \\(X_i=0\\) then no claim has occurred. Typically we assume that conditional on \\(N=n\\), \\(X_{1},X_{2},\\cdots ,X_{n}\\) are i.i.d. random variables. The distribution of \\(N\\) is known as the frequency distribution, and the common distribution of \\(X\\) is known as the severity distribution. We further assume \\(N\\) and \\(X\\) are independent. With the collective risk model, we may decompose the aggregate losses into the frequency (\\(N\\)) process and the severity (\\(X\\)) process. This flexibility allows the analyst to comment on these two separate components. For example, sales growth due to lower underwriting standards could lead to higher frequency of losses but might not affect severity. Similarly, inflation or other economic forces could have an impact on severity but not on frequency. 5.2 Individual Risk Model As noted earlier, for the individual risk model, we think of \\(X_i\\) as the loss from \\(i^{th}\\) contract and interpret \\[\\begin{eqnarray*} S_n=X_1 +X_2 +\\cdots+X_n \\end{eqnarray*}\\] to be the aggregate loss from all contracts in a portfolio or group of contracts. Under the independence assumption on \\(X_i&#39;s\\), it is straightforward to show \\[\\begin{aligned} {\\rm E}(S_n) &amp;= \\sum_{i=1}^{n} {\\rm E}(X_i),~~~~ {\\rm Var}(S_n) = \\sum_{i=1}^{n} {\\rm Var}(X_i)\\\\ P_{S_n}(z) &amp;= \\prod_{i=1}^{n}P_{X_i}(z), ~~~~ M_{S_n}(t) = \\prod_{i=1}^{n}M_{X_i}(t) \\\\ \\end{aligned}\\] where \\(P_S(\\cdot)\\) and \\(M_S(\\cdot)\\) are probability generating function and moment generating function of \\(S\\), respectively. The distribution of each \\(X_i\\) contains mass at zero, corresponding to the event of no claim. One strategy to incorporate the zero mass in the distribution is using the two-part framework: \\[\\begin{aligned} X_i = I_i\\times B_i = \\left\\{\\begin{array}{ll} 0 &amp; I_i=0 \\\\ B_i &amp; I_i=1 \\end{array} \\right. \\end{aligned}\\] Here \\(I_i\\) is a Bernoulli variable indicating whether or not a loss occurs for the \\(i\\)th contract, and \\(B_i\\), a r.v. with nonnegative support, represents the amount of losses of the contract given loss occurrence. Assume that \\(I_1 ,\\ldots,I_n ,B_1 ,\\ldots,B_n\\) are mutually independent. Denote \\({\\rm Pr} (I_i =1)=q_i\\), \\(\\mu_i={\\rm E}(B_i)\\), and \\(\\sigma_i^2={\\rm Var}(B_i)\\). One can show \\[\\begin{aligned} \\mathrm{E}(S_n)&amp; =\\sum_{i=1}^n ~q_i ~\\mu _j \\\\ \\mathrm{Var}(S_n) &amp; =\\sum_{i=1}^n \\left( q_i \\sigma _i^2+q_i (1-q_j)\\mu_i^2 \\right)\\\\ P_{S_n}(z) &amp; =\\sum_{i=1}^n \\left( 1-q_i+q_i P_{B_i}(z) \\right)\\\\ M_{S_n}(t) &amp; =\\sum_{i=1}^n \\left( 1-q_i+q_i M_{B_i}(t) \\right) \\end{aligned}\\] A special case of the above model is when \\(B_i\\) follows a degenerate distribution with \\(\\mu_i=b_i\\) and \\(\\sigma^2_i=0\\). One example is term life insurance or a pure endowment insurance where \\(b_i\\) represents the amount of insurance of the \\(i\\)th contract. Another strategy to accommodate zero mass in the distribution of \\(X_i\\) is a collective risk model, i.e. \\(X_i=Z_{i1}+\\cdots+Z_{iN_i}\\) where \\(X_i=0\\) when \\(N_i=0\\). The collective risk model will be discussed in detail in the next section. Example. Course 3, May 2000, 19. An insurance company sold 300 fire insurance policies as follows: \\[\\begin{matrix} \\begin{array}{c c c} \\hline \\text{Number of} &amp; \\text{Policy} &amp; \\text{Probability of}\\\\ \\text{Policies} &amp; \\text{Maximum} &amp; \\text{Claim Per Policy}\\\\ \\hline 100 &amp; 400 &amp; 0.05\\\\ 200 &amp; 300 &amp; 0.06\\\\ \\hline \\end{array} \\end{matrix}\\] You are given: (i) The claim amount for each policy is uniformly distributed between \\(0\\) and the policy maximum. (ii) The probability of more than one claim per policy is \\(0\\). (iii) Claim occurrences are independent. Calculate the mean \\(\\mathrm{E~}S_n\\) and variance \\(\\mathrm{Var~}S_n\\) of the aggregate claims. How would these results change if every claim is equal to the policy maximum? Solution. The aggregate claims are \\(S_{300} = X_1+\\cdots+X_{300}\\). Policy claims amounts are uniformly distributed on \\((0,PolMax)\\), so the mean claim amount is \\(PolMax/2\\) and the variance is\\(PolMax^2/12\\). Thus, for policy \\(i=1,...,300\\), we have \\[\\begin{matrix} \\begin{array}{ccccc} \\hline \\text{Number of} &amp; \\text{Policy} &amp; \\text{Probability of} &amp; \\text{Mean} &amp; \\text{Variance}\\\\ \\text{Policies} &amp; \\text{Maximum} &amp; \\text{Claim Per Policy} &amp; \\text{Amount} &amp; \\text{Amount}\\\\ &amp; &amp; (q_{i}) &amp; (\\mu_{i}) &amp; (\\sigma_{i}^2) \\\\ \\hline 100 &amp; 400 &amp; 0.05 &amp; 200 &amp; 400^2/12\\\\ 200 &amp; 300 &amp; 0.06 &amp; 150 &amp; 300^2/12 \\\\ \\hline \\end{array} \\end{matrix}\\] The mean of the aggregate claims is \\[\\mathrm{E~} S_{300} = \\sum_{i=1}^{300} q_i \\mu_i = 100\\left\\{0.05(200)\\right\\} + 200\\left\\{0.06 (150) \\right\\} = 1,000+1,824 = 2,824\\] The variance of the aggregate claims is \\[\\begin{eqnarray*} \\mathrm{Var~}S_{300} &amp;=&amp; \\sum_{i=1}^{300} \\left( q_i \\sigma _i^2+q_i (1-q_i )\\mu_i^2 \\right) \\\\ &amp;=&amp; 100\\left\\{ 0.05 \\left(\\frac{400^2}{12}\\right) +0.05 (1-0.05 )200^2 \\right\\}+ 200\\left\\{ 0.06 \\left(\\frac{300^2}{12}\\right) +0.06 (1-0.06 )150^2 \\right\\}\\\\ &amp;=&amp; 600,466.67 . \\end{eqnarray*}\\] \\(\\Box\\) Follow-Up. Now suppose everybody receives the policy maximum if a claim occurs. What is the expected aggregate loss and variance of the aggregate loss? Each policy claim amount \\(B_i\\) is now fixed at \\(PolMax\\) instead of random, so \\(\\sigma_i^2 = \\mathrm{Var~} B_i = 0\\) and \\(\\mu_i = PolMax\\). \\[\\begin{aligned} \\mathrm{E~}S^X &amp;= \\sum_{i=1}^{300} q_i \\mu_i = 100 \\left\\{0.05(400) \\right\\} + 200 \\left\\{ 0.06(300) \\right\\} = 5,648 \\end{aligned}\\] \\[\\begin{aligned} \\mathrm{Var~}S^X &amp;= \\sum_{i=1}^{300} \\left( q_i \\sigma _i^2+q_i (1-q_i )\\mu_i^2 \\right) = \\sum_{i=1}^{300} \\left( q_i (1-q_i) \\mu_i^2 \\right) \\\\ &amp;= 100 \\left\\{(0.05) (1-0.05) 400^2\\right\\} + 200 \\left\\{(0.06) (1-0.06)300^2\\right\\} \\\\ &amp;= 76,000 + 101,520 = 177,520 \\end{aligned}\\] \\(\\Box\\) The individual risk model can also be used for claim frequency. If \\(X_i\\) denotes the number of claims from the \\(i\\)th contract, and \\(S_n\\) is interpreted as the total number of claims from the portfolio. In this case, the above two-part framework still applies. Assume \\(X_i\\) belongs to the \\((a,b,0)\\) class with pmf denoted by \\(p_{ik}\\). Let \\(X_i^{T}\\) denote the associated zero-truncated distribution in the \\((a,b,1)\\) class with the pmf \\(p_{ik}^T=p_{ik}/(1-p_{i0})\\) for \\(k=1,2,\\ldots\\). Using the relationship between their generating functions: \\[\\begin{aligned} P_{X_i}(z) = p_{i0} +(1-p_{i0}) P_{X_i^{T}}(z), \\end{aligned}\\] we can write \\(X_i=I_i\\times B_i\\) with \\(q_i={\\rm Pr}(I_i=1)={\\rm Pr}(X_i&gt;0)=1-p_{i0}\\) and \\(B_i=X_i^T\\). Example.An insurance company sold a portfolio of 100 independent homeowners insurance policies, each of which has claim frequency following a zero-modified Poisson distribution, as follows: \\[\\begin{matrix} \\begin{array}{cccc} \\hline \\text{Type of} &amp; \\text{Number of} &amp; \\text{Probability of} &amp; \\lambda \\\\ \\text{Policy} &amp; \\text{Policies} &amp; \\text{At Least 1 Claim} &amp; \\\\ \\hline \\text{Low-risk} &amp; 40 &amp; 0.03 &amp; 1 \\\\ \\text{High-risk} &amp; 60 &amp; 0.05 &amp; 2 \\\\ \\hline \\end{array} \\end{matrix}\\] Find the expected value and variance of the claim frequency for the entire portfolio. Solution. For each policy, we can write the zero-modified Poisson claim frequency \\(N_i\\) as \\(N_i = I_i \\times B_i\\), where \\[q_i = \\Pr(I_i = 1) = \\Pr(N_i &gt; 0) = 1-p_{i0}\\] For the low-risk policies, we have \\(q_i = 0.03\\) and for the high-risk policies, we have \\(q_i=0.05\\). Further, \\(B_i = N_i^T\\), the zero-truncated version of \\(N_i\\). Thus, we have \\[\\begin{aligned} \\mu_i &amp;={\\rm E}(B_i) = {\\rm E}(N_i^T) = \\frac{\\lambda}{1-e^{-\\lambda}} \\\\ \\sigma_i^2 &amp;={\\rm Var}(B_i) = {\\rm Var}(N_i^T) = \\frac{\\lambda [1-(\\lambda+1)e^{-\\lambda}]}{(1-e^{-\\lambda})^2} \\end{aligned}\\] Let the portfolio claim frequency be \\(S_n = \\sum_{i=1}^n N_i\\). Using the formulas above, the expected claim frequency of the portfolio is \\[\\begin{aligned} \\mathrm{E~} S_n &amp;= \\sum_{i=1}^{100} q_i \\mu_i \\\\ &amp; = 40\\left[0.03 \\left(\\frac{1}{1-e^{-1}} \\right) \\right] + 60 \\left[0.05 \\left( \\frac{2}{1-e^{-2}} \\right) \\right] \\\\ &amp;= 40(0.03)(1.5820) + 60(0.05)(2.3130) = 8.8375 \\end{aligned}\\] The variance of the claim frequency of the portfolio is \\[\\begin{aligned} \\mathrm{Var~}S_n &amp;= \\sum_{i=1}^{100} \\left( q_i \\sigma _i^2+q_i (1-q_i )\\mu_i^2 \\right) \\\\ &amp;= 40 \\left[ 0.03 \\left(\\frac{1-2e^{-1}}{(1-e^{-1})^2} \\right) + 0.03(0.97)(1.5820^2) \\right] + 60 \\left[0.05 \\left( \\frac{2[1-3e^{-2}]}{ (1-e^{-2})^2} \\right) + 0.05(0.95)(2.3130^2) \\right] \\\\ &amp;= 23.7214 \\end{aligned}\\] Note that equivalently, we could have calculated the mean and variance of an individual policy directly using the relationship between the zero-modified and zero-truncated Poisson distributions. \\(\\Box\\) To understand the distribution of the aggregate loss, one could use central limit theorem to approximate the distribution of \\(S_n\\). Denote \\(\\mu_S={\\rm E}(S)\\) and \\(\\sigma^2_S={\\rm Var}(S)\\), the cdf of \\(S_n\\) is: \\[\\begin{aligned} F_{S_n}(s)={\\rm Pr}({S_n}\\leq s) = \\Phi \\left(\\frac{s-\\mu_S}{\\sigma_S}\\right). \\end{aligned}\\] Example. Course 3, May 2000, 19 (Follow-Up). As in the original example earlier, an insurance company sold 300 fire insurance policies, with claim amounts uniformly distributed between 0 and the policy maximum. Using the normal approximation, calculate the probability that the aggregate claim amount exceeds \\(\\$3,500\\). Solution. We have seen earlier that \\(\\mathrm{E~} S_{300}=2,824\\) and \\(\\mathrm{Var~}S_{300} = 600,466.67\\). Then \\[\\begin{aligned} {\\rm Pr}(S_{300} &gt; 3,500) &amp;= 1 - {\\rm Pr}(S_{300} \\leq 3,500) \\\\ &amp;= 1- \\Phi \\left( \\frac{3,500-2,824}{\\sqrt{600,466.67}} \\right) = 1 - \\Phi \\left( 0.87237 \\right) \\\\ &amp;= 1 - 0.8085 = 0.1915 \\end{aligned}\\] \\(\\Box\\) For small \\(n\\), the distribution of \\(S_n\\) is likely skewed, and the normal approximation would be a poor choice. To examine the aggregate loss distribution, we go back to the basics and first principles. Specifically, the distribution can be derived recursively. Define \\(S_k=X_1 + \\cdots + X_k, k=1,\\ldots,n\\), we have: For \\(k=1\\): \\[F_{S_1}(s) = {\\rm Pr}(S_1\\leq s) = {\\rm Pr}(X_1\\leq s) = F_{X_1}(s).\\] For \\(k=2,\\ldots,n\\), \\[\\begin{aligned} F_{S_k}(s)&amp;={\\Pr}(X_1+\\cdots+X_k\\leq s) ={\\Pr}(S_{k-1}+X_k\\leq s) \\\\ &amp;={\\rm E}_{X_k}\\left[{\\rm Pr}(S_{k-1}\\leq s-X_k|X_k)\\right]= {\\rm E}_{X_k}\\left[F_{S_{k-1}}(s-X_k)\\right]. \\end{aligned}\\] There are some simple cases where the \\(S_n\\) has a closed form. Examples include If \\(X_i\\sim N(\\mu_i,\\sigma_i^2)\\), then \\(S_n\\sim N(\\sum_{i=1}^{n}\\mu_i,\\sum_{i=1}^{n}\\sigma_i^2)\\) If \\(X_i\\sim Exponential(\\theta)\\), then \\(S_n\\sim Gamma(n,\\theta)\\) If \\(X_i\\sim Gamma(\\alpha_i,\\theta)\\), then \\(S_n\\sim Gamma(\\sum_{i=1}^n\\alpha_i,\\theta)\\) If \\(X_i\\sim Poisson(\\lambda_i)\\), then \\(S_n\\sim Poisson(\\sum_{i=1}^{n}\\lambda_i)\\) If \\(X_i\\sim Bin(q,m_i)\\), then \\(S_n\\sim Bin(q,\\sum_{i=1}^n m_i)\\) If \\(X_i\\sim Geometric(\\beta)\\), then \\(S_n\\sim NegBin(\\beta,n)\\) If \\(X_i\\sim NegBin(\\beta,r_i)\\), then \\(S_n\\sim NegBin(\\beta,\\sum_{i=1}^n r_i)\\) A special case is when \\(X_i&#39;s\\) are identically distributed. Let \\(F_X(x)={\\Pr}(X\\leq x)\\) be the common distribution of \\(X_i\\) \\((i=1,\\ldots,n)\\), we define \\[F^{*n}_X(x)={\\Pr}(X_1+\\cdots+X_n\\leq x)\\] the \\(n\\)-fold convolution of \\(F_X\\). Example: Gamma Distribution. For an easy case, assume that \\(X_i \\sim\\) gamma with parameters \\((\\alpha, \\theta)\\). As we know, the moment generating function (mgf) is \\(M_{X}(t) = (1 - \\theta t)^{- \\alpha}\\). Thus, the mgf of the sum \\(S_n = X_1 + \\cdots + X_n\\) is \\[\\begin{eqnarray*} M_{S_n}(t) = \\mathrm{E~} \\exp(t(X_1 + \\cdots + X_n)) = (1 - \\theta t)^{-n \\alpha} , \\end{eqnarray*}\\] Thus, \\(S_n\\) has a gamma distribution with parameters \\((n \\alpha, \\theta)\\). This makes it easy to compute \\(F^{\\ast n}(x) = \\Pr(S_n \\le x).\\) This property is known as ``closed under convolution’’. \\(\\Box\\) Example: Negative Binomial Distribution. Assume \\(X_i \\sim NegBin(\\beta, r_i)\\). The probability generating function (pgf) is \\(P_X(z) = \\left[1-\\beta(z-1) \\right]^{-r}\\). Thus, the pgf of the sum \\(S_n =X_1+\\cdots+X_n\\) is \\[\\begin{aligned} P_{S_n}(z) &amp;= \\mathrm{E~}\\left[ z^{S_n} \\right] = \\mathrm{E~}\\left[ z^{X_1+\\cdots+X_n} \\right] = \\mathrm{E~}\\left[ z^{X_1} z^{X_2} \\cdots z^{X_n} \\right] = \\mathrm{E~}\\left[z^{X_1}\\right] \\cdots \\mathrm{E~}\\left[z^{X_n}\\right] \\\\ &amp;= \\prod_{i=1}^n P_{X_i}(z) = \\prod_{i=1}^n \\left[1-\\beta(z-1) \\right]^{-r_i} = \\left[1-\\beta(z-1) \\right]^{-\\sum_{i=1}^n r_i} \\end{aligned}\\] Thus, \\(S_n\\) has a negative binomial distribution with parameters \\((\\beta, \\sum_{i=1}^n r_i)\\). \\(\\Box\\) More generally, we can compute \\(F^{\\ast n}\\) recursively. Begin the recursion at \\(n=1\\) using \\(F^{\\ast 1} \\left(x \\right) =F(x)\\). Next, for \\(n=2\\), we have \\[\\begin{eqnarray*} F^{\\ast 2} \\left(x \\right) &amp;=&amp; \\Pr(X_1 + X_2 \\le x) = \\mathrm{E}_{X_2} \\Pr(X_1 \\le x - X_2|X_2)\\\\ &amp;=&amp; \\mathrm{E}_X F(x - X)\\\\ &amp;=&amp; \\int_{0}^{x} F(x-y) f(y) dy \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\text{continuous}\\\\ &amp;=&amp; \\sum_{y \\le x} F(x-y) f(y) \\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\;\\text{discrete}\\\\ \\end {eqnarray*}\\] $$ Recall \\(F(0) = 0\\). Similarly, let \\(S_n = X_1 + X_2 + \\cdots + X_n\\) \\[\\begin{eqnarray*} F^{\\ast n}\\left(x\\right) &amp;=&amp; \\Pr(S_n \\le x) = \\Pr(S_{n-1} + X_n \\le x)\\\\ &amp;=&amp;\\mathrm{E}_{X_n}\\Pr(S_{n-1} \\le x - X_n|X_n)\\\\ &amp;=&amp;\\mathrm{E}_X F^{\\ast(n-1)}(x - X)\\\\ &amp;=&amp; \\int_{0}^{x} F^{\\ast(n-1)}(x-y)f(y)dy \\:\\:\\:\\text{continuous}\\\\ &amp;=&amp; \\sum_{y \\le x} F^{\\ast(n-1)}(x-y)f(y) \\:\\:\\:\\:\\:\\:\\:\\:\\text{discrete}\\\\ \\end{eqnarray*}\\] Example. SOA Sample Question, 283 (modified). The annual number of doctor visits for each individual in a family of 4 has geometric distribution with mean 1.5. The annual numbers of visits for the family members are mutually independent. An insurance pays 100 per doctor visit beginning with the 4th visit per family. Calculate the probability that the family will receive an insurance payment this year. Solution. Let \\(X_i \\sim Geometric(\\beta=1.5)\\) be the number of doctor visits for one individual in the family and \\(S_4 = X_1 + X_2 + X_3 + X_4\\) be the number of doctor visits for the family. The sum of 4 independent geometric distributions each with mean 1.5 follows a negative binomial distribution, i.e. \\(S_4 \\sim NegBin(\\beta=1.5, r=4)\\). If the insurance pays 100 per visit beginning with the 4th visit for the family, then the family will not receive an insurance payment if they have less than 4 claims. This probability is \\[\\begin{aligned} \\Pr(S_4 &lt; 4) &amp;= \\Pr(S_4 = 0) + \\Pr(S_4 = 1) + \\Pr(S_4 = 2) +\\Pr(S_4 = 3) \\\\ &amp;= (1+1.5)^{-4} + \\frac{4(1.5)}{(1+1.5)^5} + \\frac{4(5)(1.5^2)}{2(1+1.5)^6} + \\frac{4(5)(6)(1.5^3)}{3!(1+1.5)^7}\\\\ &amp;= 0.0256 + 0.0614 + 0.0922 + 0.1106 = 0.2898 \\end{aligned}\\] \\(\\Box\\) 5.3 Collective Risk Model 5.3.1 Moments and Distribution Under the collective risk model \\(S=X_1+\\cdots+X_N\\), \\(\\{X_i\\}\\) are i.i.d., and independent of \\(N\\). Let \\(\\mu = {\\rm E}\\left( X_{i}\\right)\\) and \\(\\sigma ^{2} = {\\rm Var}\\left(X_{i}\\right)\\) \\(\\forall\\) \\(i\\). Using the law of iterated expectations, the mean is \\[\\begin{eqnarray*} {\\rm E}(S)={\\rm E}_N[{\\rm E}_S(S|N)] = {\\rm E}_N[N\\mu] = \\mu {\\rm E}(N). \\end{eqnarray*}\\] Using the law of total variation, the variance is \\[\\begin{aligned} {\\rm Var}(S) &amp;= {\\rm E}_N[{\\rm Var}_S(S|N)] + {\\rm Var}_N[{\\rm E}_S(S|N)] \\\\ &amp;={\\rm E}_N[N\\sigma^2] + {\\rm Var}_N[N\\mu] \\\\ &amp;=\\sigma^2{\\rm E}[N] + \\mu^2{\\rm Var}[N] \\end{aligned}\\] Special Case: Poisson Distributed Frequency. If \\(N \\sim Poisson (\\lambda)\\), then \\[\\begin{eqnarray*} \\mathrm{E~}N &amp;=&amp; \\mathrm{Var~}N = \\lambda\\\\ \\mathrm{Var~}S &amp;=&amp; \\lambda (\\sigma^2 + \\mu^2) = \\lambda ~\\mathrm{E~} X^2 . \\end{eqnarray*}\\] Example. Course 3, May 2001, 36. The number of accidents follows a Poisson distribution with mean 12. Each accident generates 1, 2, or 3 claimants with probabilities 1/2, 1/3, and 1/6 respectively. Calculate the variance in the total number of claimants. Solution: \\[\\mathrm{E~}X^2 = 1^2 \\left( \\frac{1}{2}\\right) + 2^2\\left(\\frac{1}{3} \\right) + 3^2\\left(\\frac{1}{6}\\right) = \\frac{10}{3}\\] \\[\\mathrm{Var~}S = \\lambda \\ \\mathrm{E~}X^2 = 12\\left(\\frac{10}{3}\\right) = 40\\] Alternatively, using the general approach, \\(\\mathrm{Var~}S = \\sigma^2 \\mathrm{E~}N + \\mu^2 \\mathrm{Var~}N\\), where \\[\\mathrm{E~}N = \\mathrm{Var~}N = 12\\] \\[\\mu = \\mathrm{E~}X = 1\\left(\\frac{1}{2}\\right) + 2\\left(\\frac{1}{3}\\right) + 3\\left(\\frac{1}{6}\\right) = \\frac{5}{3}\\] \\[\\sigma^2 = \\mathrm{E~}X^2 - (\\mathrm{E~}X)^2 = \\frac{10}{3} - \\frac{25}{9} = \\frac{5}{9}\\] \\[\\Rightarrow \\ \\mathrm{Var~}S = \\left(\\frac{5}{9}\\right)\\left(12\\right) + \\left(\\frac{5}{3}\\right)^2\\left(12\\right) = 40\\] \\(\\Box\\) In general, the moments of \\(S\\) can be derived from its moment generating function (mgf). Because \\(\\{X_i\\}\\) are i.i.d., we denote the mgf of \\(X\\) as \\(M_{X}(t) = \\mathrm{E~}(e^{tX})\\). Using the law of iterated expectations, the mgf of \\(S\\) is \\[\\begin{eqnarray*} M_{S}(t) &amp;=&amp; \\mathrm{E}(e^{St})=\\mathrm{E}[\\mathrm{E}(e^{St}|N)]\\\\ &amp;=&amp; \\mathrm{E~}[(M_{X}(t))^N] \\end{eqnarray*}\\] where we use the relation \\(\\mathrm{E}[e^{t(X_1+\\cdots+X_n)}] = \\mathrm{E}(e^{tX_1})\\cdots\\mathrm{E}(e^{tX_n}) = (M_{X}(t))^n\\). Now, recall that the probability generating function (pgf) of \\(N\\) is \\(P(z) = \\mathrm{E}(z^N)\\). Denote \\(M_{X}(t)=z\\), it is shown \\[\\begin{eqnarray*} M_{S}(t) = \\mathrm{E~}(z^N) = P_{N}(z) = P_{N}[M_{X}(t)]. \\end{eqnarray*}\\] Similarly, if \\(S\\) a discrete, one can show the pgf of \\(S\\) is: \\[\\begin{eqnarray*} P_{S}(z) = P_{N}[P_{X}(z)]. \\end{eqnarray*}\\] To get \\(\\mathrm{E~}S = M_{S}&#39;(0)\\), we use the chain rule \\[ M_{S}&#39;(t) = \\frac{\\partial}{\\partial t} P_{N}(M_{X}(t)) = P_{N}&#39;(M_{X}(t)) M_{X}&#39;(t)\\\\ \\] and recall \\(M_{X}(0) = 1, M_{X}&#39;(0) = \\mathrm{E~}X = \\mu, P_{N}&#39;(1) = \\mathrm{E~}N\\). So, \\[\\begin{eqnarray*} \\mathrm{E~}S = M_{S}&#39;(0) = P_{N}&#39;(M_{X}&#39;(0)) M_{X}&#39;(0) = \\mu {\\rm E}(X) \\end{eqnarray*}\\] Similarly, one could use relation \\[ \\mathrm{E~}S^2 = M_{S}&#39;&#39;(0) \\] to get \\[\\mathrm{Var~}S = \\sigma^2 \\mathrm{E~}N + \\mu^2 \\mathrm{Var~}N.\\] Special Case. Poisson Frequency. Let \\(N \\sim Poisson (\\lambda)\\). Thus, the pgf of \\(N\\) is \\(P_N (z) =\\exp[\\lambda(z-1)]\\), and the mgf of \\(S\\) is \\[\\begin{eqnarray*} M_{S}(t) &amp;=&amp;\\exp[\\lambda(M_{X}(t) - 1)]. \\end{eqnarray*}\\] Taking derivatives yield \\[\\begin{eqnarray*} M_{S}(t) &amp;=&amp;\\exp(\\lambda(M_{X}(t) - 1))\\\\ M_{S}&#39;(t) &amp;=&amp;\\exp(\\lambda(M_{X}(t) - 1)) \\lambda M_{X}&#39;(t)\\\\ &amp;=&amp; M_{S}(t) \\lambda M_{X}&#39;(t)\\\\ M_{S}&#39;&#39;(t) &amp;=&amp; M_{S}(t) \\lambda M_{X}&#39;&#39;(t) + \\{M_{S}(t) \\lambda M_{X}&#39;(t)\\} \\lambda M_{X}&#39;(t) \\end{eqnarray*}\\] Evaluating these at \\(t=0\\) yields \\[\\begin{eqnarray*} M_{S}&#39;&#39;(0) &amp;=&amp; \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2\\\\ \\mathrm{Var~}S &amp;=&amp; \\lambda \\mathrm{E}(X^2) + \\lambda^2 \\mu^2 - (\\lambda \\mu)^2\\\\ &amp;=&amp; \\lambda \\mathrm{E}(X^2). \\end{eqnarray*}\\] \\(\\Box\\) Example. Course 3, May 2001, 29. You are the producer of a television quiz show that gives cash prizes. The number of prizes, \\(N\\), and prize amount, \\(X\\), have the following distributions: \\[\\begin{matrix} \\begin{array}{ccccc}\\hline n &amp; \\Pr(N=n) &amp; &amp; x &amp; \\Pr(X=x)\\\\ \\hline 1 &amp; 0.8 &amp; &amp; 0 &amp; 0.2 \\\\ 2 &amp; 0.2 &amp; &amp; 100 &amp; 0.7 \\\\ &amp; &amp; &amp; 1000 &amp; 0.1\\\\\\hline \\end{array} \\end{matrix}\\] Your budget for prizes equals the expected aggregate cash prizes plus the standard deviation of aggregate cash prizes. Calculate your budget. Solution. We need to calculate the mean and standard deviation of the aggregate (sum) of cash prizes. The moments of the frequency distribution \\(N\\) are \\[\\begin{eqnarray*} \\mathrm{E~}N &amp;=&amp; 1 (0.8) + 2 (0.2) =1.2\\\\ \\mathrm{E~}N^2 &amp;=&amp; 1^2 (0.8) + 2^2 (0.2) =1.6\\\\ \\mathrm{Var~}N &amp;=&amp; \\mathrm{E~}N^2 - \\left( \\mathrm{E~}N \\right)^2= 0.16 \\end{eqnarray*}\\] The moments of the severity distribution \\(X\\) are \\[\\begin{eqnarray*} \\mathrm{E~}X &amp;=&amp; 0 (0.2) + 100 (0.7) + 1000 (0.1) = 170 = \\mu\\\\ \\mathrm{E~}X^2 &amp;=&amp; 0^2 (0.2) + 100^2 (0.7) + 1000^2 (0.1) = 107,000\\\\ \\mathrm{Var~}X &amp;=&amp; \\mathrm{E~}X^2 - \\left( \\mathrm{E~}X \\right)^2=78,100 = \\sigma^2 \\end{eqnarray*}\\] Thus, the mean and variance of the aggregate cash prize are \\[\\begin{eqnarray*} \\mathrm{E~}S &amp;=&amp; \\mu \\mathrm{E~}N = 170 (1.2) = 204 \\\\ \\mathrm{Var~}S &amp;=&amp; \\sigma^2 \\mathrm{E~}N + \\mu^2 \\mathrm{Var~}N\\\\ &amp;=&amp; 78,100 (1.2) + 170^2 (0.16) = 98,344 \\end{eqnarray*}\\] This gives the following required budget \\[\\begin{eqnarray*} Budget &amp;=&amp; \\mathrm{E~}S + \\sqrt{\\mathrm{Var~}S} \\\\ &amp;=&amp; 204 + \\sqrt{98,344} = 517.60 . \\end{eqnarray*}\\] \\(\\Box\\) The distribution of \\(S\\) is called a compound distribution, and it can be derived based on the convolution of \\(F_X\\) as follows: \\[\\begin{eqnarray*} F_{S}(s) &amp;=&amp; \\Pr \\left(X_1 + \\cdots + X_N \\le s \\right) \\\\ &amp;=&amp; \\mathrm{E} \\left[ \\Pr \\left(X_1 + \\cdots + X_N \\le s|N=n \\right) \\right]\\\\ &amp;=&amp; \\mathrm{E} \\left[ F_{X}^{\\ast N}(s) \\right] \\\\ &amp;=&amp; p_0 + \\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}(s) \\end{eqnarray*}\\] Example. Course 3, Fall 2002, 36. The number of claims in a period has a geometric distribution with mean \\(4\\). The amount of each claim \\(X\\) follows \\(\\Pr(X=x) = 0.25, \\ x=1,2,3,4.\\) The number of claims and the claim amounts are independent. Let \\(S\\) denote the aggregate claim amount in the period. Calculate \\(F_{S}(3)\\) Solution. By definition, we have \\[\\begin{aligned} F_{S}\\left(3 \\right) &amp;= {\\rm Pr}\\left(\\sum_{i=1}^N X_i \\leq 3\\right) = \\sum_{n=0}^\\infty {\\rm Pr}\\left(\\sum_{i=1}^n X_i\\leq 3|N=n\\right){\\rm Pr}(N=n) \\\\ &amp;= \\sum_n F^{\\ast n} \\left(3 \\right) p_n = \\sum_{n=0}^3 F^{\\ast n}(3) p_n \\\\ &amp;= p_0 + F^{\\ast 1}(3) \\ p_1 + F^{\\ast 2}(3) \\ p_2 + F^{\\ast 3}(3) \\ p_3 \\end{aligned}\\] Because \\(N\\) has a geometric distribution with mean 4, we know that \\[\\begin{aligned} p_n &amp;= \\frac{1}{1+\\beta} \\left(\\frac{\\beta}{1+ \\beta} \\right)^n = \\frac{1}{5} \\left(\\frac{4}{5} \\right)^n \\end{aligned}\\] For the claim severity distribution, recursively, we have \\[\\begin{aligned} F^{\\ast 1}(3) &amp;= \\Pr(X \\le 3) = \\frac{3}{4} \\\\ F^{\\ast 2}(3) &amp;= \\sum_{y \\le 3} F^{\\ast 1} (3-y) f(y) = F^{\\ast 1}(2)f(1) + F^{\\ast 1}(1)f(2) \\\\ &amp;= \\frac{1}{4}\\left[F^{\\ast 1} (2) + F^{\\ast 1}(1)\\right] = \\frac{1}{4}\\left[{\\rm Pr}(X\\leq 2) + {\\rm Pr}(X \\leq 1) \\right] \\\\ &amp;= \\frac{1}{4} \\left(\\frac{2}{4} + \\frac{1}{4} \\right) = \\frac{3}{16}\\\\ F^{\\ast 3}(3) &amp;= \\Pr(X_1+X_2 + X_3 \\le 3) = \\Pr(X_1=X_2=X_3=1) = \\left(\\frac{1}{4} \\right)^3 \\end{aligned}\\] Notice that we did not need to recursively calculate \\(F^{\\ast 3}(3)\\) by recognizing that each \\(X \\in \\{1,2,3,4\\}\\), so the only way of obtaining \\(X_1+X_2+X_3 \\leq 3\\) is to have \\(X_1=X_2=X_3=1\\). Additionally, for \\(n \\geq 4\\), \\(F^{\\ast n} (3)=0\\) since it is impossible for the sum of 4 or more \\(X\\)’s to be less than 3. For \\(n=0\\), \\(F^{\\ast 0}(3) = 1\\) since the sum of 0 \\(X\\)’s is 0, which is always less than 3. Laying out the probabilities systematically, \\[\\begin {matrix} \\begin{array}{c c c c}\\hline x &amp; F^{\\ast 1}(x) &amp; F^{\\ast 2}(x) &amp; F^{\\ast 3}(x)\\\\ \\hline 0 &amp; &amp; &amp; \\\\ 1 &amp; \\frac{1}{4} &amp; 0 &amp; \\\\ 2 &amp; \\frac{2}{4} &amp; \\left( \\frac{1}{4} \\right)^2 &amp; \\\\ 3 &amp; \\frac{3}{4} &amp; \\frac{3}{16} &amp; \\left( \\frac{1}{4} \\right)^3 \\\\ \\hline \\end{array} \\end{matrix}\\] Finally, \\[\\begin{aligned} F_{S}(3) &amp;= p_0 + F^{\\ast 1}(3) \\ p_1 + F^{\\ast 2}(3) \\ p_2 + F^{\\ast 3}(3) \\ p_3 \\\\ &amp;= \\frac{1}{5} + \\frac{3}{4}\\left(\\frac{4}{25} \\right) + \\frac{3}{16} \\left( \\frac{16}{125} \\right) + \\frac{1}{64} \\left( \\frac{64}{625}\\right) = 0.3456\\\\ \\end{aligned}\\] \\(\\Box\\) When \\(\\mathrm{E}(N)\\), one may also use the central limit theorem to approximate the distribution of \\(S\\) as in the individual risk model. That is, \\(\\frac{S - \\mathrm{E}(S)}{\\sqrt{\\mathrm{Var}(S)}}\\) approximately follows \\(N(0,1)\\). Example. Exam 3, May 2000, 16. You are given: \\[\\begin{matrix} \\begin{array}{ c | c c } \\hline &amp; \\text{Mean} &amp; \\text{Standard Deviation}\\\\ \\hline \\text{Number of Claims} &amp; 8 &amp; 3\\\\ \\text{Individual Losses} &amp; 10,000 &amp; 3,937\\\\ \\hline \\end{array} \\end{matrix}\\] Using the normal approximation, determine the probability that the aggregate loss will exceed 150\\(\\%\\) of the expected loss: \\[\\begin{array}{ c c c c c} (A) \\Phi (1.25) &amp; (B) \\Phi (1.5) &amp; (C) 1-\\Phi (1.5) &amp; (D) 1-\\Phi (1.25) &amp; (E) 1.5 \\times \\Phi (1) \\end{array}\\] Solution. To use the normal approximation, we must first find the mean and variance of the aggregate loss \\(S\\) \\[\\begin{eqnarray*} \\mathrm{E~}S &amp;=&amp; \\mu \\ \\mathrm{E~}N = 10,000(8) = 80,000\\\\ \\mathrm{Var~}S &amp;=&amp; \\sigma^2 \\ \\mathrm{E~}N + \\mu^2 \\ \\mathrm{Var~}N\\\\ &amp;=&amp; 3937^2(8) + 10000^2 (3^2) = 1,023,999,752\\\\ \\sqrt{\\mathrm{Var~}S} &amp;=&amp; 31,999.996 \\approx 32,000 \\end{eqnarray*}\\] Then under the normal approximation, aggregate loss \\(S\\) is approximately normal with mean 80,000 and standard deviation 32,000. The probability that \\(S\\) will exceed 150\\(\\%\\) of the expected aggregate loss is therefore \\[\\begin{aligned} \\Pr(S&gt;1.5 \\mathrm{E~}S) &amp;= \\Pr \\left( \\frac{S - \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~}S}} &gt; \\frac{1.5 \\mathrm{E~}S - \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~}S}} \\right) \\\\ &amp;= \\Pr \\left( N(0,1) &gt; \\frac{0.5 \\mathrm{E~}S}{\\sqrt{\\mathrm{Var~}S} } \\right) \\\\ &amp;= \\Pr \\left( N(0,1) &gt; \\frac{0.5(80,000)}{32,000} \\right) = \\Pr( N(0,1) &gt; 1.25) \\\\ &amp;= 1-\\Phi(1.25) = 0.1056 \\end{aligned}\\] \\(\\Box\\) Example. Course 3, November 2000, 32. For an individual over \\(65\\): (i) The number of pharmacy claims is a Poisson random variable with mean \\(25\\). (ii) The amount of each pharmacy claim is uniformly distributed between \\(5\\) and \\(95\\). (iii) The amounts of the claims and the number of claims are mutually independent. Determine the probability that aggregate claims for this individual will exceed \\(2000\\) using the normal approximation: \\[\\begin{array}{ c c c c c} (A) 1-\\Phi (1.33) &amp; (B) 1-\\Phi (1.66) &amp; (C) 1-\\Phi (2.33) &amp; (D) 1-\\Phi (2.66) &amp; (E) 1-\\Phi (3.33) \\end{array}\\] Solution. We have claim frequency \\(N \\sim Poisson (\\lambda = 25)\\) and claim severity \\(X \\sim U \\left(5, 95 \\right)\\). To use the normal approximation, we need to find the mean and variance of the aggregate claims \\(S\\). Note \\[\\begin{matrix} \\begin{array}{lll} \\mathrm{E~} N = 25 &amp; &amp; \\mathrm{Var~} N = 25\\\\ \\mathrm{E~}X = \\frac{5+95}{2} = 50 = \\mu &amp; &amp; \\mathrm{Var~}X = \\frac{(95-5)^2}{12} = 675 = \\sigma^2\\\\ \\end{array} \\end{matrix}\\] Then for \\(S\\), \\[\\begin{eqnarray*} \\mathrm{E~}S &amp;=&amp; \\mu \\ \\mathrm{E~} N = 50(25) = 1,250\\\\ \\mathrm{Var~}S &amp;=&amp; \\sigma^2 \\ \\mathrm{E~}N + \\mu^2 \\ \\mathrm{Var~}N\\\\ &amp;=&amp; 675 (25) + 50^2 (25) = 79,375 \\end{eqnarray*}\\] Using the normal approximation, \\(S\\) is approximately normal with mean 1,250 and variance 79,375. The probability that \\(S\\) exceeds 2,000 is \\[\\begin{aligned} \\Pr(S&gt;2,000) &amp;= \\Pr \\left(\\frac{S - \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~} S}} &gt; \\frac{2,000- \\mathrm{E~} S}{\\sqrt{\\mathrm{Var~} S}} \\right) \\\\ &amp;= \\Pr\\left( N(0,1) &gt; \\frac{2,000-1,250}{\\sqrt{79,375}} \\right) \\\\ &amp;= \\Pr (N(0,1) &gt; 2.662) = 1-\\Phi(2.662) = 0.003884 \\end{aligned}\\] \\(\\Box\\) 5.3.2 Stop-loss Insurance Insurance on the aggregate loss \\(S\\), subjected to a deductible \\(d\\), is called . The quantity \\[\\begin{eqnarray*} \\mathrm{E}[(S-d)_+] \\end{eqnarray*}\\] is known as the net stop-loss premium. To calculate the net stop-loss premium, we have \\[\\begin{eqnarray*} \\mathrm{E}(S-d)_+ &amp; =&amp; \\int_{d}^{\\infty} \\left(1-F_S(s) \\right) ds\\\\ &amp;=&amp; \\int_{d}^{\\infty}(s-d) f_{S}(s) ds ~~~~{\\rm continuous}\\\\ &amp;=&amp; \\sum_{s&gt;d}(s-d) f_{S}(s) ds ~~~~~{\\rm discrete}\\\\ &amp;=&amp; \\mathrm{E}(S) - \\mathrm{E}(S\\wedge d)\\\\ \\end{eqnarray*}\\] Example. Exam M, Fall 2005, 19. In a given week, the number of projects that require you to work overtime has a geometric distribution with \\(\\beta=2\\). For each project, the distribution of the number of overtime hours in the week is as follows: \\[\\begin{matrix} \\begin{array}{ccc} \\hline x &amp; &amp; f(x)\\\\ \\hline 5 &amp; &amp; 0.2 \\\\ 10 &amp; &amp; 0.3 \\\\ 20 &amp; &amp; 0.5\\\\ \\hline \\end{array} \\end{matrix}\\] The number of projects and the number of overtime hours are independent. You will get paid for overtime hours in excess of 15 hours in the week. Calculate the expected number of overtime hours for which you will get paid in the week. Solution. The number of projects in a week requiring overtime work has distribution \\(N \\sim Geometric(\\beta=2)\\), while the number of overtime hours worked per project has distribution \\(X\\) as described above. The aggregate number of overtime hours in a week is \\(S\\) and we are therefore looking for \\[\\mathrm{E~}(S-15)_+ = \\mathrm{E~}S - \\mathrm{E~}(S \\wedge 15).\\] To find \\(\\mathrm{E~}S = \\mathrm{E~}X \\ \\mathrm{E~}N\\), we have \\[\\begin{eqnarray*} \\mathrm{E~}X &amp;=&amp; 5(0.2) + 10(0.3)+ 20(0.5)= 14 \\\\ \\mathrm{E~}N &amp;=&amp; 2 \\\\ \\Rightarrow \\ \\mathrm{E~}S &amp;=&amp; \\mathrm{E~}X \\ \\mathrm{E~}N = 14(2) = 28 \\end{eqnarray*}\\] To find \\(\\mathrm{E~} (S \\wedge 15) = 0 \\Pr (S=0) + 5 \\Pr(S=5) + 10 \\Pr(S=10) + 15 \\Pr(S \\geq 15)\\), we have \\[\\begin{eqnarray*} \\Pr(S=0) &amp;=&amp; \\Pr(N=0) = \\frac{1}{1+\\beta} = \\frac{1}{3} \\\\ \\Pr(S=5) &amp;=&amp; \\Pr(X=5, \\ N=1) = 0.2 \\left(\\frac{2}{9} \\right)= \\frac{0.4}{9}\\\\ \\Pr(S=10) &amp;=&amp; \\Pr(X=10, \\ N=1) + \\Pr(X_1=X_2=5, N=2) \\\\ &amp;=&amp; 0.3 \\left(\\frac{2}{9} \\right) + (0.2)(0.2) \\left( \\frac{4}{27} \\right)= 0.0726 \\\\ \\Pr(S \\geq 15) &amp;=&amp; 1 - \\left(\\frac{1}{3} + \\frac{0.4}{9} + 0.0726 \\right) = 0.5496\\\\ \\Rightarrow \\mathrm{E~}(S \\wedge 15) &amp;=&amp; 0 \\Pr (S=0) + 5 \\Pr(S=5) + 10 \\Pr(S=10) + 15 \\Pr(S \\geq 15) \\\\ &amp;=&amp; 0 \\left( \\frac{1}{3} \\right) + 5 \\left( \\frac{0.4}{9} \\right) + 10 (0.0726) + 15 (0.5496) = 9.193\\\\ \\end{eqnarray*}\\] Therefore, \\[\\begin{eqnarray*} \\mathrm{E~}(S-15)_+ &amp;=&amp; \\mathrm{E~}S - \\mathrm{E~}(S \\wedge 15) \\\\ &amp;=&amp; 28 - 9.193 = 18.807 \\end{eqnarray*}\\] \\(\\Box\\) Recursive Net Stop-Loss Premium Calculation. For the discrete case, this can be computed recursively as \\[\\begin{eqnarray*} \\mathrm{E~}\\left[ S-(j+1)h \\right] _{+}=\\mathrm{E~}\\left[ ( S-jh )_{+} \\right] -h \\left( 1-F_S(jh) \\right) . \\end{eqnarray*}\\] This assumes that the support of \\(S\\) is equally spaced over units of \\(h\\). To establish this, we assume that \\(h=1\\). Now, on the left-hand side, we have \\(\\mathrm{E~}\\left[ S-(j+1) \\right] _{+}=\\mathrm{E~}S - \\mathrm{E~}S\\wedge (j+1)\\). We can write \\[\\begin{eqnarray*} \\mathrm{E~}S\\wedge (j+1) = \\sum_{x=0}^{j}xf_S(x) + (j+1)\\Pr(S \\ge j+1). \\end{eqnarray*}\\] Similarly \\[\\begin{eqnarray*} \\mathrm{E~}S\\wedge j = \\sum_{x=0}^{j}xf_S(x) + j\\Pr(S\\ge j+1). \\end{eqnarray*}\\] With these, expressions, we have \\[\\begin{eqnarray*} \\mathrm{E~}\\left[ S-(j+1) \\right] _{+} &amp;-&amp; \\mathrm{E~}\\left[ ( S-j )_{+} \\right] \\\\ &amp;=&amp;\\left\\{\\mathrm{E~}S - \\mathrm{E~}S\\wedge (j+1) \\right\\} -\\left\\{\\mathrm{E~}S - \\mathrm{E~}S\\wedge j \\right\\} \\\\ &amp;=&amp;\\left\\{ \\sum_{x=0}^{j}xf_S(x) + j\\Pr(S\\ge j+1) \\right\\} - \\left\\{ \\sum_{x=0}^{j}xf_S(x) + (j+1)\\Pr(S \\ge j+1) \\right\\} \\\\ &amp;=&amp; -\\Pr(S\\ge j+1) = -\\{1 - F_{S}(j)\\}, \\end{eqnarray*}\\] as required. \\(\\Box\\) Exercise. Exam M, Fall 2005, 19 - Continued. Recall that the goal of this question was to calculate \\(\\mathrm{E~}(S-15)_+\\). Note that the support of \\(S\\) is equally spaced over units of 5, so this question can also be done recursively, using steps of \\(h=5\\): Step 1: \\[\\begin{aligned} \\mathrm{E~}(S-5)_+ &amp;= \\mathrm{E~}S - 5 [1-\\Pr(S \\leq 0) ]\\\\ %\\Pr (S\\geq 5) \\\\ &amp;= 28 - 5 \\left(1 - \\frac{1}{3}\\right) = \\frac{74}{3}=24.6667 \\end{aligned}\\] Step 2: \\[\\begin{aligned} \\mathrm{E~}(S-10)_+ &amp;= \\mathrm{E~}(S-5)_+ - 5 [1-\\Pr(S \\leq 5)]\\\\ %\\Pr (S\\ge 10) \\\\ &amp;= \\frac{74}{3} - 5\\left( 1 - \\frac{1}{3} - \\frac{0.4}{9}\\right) = 21.555 \\end{aligned}\\] Step 3: \\[\\begin{aligned} \\mathrm{E~}(S-15)_+ &amp;= \\mathrm{E~}(S-10)_+ - 5 [1-\\Pr(S \\leq 10)] \\\\ %\\Pr (S\\ge 15) \\\\ &amp;= \\mathrm{E~}(S-10)_+ - 5\\Pr (S\\ge 15) \\\\ &amp;= 21.555 - 5 (0.5496) = 18.887 \\end{aligned}\\] \\(\\Box\\) 5.3.3 Analytic Results There are few combinations of claim frequency and severity distributions that result in an easy-to-compute distribution for aggregate losses. This section gives some simple examples. Analysts view these examples as too simple to be used in practice. Example \\(\\#1\\) One has a closed-form expression for the aggregate loss distribution by assuming a geometric frequency distribution and an exponential severity distribution. Assume that claim count \\(N\\) is geometric with parameter \\(\\beta\\) such that \\(\\mathrm{E}(N)=\\beta\\), and that claim amount \\(X\\) is exponential with parameter \\(\\theta\\) such that \\(\\mathrm{E}(X)=\\theta\\). Recall that the pgf of \\(N\\) and the mgf of \\(X\\) are: \\[\\begin{aligned} P_N (z) &amp;=\\frac{1}{1- \\beta (z-1)},\\\\ M_{X}(t) &amp;=\\frac{1}{1-\\theta t}. \\end{aligned}\\] Thus, the mgf of aggregate loss \\(S\\) is \\[\\begin{eqnarray} M_{S}(t) &amp;=&amp; P_N [M_{X}(t)] = \\frac{1}{1 - \\beta \\left( \\frac{1}{1-\\theta t} + 1\\right)} \\nonumber\\\\ &amp;=&amp; 1+ \\frac{1}{1+\\beta} ([1-\\theta(1+\\beta)z]^{-1}-1)...(1)\\\\ &amp;=&amp; \\frac{1}{1+\\beta}(1) +\\frac{\\beta}{1+\\beta} \\left( \\frac{1}{1-\\theta (1+\\beta)t}\\right)...(2) \\end{eqnarray}\\] From (1), we note that \\(S\\) is equivalent to the compound distribution of \\(S=X^{*}_1+\\cdots+X^{*}_{N^{*}}\\), where \\(N^{*}\\) is a Bernoulli with mean \\(\\beta/(1+\\beta)\\) and \\(x^{*}\\) is an exponential with mean \\(\\theta(1+\\beta)\\). To see this, we examine the mgf of \\(S\\): \\[\\begin{aligned} M_{S}(t) = P_N [M_{X}(t)] = P_{N^{*}} [M_{X^{*}}(t)], \\end{aligned}\\] where \\[\\begin{aligned} P_{N^*} (z) &amp;=1+ \\color{blue}{\\frac{\\beta}{1+ \\beta}} (z-1),\\\\ M_{X^*} (t) &amp;=\\frac{1}{1- {\\color{blue}{\\theta(1+\\beta)}} t}. \\end{aligned}\\] From (2), we note that \\(S\\) is also equivalent to a 2-point mixture of 0 and \\(X^{*}\\). Specifically, \\[\\begin{eqnarray*} S &amp;=&amp; \\left\\{ \\begin{array}{cl} 0 &amp; {\\rm with~ probability ~Pr}(N^*=0) = 1/(1+\\beta) \\\\ Y^{*} &amp; {\\rm with~ probability ~Pr}(N^*=1) = \\beta/(1+\\beta) \\end{array} \\right.. \\end{eqnarray*}\\] The distribution function of \\(S\\) is: \\[\\begin{eqnarray*} \\Pr(S=0) &amp;=&amp; \\frac{1}{1+\\beta}\\\\ \\Pr(S&gt;s) &amp;=&amp; \\Pr(X^*&gt;s) =\\frac{\\beta}{1+\\beta} \\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right) \\end{eqnarray*}\\] with pdf \\[\\begin{eqnarray*} f_{S}(s) = \\frac{\\beta}{\\theta (1+\\beta)^2}\\exp\\left( -\\frac{s}{ \\theta (1+\\beta)}\\right). \\end{eqnarray*}\\] \\(\\Box\\) Example \\(\\#2\\) Consider a collective risk model with an exponential severity and an arbitrary frequency distribution. Recall that if If \\(X_i\\sim Exponential(\\theta)\\), then the sum of i.i.d. exponential, \\(S_n=X_1+\\cdots+X_n\\), has a Gamma distribution, i.e. \\(S_n\\sim Gamma(n,\\theta)\\). This has cdf: \\[\\begin{eqnarray*} F_{X}^{\\ast n}(s) &amp;=&amp; \\Pr (S_n \\le s) = \\int_{0}^{s} \\frac{1}{\\Gamma(n)\\theta^n}s^{n-1}\\exp\\left(-\\frac{s}{\\theta}\\right) ds\\\\ &amp;=&amp; 1-\\sum_{j=0}^{n-1}\\frac{1}{j!}\\left( \\frac{s}{\\theta}\\right)^j e^{-s/\\theta } . \\end{eqnarray*}\\] The last equality is derived by integration by parts. For the aggregate loss distribution, we can interchange order of summations to get \\[\\begin{eqnarray*} F_{S}\\left(s\\right) &amp;=&amp; p_{0}+\\sum_{n=1}^{\\infty }p_n F_{X}^{\\ast n}\\left(s\\right)\\\\ &amp;=&amp; 1 - \\sum_{n=1}^{\\infty }p_n \\sum_{j=0}^{n-1}\\frac{1}{j!} \\left( \\frac{s}{\\theta}\\right)^j e^{-s/\\theta }\\\\ &amp;=&amp; 1-e^{-s/\\theta}\\sum_{j=0}^{\\infty} \\frac{1}{j!} \\left( \\frac{s}{\\theta} \\right)^j \\overline{P}_j \\end{eqnarray*}\\] where \\(\\overline{P}_j =p_{j+1}+p_{j+2}+\\cdots = \\Pr (N&gt;j),\\) the ``survival function’’ of the claims count distribution. 5.3.4 Tweedie Distribution In this section, we examine a particular compound distribution where the number of claims is a Poisson distribution and the amount of claims is a Gamma distribution. This specification leads to what is known as a Tweedie distribution. The Tweedie distribution has a mass probability at zero and a continuous component for positive values. Because of this feature, it is widely used in insurance claims modeling, where the zero mass is interpreted as no claims and the positive component as the amount of claims. Specifically, consider the collective risk model \\(S=X_1+\\cdots+X_N\\). Suppose that \\(N\\) has a Poisson distribution with mean \\(\\lambda\\), and each \\(X_i\\) has a Gamma distribution shape parameter \\(\\alpha\\) and scale parameter \\(\\gamma\\). The Tweedie distribution is derived as the Poisson sum of gamma variables. To understand the distribution of \\(S\\), we first examine the mass probability at zero. It is straightforward to see that the aggregate loss is zero when there is no claims occurred, thus: \\[f_S(0)={\\rm Pr}(S=0)= {\\rm Pr}(N=0)=e^{-\\lambda}.\\] In addition, one notes that that \\(S\\) conditional on \\(N_i=n\\), denoted by \\(S_n=X_1+\\cdots+X_n\\), follows a gamma distribution with shape \\(n\\alpha\\) and scale \\(\\gamma_i\\). Thus, for \\(s&gt;0\\), the density of a Tweedie distribution can be calculated as \\[\\begin{aligned} f_S(s)&amp;=\\sum_{n=1}^{+\\infty} p_n f_{S_n}(s)\\\\ &amp;=\\sum_{n=1}^{\\infty}e^{-\\lambda_i}\\frac{(\\lambda_i)^n}{n!}\\frac{1}{\\gamma^{n\\alpha}}y^{n\\alpha-1}e^{-y\\gamma} \\end{aligned}\\] Thus, the Tweedie distribution can be thought of a mixture of zero and a positive valued distribution, which makes it a convenient tool for modeling insurance claims and for calculating pure premiums. The mean and variance of the Tweedie compound Poisson model are: \\[{\\rm E} (S)=\\lambda\\frac{\\alpha}{\\gamma}~~~~{\\rm and}~~~~{\\rm Var} (S)=\\lambda\\frac{\\alpha(1+\\alpha)}{\\gamma^2}.\\] As another important feature, the Tweedie distribution is a special case of exponential dispersion models, a class of models used to describe the random component in generalized linear models. To see this, we consider the following reparameterizations: \\[\\begin{equation*} \\lambda=\\frac{\\mu^{2-p}}{\\phi(2-p)},~~~~\\alpha=\\frac{2-p}{p-1},~~~~\\gamma=\\phi(p-1)\\mu^{p-1} \\end{equation*}\\] With the above relationships, one can show that the distribution of \\(S\\) is \\[f_S(s)=\\exp\\left[\\frac{1}{\\phi}\\left(\\frac{-s}{(p-1)\\mu^{p-1}}-\\frac{\\mu^{2-p}}{2-p}\\right)+C(s;\\phi)\\right]\\] where \\[\\begin{equation*} C(s;\\phi/\\omega_i)=\\left\\{\\begin{array}{ll} \\displaystyle 0 &amp; {\\rm if}~ y=0 \\\\ \\displaystyle \\ln \\sum\\limits_{n\\ge 1} \\left\\{\\frac{(1/\\phi)^{1/(p-1)}y^{(2-p)/(p-1)}}{(2-p)(p-1)^{(2-p)/(p-1)}}\\right\\}^{n}\\frac{1}{n!\\Gamma(n(2-p)/(p-1))s} &amp; {\\rm if}~ y&gt;0 \\end{array}\\right. \\end{equation*}\\] Hence, the distribution of \\(S\\) belongs to the exponential family with parameters \\(\\mu\\), \\(\\phi\\), and \\(p\\in(1,2)\\), and we have \\[{\\rm E} (S)=\\mu~~~~{\\rm and}~~~~{\\rm Var} (S)=\\phi\\mu^{p}\\] It is also worth mentioning the two limiting cases of the Tweedie model: \\(p\\rightarrow 1\\) results in the Poisson distribution and \\(p\\rightarrow 2\\) results in the gamma distribution. The Tweedie compound Poisson model accommodates the situations in between. 5.4 Computing the Aggregate Claims Distribution Computing the distribution of aggregate losses is a difficult, yet important, problem. As we have seen, for both individual risk model and collective risk model, computing the distribution involves the evaluation of a \\(n\\)-fold convolution. To make the problem tractable, one strategy is to use a distribution that is easy to evaluate to approximate the aggregate loss distribution. For instance, normal distribution is a natural choice based on central limit theorem where parameters of the normal distribution can be estimated by matching the moments. This approach has its strength and limitations. The main advantage is the ease of computation. The disadvantage are: first, the size and direction of approximation error are unknown; second, the approximation may fail to capture some special features of the aggregate loss such as mass point at zero. This section discusses two practical approaches to computing the distribution of aggregate loss, the recursive method and the simulation. 5.4.1 Recursive Method The recursive method applies to compound models where the frequency component \\(N\\) belongs to either \\((a,b,0)\\) or \\((a,b,1)\\) class and the severity component \\(X\\) has a discrete distribution. For continuous \\(X\\), a common practice is to first discretize the severity distribution and then the recursive method is ready to apply. Assume that \\(N\\) is in the \\((a,b,1)\\) class so that \\(p_{k}=\\left( a+\\frac{b}{k} \\right) p_{k-1}, k = 2,3,\\ldots\\). Further assume that the support of \\(X\\) is \\(\\{0,1,\\ldots,m\\}\\), discrete and finite. Then, the probability function of \\(S\\) is: \\[\\begin{aligned} f_{S}(s)&amp;=\\Pr (S=s) \\\\ &amp;=\\frac{1}{1-af_{X}(0)}\\left\\{ \\left[ p_1 -(a+b)p_{0}\\right] f_X (s)+\\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx}{s} \\right) f_X (x)f_{S}(s-x)\\right\\}. \\end{aligned}\\] If \\(N\\) is in the \\((a,b,0)\\) class, then \\(p_1=(a+b)p_0\\) and so \\[ f_S(s)=\\frac{1}{1-af_X (0)}\\left\\{ \\sum_{x=1}^{s\\wedge m}\\left( a+\\frac{bx }{s}\\right) f_X (x)f_{S}(s-x)\\right\\}. \\] Special Case: If \\(N \\sim\\) Poisson with mean \\(\\lambda\\), then \\(a=0\\) and \\(b=\\lambda\\), thus \\[ f_{S}(s)=\\frac{\\lambda }{s}\\left\\{ \\sum_{x=1}^{s \\wedge m} x f_X (x) f_S (s-x)\\right\\} . \\] Example. SOA Fall 2002, 36. The number of claims in a period \\(N\\) has a geometric distribution with mean 4. The amount of each claim \\(X\\) follows \\({\\rm Pr} (X = x) = 0.25\\), for \\(x = 1,2,3,4\\). The number of claims and the claim amount are independent. \\(S\\) is the aggregate claim amount in the period. Calculate \\(F_S(3)\\). Solution. The severity distribution \\(X\\) follows \\[f_X (x) = \\frac{1}{4}, \\ \\ x=1, 2, 3, 4.\\] The frequency distribution \\(N\\) is geometric with mean 4, which is a member of the \\((a,b,0)\\) class with \\(b=0\\), \\(a=\\frac{\\beta}{1+\\beta} = \\frac{4}{5}\\), and \\(p_0 = \\frac{1}{1+\\beta} = \\frac{1}{5}\\). Thus, we can use the recursive method \\[\\begin{eqnarray*} f_S (x) &amp;=&amp; 1 \\sum_{y=1}^{x\\wedge m} (a+0) f_X (y) f_S (x-y) \\\\ &amp;=&amp; \\frac{4}{5} \\sum_{y=1}^{x\\wedge m} f_X (y) f_S (x-y) \\end{eqnarray*}\\] Specifically, we have \\[\\begin{eqnarray*} f_S (0) &amp;=&amp; \\Pr(N=0) = p_0=\\frac{1}{5}\\\\ f_S (1) &amp;=&amp; \\frac{4}{5}\\sum_{y=1}^{1} f_X (y) f_S (1-y) = \\frac{4}{5} f_X(1) f_S(0)\\\\ &amp;=&amp; \\frac{4}{5}\\left( \\frac{1}{4}\\right)\\left(\\frac{1}{5} \\right) = \\frac{1}{25}\\\\ f_S (2) &amp;=&amp; \\frac{4}{5}\\sum_{y=1}^{2} f_X (y) f_S (2-y) = \\frac{4}{5} \\left[ f_X(1)f_S(1) + f_X(2) f_S(0) \\right] \\\\ &amp;=&amp; \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5}\\right) \\right] = \\frac{4}{5}\\left( \\frac{6}{100}\\right) = \\frac{6}{125}\\\\ f_S (3) &amp;=&amp; \\frac{4}{5} \\left[ f_X(1) f_S(2) + f_X(2)f_S(1) + f_X(3) f_S(0) \\right]\\\\ &amp;=&amp; \\frac{4}{5}\\left[ \\frac{1}{4} \\left( \\frac{1}{25} + \\frac{1}{5} + \\frac{6}{125}\\right) \\right] = \\frac{1}{5}\\left( \\frac{5+25+6}{125}\\right) = 0.0576\\\\ \\Rightarrow \\ F_S (3) &amp;=&amp; f_S (0)+f_S (1)+f_S (2)+f_S (2)+f_S (3) = 0.3456 \\end{eqnarray*}\\] \\(\\Box\\) 5.4.2 Simulation The distribution of aggregate loss can be evaluated using Monte Carlo simulation. The idea is one can calculate the empirical distribution of \\(S\\) using a random sample. Blow we summarize the simulation procedures for the aggregate loss models. Individual Risk Model \\(S_n=X_1+\\cdots+X_n\\) For each \\(X_i\\), \\(i=1,\\ldots,n\\), generate random sample of size \\(m\\), denoted by \\(x_{ij}~(j=1,\\ldots,m)\\); Calculate the aggregate loss \\(s_j=x_{1j}+\\ldots+x_{nj}\\) for \\(j=1,\\ldots,m\\); We obtain a random sample of \\(S\\), i.e. \\(\\{s_1,\\ldots,s_m\\}\\). Collective Risk Model \\(S=Y_1+\\cdots+Y_N\\) Generate the number of claims \\(n_j\\) from frequency distribution \\(N\\); Given \\(n_j\\), generate the amount of claims for each claim independently from \\(Y\\), denoted by \\(y_{1},\\ldots,y_{n_j}\\); Calculate the aggregate loss \\(s_j=y_{1j}+\\ldots+y_{n_j}\\); Repeat the above three steps for \\(j=1,\\ldots,m\\); We obtain a random sample of \\(S\\), i.e. \\(\\{s_1,\\ldots,s_m\\}\\). Given the random sample of \\(S\\), the empirical distribution can be calculated as \\[\\hat{F}_S(s)=\\frac{1}{m}\\sum_{i=1}^{m}I(s_i\\leq s),\\] where \\(I(\\cdot)\\) is an indicator function. The empirical distribution \\(\\hat{F}_S(s)\\) will converge to \\({F}_S(s)\\) almost surely as \\(m\\rightarrow \\infty\\). The above procedure assumes that the parameters of the frequency and severity distributions are known. In practice, one would need to estimate these parameters from the data. For instance, the assumptions in the collective risk model suggest a two-stage estimation where a model is developed for the number of claims \\(N\\) from the data on claim counts and a model is developed for the severity of claims \\(X\\) from the data on the amount of claims. 5.5 Effects of Coverage Modifications 5.5.1 Impact of Exposure on Frequency This section focuses on an individual risk model for claim counts. Consider the number of claims from a group of \\(n\\) policies: \\[S=X_1+\\cdots+X_n\\] where we assume \\(X_i\\) are i.i.d. representing the number of claims from policy \\(i\\). In this case, the exposure for the portfolio is \\(b\\) using policy as exposure base. The pgf of \\(S\\) is \\[\\begin{aligned} P_S(z)&amp;={\\rm E}(z^S)={\\rm E}\\left(z^{\\sum_{i=1}^nX_i}\\right)\\\\ &amp;=\\prod_{i=1}^n{\\rm E}(z^{X_i})=[P_X(z)]^n \\end{aligned}\\] Special Case Poisson. If \\(X_i\\sim Poisson(\\lambda)\\), its pgf is \\(P_X(z)=e^{\\lambda(z-1)}\\). Then the pgf of \\(S\\) is \\[P_S(z)=[e^{\\lambda(z-1)}]^n=e^{n\\lambda(z-1)}.\\] So \\(S\\sim Poisson(n\\lambda)\\). Special Case Negative binomial. If \\(X_i\\sim NegBin(\\beta,r)\\), its pgf is \\(P_X(z)=[1-\\beta(z-1)]^{-r}\\). Then the pgf of \\(S\\) is \\[P_S(z)=[[1-\\beta(z-1)]^{-r}]^n=[1-\\beta(z-1)]^{-nr}.\\] So \\(S\\sim NB(\\beta,nr)\\). Example. Assume that the number of claims for each vehicle is Poisson with mean \\(\\lambda\\). Given the following data on the observed number of claims for each household, calculate the MLE of \\(\\lambda\\). \\[\\begin{matrix} \\begin{array}{|c|c|c|} \\hline \\text{Household ID} &amp; \\text{Number of vehicles} &amp; \\text{Number of claims} \\\\ \\hline 1 &amp; 2 &amp; 0 \\\\ 2 &amp; 1 &amp; 2 \\\\ 3 &amp; 3 &amp; 2 \\\\ 4 &amp; 1 &amp; 0 \\\\ 5 &amp; 1 &amp; 1 \\\\ \\hline \\end{array} \\end{matrix}\\] Solution. Each of the 5 households has number of exposures \\(b_j\\) (number of vehicles) and number of claims \\(S_j\\), \\(j=1,...,5\\). Note for each household, the number of claims \\(S_j \\sim Poisson (b_j \\lambda)\\). The likelihood function is \\[\\begin{aligned} L(\\lambda) &amp;= \\prod_{j=1}^5 \\Pr(S_j=s_j) = \\prod_{j=1}^5 \\frac{e^{-b_j\\lambda} (b_j \\lambda)^{s_j}}{s_j!} \\\\ &amp;= \\left(\\frac{e^{-2\\lambda} (2 \\lambda)^{0}}{0!} \\right) \\left(\\frac{e^{-1\\lambda} (1 \\lambda)^{2}}{2!} \\right) \\left(\\frac{e^{-3\\lambda} (3 \\lambda)^{2}}{2!} \\right) \\left(\\frac{e^{-1\\lambda} (1 \\lambda)^{0}}{0!} \\right) \\left(\\frac{e^{-1\\lambda} (1 \\lambda)^{1}}{1!} \\right) \\\\ &amp;\\propto e^{-8\\lambda} \\lambda^5 \\end{aligned}\\] Taking the log-likehood, we have \\[\\begin{aligned} l(\\lambda) = \\log L(\\lambda) = -8\\lambda + 5\\log(\\lambda) \\end{aligned}\\] Setting the first derivative of the log-likehood to 0, we have \\[\\begin{aligned} &amp;l&#39;(\\lambda) = -8 + \\frac{5}{\\lambda} = 0 \\\\ \\Rightarrow \\ &amp; 8 = \\frac{5}{\\hat{\\lambda}} \\ \\Rightarrow \\ \\hat{\\lambda} = \\frac{5}{8} \\end{aligned}\\] \\(\\Box\\) If the exposure of the portfolio change from \\(n_1\\) to \\(n_2\\), we can establish the following relation between the aggregate claim counts: \\[P_{S_2}(z)=[P_X(z)]^{n_2}=[P_X(z)^{n_1}]^{n_2/n_1}=P_{S_1}(z)^{n_2/n_1}.\\] 5.5.2 Impact of Deductibles on Claim Frequency This section examine the effect of deductible on claim frequency. Intuitively, there will be fewer claims filed when a policy deductible is imposed because a loss below deductible might not result in a claim. Even if an insured does file a claim, this may not result in a payment by the policy, since the claim may be denied or the loss amount may ultimately be determined to be below deductible. Let \\(N^L\\) denote the number of loss (i.e. the number of claims with no deductible), and \\(N^P\\) denote the number of payments when a deductible \\(d\\) is imposed. Our goal is to identify the distribution of \\(N^P\\) given the distribution of \\(N^L\\). We show below that the relationship between \\(N^L\\) and \\(N^P\\) can be established within an aggregate risk model framework. Note that sometimes changes in deductible will affect policyholder behavior. We assume that this is not the case, i.e. the distribution of losses for both frequency and severity remain unchanged when the deductible changes. Given there are \\(N^L\\) losses, let \\(X_1,X_2\\ldots,X_{N^L}\\) be the associated amount of losses. For \\(j=1,\\ldots,N^L\\), define \\[\\begin{eqnarray*} I_j&amp;=&amp; \\left \\{ \\begin{array}{cc} 1 &amp; \\text{if} ~X_j&gt;d\\\\ 0 &amp; \\text{otherwise}\\\\ \\end{array} \\right.. \\end{eqnarray*}\\] Then we establish \\[N^P=I_1+I_2+\\cdots+I_{N_L}.\\] Note that conditioning on \\(N^L\\), the distribution of \\(N^P \\sim Binomial (N^L, v)\\), where \\(v=\\Pr(X&gt;d)\\). Thus, given \\(N^L\\), \\[\\begin{eqnarray*} \\mathrm{E}\\left(z^{N^P}|N^L\\right)&amp;=&amp;\\left[ 1+v(z-1)\\right]^{N^L} \\end{eqnarray*}\\] So the p.g.f. of \\(N^P\\) is \\[\\begin{eqnarray*} P_{N^P}(z)&amp;=&amp;\\mathrm{E}_{N^P}\\left(z^{N^P}\\right)=\\mathrm{E}_{N^L}\\left[\\mathrm{E}_{N^P}\\left(z^{N^P}|N^L\\right)\\right]\\\\ &amp;=&amp;\\mathrm{E}_{N^L}\\left[(1+v(z-1))^{N^L}\\right]\\\\ &amp;=&amp;P_{N^L}\\left(1+v(z-1)\\right) \\end{eqnarray*}\\] Thus, we can write the pgf of \\(N^P\\) as the pgf of \\(N^L\\), evaluated at a new argument \\(z^* = 1+v(z-1)\\), that is, \\(P_{N^P}(z)=P_{N^L}(z^*)\\). Special Cases: \\(N^L\\sim Poisson (\\lambda)\\). The pgf of \\(N^L\\) is \\(P_{N^L}=\\exp(\\lambda(z-1))\\). Thus the pgf of \\(N^P\\) is \\[\\begin{eqnarray*} P_{N^P}(z)&amp;=&amp;\\exp\\left( \\lambda(1+v(z-1)-1)\\right)\\\\ &amp;=&amp;\\exp(\\lambda v(z-1))\\sim Poisson (\\lambda v) \\end{eqnarray*}\\] So the payment number has the same distribution as the loss number but with the expected number of payments equal to \\(\\lambda v = \\lambda \\Pr(X&gt;d)\\). \\(N^L \\sim NegBin(\\beta, r)\\). The pgf of \\(N^L\\) is \\(P_{N^{L}}\\left( z\\right) =\\left[ 1-\\beta \\left( z-1\\right)\\right]^{-r}\\). \\[\\begin{eqnarray*} P_{N^P}(z)&amp;=&amp;\\left( 1-\\beta (1+v(z-1)-1)\\right)^{-r}\\\\ &amp;=&amp;\\left( 1-\\beta v(z-1)\\right)^{-r} \\:\\:\\:\\sim NegBin(\\beta v, r) \\end{eqnarray*}\\] So the payment number has the same distribution as the loss number but with parameters \\(\\beta v\\) and \\(r\\). Example. Suppose that loss amounts \\(X_i\\sim Pareto(\\alpha=4,\\ \\theta=150)\\). You are given that the loss frequency is \\(N^L\\sim Poisson(\\lambda)\\) and the payment frequency distribution \\(N^{P_1}\\sim Poisson (0.4)\\) with \\(d_1=30\\). Find the distribution of \\(N^{P_2}\\) with \\(d_2=100\\). Solution. Because the loss frequency \\(N^L\\) is Poisson, we can relate the means of the loss distribution \\(N^L\\) and the first payment distribution \\(N^{P_1}\\) as \\(0.4 = v_1 \\lambda\\), where \\[\\begin{aligned} &amp;v_1 = \\Pr(X &gt; 30) = \\left( \\frac{150}{30+150}\\right)^4=\\left( \\frac{5}{6}\\right)^4 \\\\ \\Rightarrow \\ &amp; \\lambda = 0.4 \\left( \\frac{6}{5} \\right)^4 \\end{aligned}\\] With this, we can assess the second payment distribution \\(N^{P_2}\\) as being Poisson with mean \\(\\lambda_2 = \\lambda v_2\\), where \\[\\begin{aligned} &amp; v_2 = \\Pr(X&gt;100)=\\left( \\frac{150}{100+150}\\right)^4=\\left( \\frac{3}{5}\\right)^4 \\\\ \\Rightarrow \\ &amp; \\lambda_2 = \\lambda v_2 = 0.4\\left( \\frac{6}{5} \\right)^4 \\left( \\frac{3}{5} \\right)^4 = 0.1075 \\end{aligned}\\] \\(\\Box\\) Follow-Up. Now suppose instead that the loss frequency is \\(N^L \\sim NegBin(\\beta,\\ r)\\) and for deductible \\(d_1=30\\), the payment frequency \\(N^{P_1}\\) is negative binomial with mean \\(0.4\\). Find the mean of the payment frequency \\(N^{P_2}\\) with deductible \\(d_2=100\\). Solution. Because the loss frequency \\(N^L\\) is negative binomial, we can relate the parameter \\(\\beta\\) of the \\(N^L\\) distribution and the parameter \\(\\beta_1\\) of the first payment distribution \\(N^{P_1}\\) using \\(\\beta_1 = \\beta v_1\\), where \\[v_1 = \\Pr(X &gt; 30) = \\left( \\frac{5}{6} \\right)^4\\] Thus, the mean of \\(N^{P_1}\\) and the mean of \\(N^L\\) are related \\[\\begin{aligned} &amp;0.4 = r \\beta_1 = r \\left(\\beta v_1\\right) \\\\ \\Rightarrow \\ &amp; r\\beta = \\frac{0.4}{v_1} = 0.4 \\left(\\frac{6}{5} \\right)^4 \\end{aligned}\\] Note that \\(v_2 = \\Pr(X &gt; 100) = \\left( \\frac{3}{5}\\right)^4\\) as in the original question. Then the second payment frequency distribution is \\(N^{P_2} \\sim NegBin(\\beta v_2, \\ r)\\) with mean \\[\\begin{aligned} r (\\beta v_2) = (r \\beta) v_2 = 0.4 \\left( \\frac{6}{5}\\right)^4 \\left( \\frac{3}{5} \\right)^4 = 0.1075 \\end{aligned}\\] \\(\\Box\\) Next we examine the more general case where \\(N^L\\) is a zero-modified distribution. Recall that a modified distribution is defined in terms of an unmodified one. That is, \\[\\begin{aligned} p_k^M = c~p_k^0, {~\\rm for~} k=1,2,3,\\ldots, {~\\rm with~}c = \\frac{1-p_0^M}{1-p_0^0}. \\end{aligned}\\] In the case that \\(p_0^M=0\\), we call this a ``truncated’’ distribution at zero, or \\(ZT\\). For other arbitrary values of \\(p_0^M\\), this is a zero-modified, or \\(ZM\\), distribution. The pgf for the modified distribution is shown as \\[\\begin{eqnarray*} P^M(z) = 1-c+c~P^0(z). \\end{eqnarray*}\\] When \\(N^L\\) follows a zero-modified distribution, the distribution of \\(N^P\\) is established using the same relation \\(P_{N^P}(z)=P_{N^L}\\left(1+v(z-1)\\right)\\). Special Cases: \\(N^{L}\\) is a ZM-Poisson with parameters \\(\\lambda\\) and \\(p_0^{M}\\). The pgf of \\(N^L\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}+\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}\\exp[\\lambda(z-1)].\\] Thus the pgf of \\(N^P\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}+\\cfrac{1-p_0^{M}}{1-\\exp(-\\lambda)}\\exp[\\lambda v(z-1)].\\] So the number of payments is also a ZM-Poisson distribution with parameters \\(\\lambda v\\) and \\(p_0^{M}\\). The probability at zero can be evaluated using \\({\\rm Pr}(N^P=0) = P_{N^P}(0)\\). \\(N^{L}\\) is a ZM-NegBin with parameters \\(\\beta\\), \\(r\\), and \\(p_0^{M}\\). The pgf of \\(N^L\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}+\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}\\left[ 1-\\beta \\left( z-1\\right)\\right]^{-r}.\\] Thus the pgf of \\(N^P\\) is \\[P_{N^{L}}(z)=1-\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}+\\cfrac{1-p_0^{M}}{1-(1+\\beta)^{-r}}\\left[ 1-\\beta v\\left( z-1\\right)\\right]^{-r}.\\] So the number of payments is also a ZM-NegBin distribution with parameters \\(\\beta v\\), \\(r\\), and \\(p_0^{M}\\). Similarly, the probability at zero can be evaluated using \\({\\rm Pr}(N^P=0) = P_{N^P}(0)\\). Example. Aggregate losses are modeled as follows: (i) The number of losses follows a zero-modified Poisson distribution with \\(\\lambda=3\\) and \\(p_0^M = 0.5\\). (ii) The amount of each loss has a Burr distribution with \\(\\alpha=3, \\theta=50, \\gamma=1\\). (iii) There is a deductible of \\(d=30\\) on each loss. (iv) The number of losses and the amounts of the losses are mutually independent. Calculate \\(\\mathrm{E~} N^P\\) and \\(\\mathrm{Var~} N^P\\). Solution. Since \\(N^L\\) follows a ZM-Poisson distribution with parameters \\(\\lambda\\) and \\(p_0^M\\), we know that \\(N^P\\) also follows a ZM-Poisson distribution, but with parameters \\(\\lambda v\\) and \\(p_0^M\\), where \\[v = \\Pr(X&gt;30) = \\left( \\frac{1}{1+(30/50)} \\right)^3 = 0.2441\\] Thus, \\(N^P\\) follows a ZM-Poisson distribution with parameters \\(\\lambda^\\ast = \\lambda v= 0.7324\\) and \\(p_0^M = 0.5\\). Finally, \\[\\begin{aligned} \\mathrm{E~} N^P &amp;= (1-p_0^M) \\frac{\\lambda^\\ast}{1-e^{-\\lambda^\\ast}} = 0.5 \\left( \\frac{0.7324}{1-e^{-0.7324}} \\right) \\\\ &amp;= 0.7053 \\\\ \\mathrm{Var~} N^P &amp;= (1-p_0^M) \\left( \\frac{\\lambda^\\ast[1-(\\lambda^\\ast + 1) e^{-\\lambda^\\ast}]}{(1-e^{-\\lambda^\\ast})^2} \\right) + p_0^M(1-p_0^M) \\left(\\frac{\\lambda^\\ast}{1-e^{-\\lambda^\\ast}} \\right)^2 \\\\ &amp;= 0.5 \\left( \\frac{0.7324(1-1.7324 e^{-0.7324})}{(1-e^{-0.7324})^2} \\right) + 0.5^2 \\left( \\frac{0.7324}{1-e^{-0.7324}} \\right)^2 \\\\ &amp;= 0.7244 \\end{aligned}\\] \\(\\Box\\) 5.5.3 Impact of Policy Modifications on Aggregate Claims In this section, we examine how the change in deductibles affect aggregate payments from an insurance portfolio. We assume that policy limits, coinsurance, and inflation have no effect on the frequency of payments made by an insurer. As in the previous section, we further assume that deductible changes do not impact the distribution of losses for both frequency and severity. Recall the notation \\(N^L\\) for the number of losses. With ground-up loss \\(X\\) and policy deductible \\(d\\), we use \\(N^P = I(X_1&gt;d) + \\cdots + I(X_{N^L}&gt;d)\\) for the number of payments. Also, define the amount of payment on a per-loss basis as \\[\\begin{eqnarray*} X^{L}&amp;=\\left\\{ \\begin{array}{cc} 0 &amp; X&lt;\\cfrac{d}{1+r} \\\\ \\alpha[(1+r)X-d] &amp; \\cfrac{d}{1+r}\\leq X&lt;\\cfrac{u}{1+r} \\\\ \\alpha(u-d) &amp; X \\ge \\cfrac{u}{1+r}\\\\ \\end{array} \\right., \\end{eqnarray*}\\] and the the amount of payment on a per-payment basis as \\[\\begin{eqnarray*} X^{P}&amp;=\\left\\{ \\begin{array}{cc} {\\rm Undefined} &amp; X&lt;\\cfrac{d}{1+r} \\\\ \\alpha[(1+r)X-d] &amp; \\cfrac{d}{1+r}\\leq X&lt;\\cfrac{u}{1+r} \\\\ \\alpha(u-d) &amp; X \\ge \\cfrac{u}{1+r}\\\\ \\end{array} \\right.. \\end{eqnarray*}\\] In the above, \\(r\\), \\(u\\), \\(\\alpha\\) represents the inflation rate, policy limit, and coinsurance, respectively. Hence, aggregate costs (payment amounts) can be expressed either on a per loss or per payment basis: \\[\\begin{eqnarray*} S &amp;=&amp; X^L_1 + \\cdots + X^L_{N^L} \\\\ &amp;=&amp;X^P_1 + \\cdots + X^P_{N^P} . \\end{eqnarray*}\\] The fundamentals regarding collective risk models are ready to apply. For instance, we have: \\[\\begin{aligned} {\\rm E}(S) &amp;= {\\rm E}\\left(N^L\\right) {\\rm E}\\left(X^L\\right) = {\\rm E}\\left(N^P\\right) {\\rm E}\\left(X^P\\right)\\\\ {\\rm Var}(S) &amp;= {\\rm E}\\left(N^L\\right) {\\rm Var}\\left(X^L\\right) + \\left[{\\rm E}\\left(X^L\\right)\\right]^2 {\\rm Var}(N^L) \\\\ &amp;= {\\rm E}\\left(N^P\\right) {\\rm Var}\\left(X^P\\right) + \\left[{\\rm E}\\left(X^P\\right)\\right]^2 {\\rm Var}(N^P)\\\\ M_S(z)&amp;=P_{N^L}\\left[M_{X^L}(z)\\right]=P_{N^P}\\left[M_{X^P}(z)\\right] \\end{aligned}\\] Example. Course 3, November 2001, 6. A group dental policy has a negative binomial claim count distribution with mean 300 and variance 800. Ground-up severity is given by the following table: \\[\\begin{matrix} \\begin{array}{ c | c } \\hline \\text{Severity} &amp; \\text{Probability}\\\\ \\hline 40 &amp; 0.25\\\\ 80 &amp; 0.25\\\\ 120 &amp; 0.25\\\\ 200 &amp; 0.25\\\\ \\hline \\end{array} \\end{matrix}\\] You expect severity to increase 50% with no change in frequency. You decide to impose a per claim deductible of 100. Calculate the expected total claim payment after these changes. Solution. The cost per loss with a 50% increase in severity and a 100 deductible per claim is \\[\\begin{eqnarray*} Y^L &amp;=&amp; \\left\\{ \\begin{array}{cc} 0 &amp; 1.5x&lt;100 \\\\ 1.5x-100 &amp; 1.5x\\ge 100\\\\ \\end{array} \\right. \\end{eqnarray*}\\] This has expectation \\[\\begin{aligned} \\mathrm{E~}Y^L &amp;= \\frac{1}{4} \\left[ \\left(1.5(40)-100\\right)_+ + \\left(1.5(80)-100\\right)_+ + \\left(1.5(120)-100\\right)_+ + \\left(1.5(200)-100\\right)_+ \\right] \\\\ &amp;= \\frac{1}{4}\\left[ (60-100)_+ + (120-100)_+ + (180-100)_+ + (300-100)_+\\right] \\\\ &amp;= \\frac{1}{4}\\left[ 0 + 20 + 80 + 200 \\right] = 75 \\end{aligned}\\] Thus, the expected aggregate loss is \\[\\mathrm{E~}S=(\\mathrm{E~}N) \\left( \\mathrm{E~}Y^L \\right)= 300 (75) = 22,500 .\\] Follow-Up. What is \\(\\mathrm{Var~}S\\)? On a per loss basis, we have \\[\\begin{aligned} \\mathrm{Var~}S &amp;= \\left(\\mathrm{E~}N \\right) \\left( \\mathrm{Var~} Y^L \\right) + \\left[ \\mathrm{E~} Y^L \\right]^2 \\left(\\mathrm{Var~} N \\right) \\end{aligned}\\] where \\(\\mathrm{E~}N = 300\\) and \\(\\mathrm{Var~} N = 800\\). We find \\[\\begin{aligned} &amp;\\mathrm{E} \\left[ (Y^L)^2 \\right] = \\frac{1}{4} \\left[ 0^2 + 20^2 + 80^2 + 200^2 \\right] = 11,700 \\\\ \\Rightarrow \\ &amp; \\mathrm{Var~} Y^L = \\mathrm{E} \\left[ (Y^L)^2 \\right] - \\left( \\mathrm{E~}Y^L \\right)^2 = 11,700 - 75^2 = 6,075 \\end{aligned}\\] Thus, the variance of the aggregate claim payment is \\[\\begin{eqnarray*} \\mathrm{Var~}S &amp;=&amp; 300(6,075) + 75^2 (800) = 6,322,500 \\end{eqnarray*}\\] \\(\\Box\\) Alternative Method: Using the Per Payment Basis. Previously, we calculated the expected total claim payment by multiplying the expected number of losses by the expected payment per loss. Recall that we can also multiply the expected number of payments by the expected payment per payment. In this case, we have \\[S=Y_1^P + \\cdots + Y_{N_P}^P \\] The probability of a payment is \\[v=\\Pr(1.5X \\ge 100)=\\Pr(X \\ge 66.\\bar{6})=\\frac{3}{4} .\\] Thus, the number of payments, \\(N^P\\) has a negative binomial distribution with mean \\[\\mathrm{E~}N^P=300 \\left(\\frac{3}{4} \\right)=225\\] The cost per payment is \\[\\begin{eqnarray*} Y^P &amp;=&amp; \\left\\{ \\begin{array}{cc} \\text{undefined} &amp; 1.5x&lt;100 \\\\ 1.5x-100 &amp; 1.5x\\ge 100\\\\ \\end{array} \\right. \\end{eqnarray*}\\] This has expectation \\[\\mathrm{E~}Y^P=\\frac{\\mathrm{E~}Y^L}{\\Pr(1.5X &gt; 100)}= \\frac{\\mathrm{E~}Y^L}{v}=\\frac{75}{(3/4)}=100\\] Thus, as before, the expected aggregate loss is \\[\\mathrm{E~}S=\\left(\\mathrm{E~}Y^P\\right) \\left(\\mathrm{E~}N^P\\right) = 100(225)=22,500\\] \\(\\Box\\) Example. SOA Sample Question, 109. A company insures a fleet of vehicles. Aggregate losses have a compound Poisson distribution. The expected number of losses is 20. Loss amounts, regardless of vehicle type, have exponential distribution with \\(\\theta=200\\). To reduce the cost of the insurance, two modifications are to be made: (i) A certain type of vehicle will not be insured. It is estimated that this will reduce loss frequency by 20\\(\\%\\). (ii) A deductible of 100 per loss will be imposed. Calculate the expected aggregate amount paid by the insurer after the modifications. Solution. On a per loss basis, we have a 100 deductible. Thus, the expectation per loss is \\[\\begin{aligned} \\mathrm{E~} Y^L &amp;= E[(X-100)_+] = E(X) - E(X\\wedge 100) \\\\ &amp;= 200 - 200(1-e^{-100/200}) = 121.31 \\end{aligned}\\] Loss frequency has been reduced by 20\\(\\%\\), resulting in an expected number of losses \\[\\mathrm{E~}N^L = 0.8(20) = 16\\] Thus, the expected aggregate amount paid after the modifications is \\[\\mathrm{E~}S = \\left(\\mathrm{E~}Y^L \\right) \\left( \\mathrm{E~} N^L\\right) = 121.31(16) = 1,941\\] \\(\\Box\\) Alternative Method: Using the Per Payment Basis. We can also use the per payment basis to find the expected aggregate amount paid after the modifications. For the per payment severity, \\[\\begin{aligned} \\mathrm{E~} Y^P = \\frac{\\mathrm{E~} Y^L}{\\Pr(X &gt; 100)} = \\frac{200 - 200(1-e^{-100/200})}{e^{-100/200}} = 200 \\end{aligned}\\] This is not surprising – recall that the exponential distribution is memoryless, so the expected claim amounts paid in excess of 100 is still exponential with mean 200. Now we look at the payment frequency. With the deductible of 100, the probability that a payment occurs is \\(\\Pr(X &gt; 100) = e^{-100/200}\\) Thus, \\[\\mathrm{E~} N^P = 16 e^{-100/200} = 9.7\\] Putting this together, we produce the same answer using the per payment basis as the per loss basis from earlier \\[\\mathrm{E~}S = \\left( \\mathrm{E~} Y^P \\right) \\left( \\mathrm{E~} N^P \\right) = 200(9.7) = 1,941\\] \\(\\Box\\) "],
["simulation-1.html", "Chapter 6 Simulation 6.1 Generating Independent Uniform Observations 6.2 Inverse Transform 6.3 How Many Simulated Values?", " Chapter 6 Simulation Simulation is a computer-based, computationally intensive, method of solving difficult problems, such as analyzing business processes. Instead of creating physical processes and experimenting with them in order to understand their operational characteristics, a simulation study is based on a computer representation - it considers various hypothetical conditions as inputs and summarizes the results. Through simulation, a vast number of hypothetical conditions can be quickly and inexpensively examined. Performing the same analysis with a physical system is not only expensive and time-consuming but, in many cases, impossible. A drawback of simulation is that computer models are not perfect representations of business processes. There are three basic steps for producing a simulation study: Generating approximately independent realizations that are uniformly distributed Transforming the uniformly distributed realizations to observations from a probability distribution of interest With the generated observations as inputs, designing a structure to produce interesting and reliable results. Designing the structure can be a difficult step, where the degree of difficulty depends on the problem being studied. There are many resources, including this tutorial, to help the actuary with the first two steps. 6.1 Generating Independent Uniform Observations We begin with a historically prominent method. Linear Congruential Generator. To generate a sequence of random numbers, start with \\(B_0\\), a starting value that is known as a “seed.” Update it using the recursive relationship \\[B_{n+1} = a B_n + c \\text{ modulo }m, ~~ n=0, 1, 2, \\ldots .\\] This algorithm is called a linear congruential generator. The case of \\(c=0\\) is called a multiplicative congruential generator; it is particularly useful for really fast computations. For illustrative values of \\(a\\) and \\(m\\), Microsoft’s Visual Basic uses \\(m=2^{24}\\), \\(a=1,140,671,485\\), and \\(c = 12,820,163\\) (see http://support.microsoft.com/kb/231847). This is the engine underlying the random number generation in Microsoft’s Excel program. The sequence used by the analyst is defined as \\(U_n=B_n/m.\\) The analyst may interpret the sequence {\\(U_{i}\\)} to be (approximately) identically and independently uniformly distributed on the interval (0,1). To illustrate the algorithm, consider the following. Example. Take \\(m=15\\), \\(a=3\\), \\(c=2\\) and \\(B_0=1\\). Then we have: step \\(n\\) \\(B_n\\) \\(U_n\\) 0 \\(B_0=1\\) 1 \\(B_1 =\\mod(3 \\times 1 +2) = 5\\) \\(U_1 = \\frac{5}{15}\\) 2 \\(B_2 =\\mod(3 \\times 5 +2) = 2\\) \\(U_2 = \\frac{2}{15}\\) 3 \\(B_3 =\\mod(3 \\times 2 +2) = 8\\) \\(U_3 = \\frac{8}{15}\\) 4 \\(B_4 =\\mod(3 \\times 8 +2) = 11\\) \\(U_4 = \\frac{11}{15}\\) Sometimes computer generated random results are known as pseudo-random numbers to reflect the fact that they are machine generated and can be replicated. That is, despite the fact that {\\(U_{i}\\)} appears to be i.i.d, it can be reproduced by using the same seed number (and the same algorithm). The ability to replicate results can be a tremendous tool as you use simulation while trying to uncover patterns in a business process. The linear congruential generator is just one method of producing pseudo-random outcomes. It is easy to understand and is (still) widely used. The linear congruential generator does have limitations, including the fact that it is possible to detect long-run patterns over time in the sequences generated (recall that we can interpret “independence” to mean a total lack of functional patterns). Not surprisingly, advanced techniques have been developed that address some of this method’s drawbacks. 6.2 Inverse Transform With the sequence of uniform random numbers, we next transform them to a distribution of interest. Let \\(F\\) represent a distribution function of interest. Then, use the inverse transform \\[X_i=F^{-1}\\left( U_i \\right) .\\] The result is that the sequence {\\(X_{i}\\)} is approximately i.i.d. with distribution function \\(F\\). To interpret the result, recall that a distribution function, \\(F\\), is monotonically increasing and so the inverse function, \\(F^{-1}\\), is well-defined. The inverse distribution function (also known as the quantile function), is defined as \\[\\begin{aligned} F^{-1}(y) = \\inf_x \\{ F(x) \\ge y \\} ,\\end{aligned}\\] where “\\(\\inf\\)” stands for “infimum”, or the greatest lower bound. Inverse Transform Visualization. Here is a graph to help you visualize the inverse transform. When the random variable is continuous, the distribution function is strictly increasing and we can readily identify a unique inverse at each point of the distribution. Figure 6.1: Inverse of a Distribution Function The inverse transform result is available when the underlying random variable is continuous, discrete or a mixture. Here is a series of examples to illustrate its scope of applications. Exponential Distribution Example. Suppose that we would like to generate observations from an exponential distribution with scale parameter \\(\\theta\\) so that \\(F(x) = 1 - e^{-x/\\theta}\\). To compute the inverse transform, we can use the following steps: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow y = 1-e^{-x/\\theta} \\\\ &amp;\\Leftrightarrow -\\theta \\ln(1-y) = x = F^{-1}(y) .\\end{aligned}\\] Thus, if \\(U\\) has a uniform (0,1) distribution, then \\(X = -\\theta \\ln(1-U)\\) has an exponential distribution with parameter \\(\\theta\\). Some Numbers. Take \\(\\theta = 10\\) and generate three random numbers to get \\(U\\) 0.26321364 0.196884752 0.897884218 \\(X = -10\\ln(1-U)\\) 1.32658423 0.952221285 9.909071325 Pareto Distribution Example. Suppose that we would like to generate observations from a Pareto distribution with parameters \\(\\alpha\\) and \\(\\theta\\) so that \\(F(x) = 1 - \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha}\\). To compute the inverse transform, we can use the following steps: \\[\\begin{aligned} y = F(x) &amp;\\Leftrightarrow 1-y = \\left(\\frac{\\theta}{x+\\theta} \\right)^{\\alpha} \\\\ &amp;\\Leftrightarrow \\left(1-y\\right)^{-1/\\alpha} = \\frac{x+\\theta}{\\theta} = \\frac{x}{\\theta} +1 \\\\ &amp;\\Leftrightarrow \\theta \\left((1-y)^{-1/\\alpha} - 1\\right) = x = F^{-1}(y) .\\end{aligned}\\] Thus, \\(X = \\theta \\left((1-U)^{-1/\\alpha} - 1\\right)\\) has a Pareto distribution with parameters \\(\\alpha\\) and \\(\\theta\\). Inverse Transform Justification. Why does the random variable \\(X = F^{-1}(U)\\) have a distribution function “\\(F\\)”? This is easy to establish in the continuous case. Because \\(U\\) is a Uniform random variable on (0,1), we know that \\(\\Pr(U \\le y) = y\\), for \\(0 \\le y \\le 1\\). Thus, \\[\\begin{aligned} \\Pr(X \\le x) &amp;= \\Pr(F^{-1}(U) \\le x) \\\\ &amp;= \\Pr(F(F^{-1}(U)) \\le F(x)) \\\\ &amp;= \\Pr(U \\le F(x)) = F(x)\\end{aligned}\\] as required. The key step is that $ F(F^{-1}(u)) = u$ for each \\(u\\), which is clearly true when \\(F\\) is strictly increasing. Bernoulli Distribution Example. Suppose that we wish to simulate random variables from a Bernoulli distribution with parameter \\(p=0.85\\). A graph of the cumulative distribution function shows that the quantile function can be written as Figure 6.2: Distribution Function of a Binary Random Variable \\[\\begin{aligned} F^{-1}(y) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;y \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; y \\leq 1.0 . \\end{array} \\right.\\end{aligned}\\] Thus, with the inverse transform we may define \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt;U \\leq 0.85 \\\\ 1 &amp; 0.85 &lt; U \\leq 1.0 \\end{array} \\right.\\end{aligned}\\] Some Numbers. Generate three random numbers to get \\(U\\) 0.26321364 0.196884752 0.897884218 \\(X =F^{-1}(U)\\) 0 0 1 Discrete Distribution Example. Consider the time of a machine failure in the first five years. The distribution of failure times is given as: Time (\\(x\\)) 1 2 3 4 5 probability 0.1 0.2 0.1 0.4 0.2 \\(F(x)\\) 0.1 0.3 0.4 0.8 1.0 Figure 6.3: Distribution Function of a Discrete Random Variable Using the graph of the distribution function, with the inverse transform we may define \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} 1 &amp; 0&lt;U \\leq 0.1 \\\\ 2 &amp; 0.1 &lt; U \\leq 0.3\\\\ 3 &amp; 0.3 &lt; U \\leq 0.4\\\\ 4 &amp; 0.4 &lt; U \\leq 0.8 \\\\ 5 &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right.\\end{aligned}\\] For general discrete random variables there may not be an ordering of outcomes. For example, a person could own one of five types of life insurance products and we might use the following algorithm to generate random outcomes: \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0&lt;U \\leq 0.1 \\\\ \\textrm{endowment} &amp; 0.1 &lt; U \\leq 0.3\\\\ \\textrm{term life} &amp; 0.3 &lt; U \\leq 0.4\\\\ \\textrm{universal life} &amp; 0.4 &lt; U \\leq 0.8 \\\\ \\textrm{variable life} &amp; 0.8 &lt; U \\leq 1.0 . \\end{array} \\right.\\end{aligned}\\] Another analyst may use an alternative procedure such as: \\[\\begin{aligned} X = \\left\\{ \\begin{array}{cc} \\textrm{whole life} &amp; 0.9&lt;U&lt;1.0 \\\\ \\textrm{endowment} &amp; 0.7 \\leq U &lt; 0.9\\\\ \\textrm{term life} &amp; 0.6 \\leq U &lt; 0.7\\\\ \\textrm{universal life} &amp; 0.2 \\leq U &lt; 0.6 \\\\ \\textrm{variable life} &amp; 0 \\leq U &lt; 0.2 . \\end{array} \\right.\\end{aligned}\\] Both algorithms produce (in the long-run) the same probabilities, e.g., \\(\\Pr(\\textrm{whole life})=0.1\\), and so forth. So, neither is incorrect. You should be aware that there is “more than one way to skin a cat.” (What an old expression!) Similarly, you could use an alterative algorithm for ordered outcomes (such as failure times 1, 2, 3, 4, or 5, above). Mixed Distribution Example. Consider a random variable that is 0 with probability 70% and is exponentially distributed with parameter \\(\\theta= 10,000\\) with probability 30%. In practice, this might correspond to a 70% chance of having no insurance claims and a 30% chance of a claim - if a claim occurs, then it is exponentially distributed. The distribution function is given as \\[\\begin{aligned} F(y) = \\left\\{ \\begin{array}{cc} 0 &amp; x&lt;0 \\\\ 1 - 0.3 \\exp(-x/10000) &amp; x \\ge 0 . \\end{array} \\right.\\end{aligned}\\] Figure 6.4: Distribution Function of a Hybrid Random Variable From the graph, we can see that the inverse transform for generating random variables with this distribution function is \\[\\begin{aligned} X = F^{-1}(U) = \\left\\{ \\begin{array}{cc} 0 &amp; 0&lt; U \\leq 0.7 \\\\ -1000 \\ln (\\frac{1-U}{0.3}) &amp; 0.7 &lt; U &lt; 1 . \\end{array} \\right.\\end{aligned}\\] As you have seen, for the discrete and mixed random variables, the key is to draw a graph of the distribution function that allows you to visualize potential values of the inverse function. 6.3 How Many Simulated Values? There are many topics to be described in the study of simulation (and fortunately many good sources to help you). The best way to appreciate simulation is to experience it. One topic that inevitably comes up is the number of simulated trials needed to rid yourself of sampling variability so that you may focus on patterns of interest. How many simulated values are recommended? 100? 1,000,000? We can use the central limit theorem to respond to this question. Suppose that we wish to use simulation to calculate \\(\\mathrm{E~}h(X)\\), where \\(h(\\cdot)\\) is some known function. Then, based on \\(R\\) simulations (replications), we get $ X_1,,X_R$. From this simulated sample, we calculate a sample average \\[\\overline{h}_R=\\frac{1}{R}\\sum_{i=1}^{R} h(X_i)\\] and a sample standard deviation \\[s_{h,R}^2 = \\frac{1}{R} \\sum_{i=1}^{R}\\left( h(X_i) -\\overline{h}_R \\right) ^2.\\] So, \\(\\overline{h}_R\\) is your best estimate of \\(\\mathrm{E~}h(X)\\) and \\(s_{h,R}^2\\) provides an indication of the uncertainty of your estimate. As one criterion for your confidence in the result, suppose that you wish to be within 1% of the mean with 95% certainty. According to the central limit theorem, your estimate should be approximately normally distributed. Thus, you should continue your simulation until \\[\\frac{.01\\overline{h}_R}{s_{h,R}/\\sqrt{R}}\\geq 1.96\\] or equivalently \\[R \\geq 38,416\\frac{s_{h,R}^2}{\\overline{h}_R^2}.\\] This criterion is a direct application of the approximate normality (recall that 1.96 is the 97.5th percentile of the standard normal curve). Note that \\(\\overline{h}_R\\) and \\(s_{h,R}\\) are not known in advance, so you will have to come up with estimates as you go (sequentially), either by doing a little pilot study in advance or by interrupting your procedure intermittently to see if the criterion is satisfied. "],
["C-PremCalc.html", "Chapter 7 Premium Calcuations Fundamentals", " Chapter 7 Premium Calcuations Fundamentals This is a placeholder file "],
["C-RiskClass.html", "Chapter 8 Risk Classification 8.1 Introduction", " Chapter 8 Risk Classification This is a placeholder file 8.1 Introduction Through insurance contracts, the policyholders effectively transfer their risks to the insurer in exchange for premiums. For the insurer to stay in business, the premium income collected from a pool of policyholders must at least equal to the bene t outgo. Ignoring the frictional expenses associated with the administrative cost and the pro t margin, the net premium thus should be equal to the expected loss occurring from the risk that is transferred from the policyholder to the insurer. If all policyholders in the insurance pool have identical risk profiles, the insurer may simply charge the same premium for each policyholder because all of them would have the same expected loss. In reality however the policyholders are hardly homogeneous. For example, mortality risk in life insurance depends on the characteristics of the policyholder, such as, age, sex and life style. In auto insurance, those characteristics may include age, occupation, the type or use of the car, and the area where the driver resides. The knowledge of these characteristics or variables of individual policyholders can enhance the ability of calculating a fair premium as they can be used to estimate or predict the expected losses more accurately at the individual level. Indeed, if the insurer do not differentiate the risk characteristics of individual policyholders and simply charges the same premium to all individuals based on the average characteristic of the portfolio, the insurer would face adverse selection, a situation where individuals with a higher chance of loss are attracted in the portfolio and low-risk individuals are repelled. For example, consider a health insurance industry where smoking status is an important risk factor for mortality and morbidity. Most health insurers in the industry require different premiums depending on smoking status, so smokers pay higher premiums than non-smokers, with other characteristics being identical. Now suppose that there is an insurer, we will call EquitabAll, that offers the same premium to all insureds regardless of smoking status, unlike other competitors. The net premium of EquitabAll is natually an average mortality loss accounting for both smokers and non-smokers; the average is a weighted one using the proportion of smokers and non-smokers. Thus it is easy to see that that a smoker would have a good incentive to purchase insurance from EquitabAll than other insurers as the offered premium by EquitabAll is relatively lower. At the same time non-smokers would prefer buying insurance from somewhere else where lower premiums, computed from the non-smoker group only, are offered. The result of this tendency for the EquitabAll’s insurance portfolio is that there will be more smokers and less non-smokers in the pool, which leads to larger-than-expected mortality losses and hence a higher premium for insureds in the next period to cover the higher costs. With the raised new premium in the next period, non-smokers in EquitabAll will have even greater incentives to switch the insurer. As the cycle continues over time, EquitabAll would gradually retain more smokers in its portfolio with the premium continually raised, and this vicious cycle eventually leads to a collapsing of business. In the literature this phenomenon is known as the adverse selection spiral or death spiral. Therefore, incorporating and differentiating important risk characteristics of individuals in the insurance pricing process are a pertinent component for both the determination of fair premium for each policyholder and the long term sustainability of an insurer. In order to incorporate relevant risk characteristics of policyholders in the pricing process insurers maintain some classification system that assigns each policyholder to one of the risk classes based on a relatively small number of risk characteristics that are deemed most relevant. These characteristics used in the classification system are called the rating factors, which are a priori variables in the sense that they are known before the contract begins (e.g., sex, health status, vehicle type, etc, are known during the underwriting process). All policyholders sharing identical risk factors thus are assigned to the same risk class, and are considered homogeneous; the insurer consequently charge them the same premium. An important task in any risk classification is to construct a quantitative model that can determine the expected loss given various rating factors for a policyholder. The standard approach is to adopt a statistical regression model which produces the expected loss as the output when the relevant risk factors are given as the inputs. We introduce and discuss the Poisson regression, which can be used when the loss is a count variable, as a prominent example of an insurance pricing tool under risk classification schemes. "],
["C-Credibility.html", "Chapter 9 Experience Rating using Credibility Theory", " Chapter 9 Experience Rating using Credibility Theory This is a placeholder file "],
["C-PortMgt.html", "Chapter 10 Portfolio Management including Reinsurance 10.1 Tails of Distributions 10.2 Measures of Risk 10.3 Reinsurance", " Chapter 10 Portfolio Management including Reinsurance Chapter preview. Define \\(S\\) to be (random) obligations that arise from a collection (portfolio) of insurance contracts We are particularly interested in probabilities of large outcomes and so formalize the notion of a heavy-tail distribution in Section 10.1. How much in assets does an insurer need to retain to meet obligations arising from the random \\(S\\)? A study of risk measures in Section 10.2 helps to address this question As with policyholders, insurers also seek mechanisms in order to spread risks. A company that sells insurance to an insurance company is known as a reinsurer, studied in Section 10.3. 10.1 Tails of Distributions In 1998 freezing rains fell on eastern Ontario, south-western Quebec and lasted for six days. The event doubled the amount of precipitation in the area experienced in any prior ice storm, and resulted in a catastrophe that produced excess of 840,000 cases of insurance claims. This number is 20% more than that of the claims caused by the Hurricane Andrew - one of the largest natural disasters in the history of North America. After all, the catastrophe caused approximately 1.44 billion Canadian dollars insurance settlements which is the highest loss burden in the history of Canada (Lecomte et al., 1998). More examples of similar catastrophic events that caused extremal insurance losses are Hurricanes Harvey and Sandy, the 2011 Japanese earthquake and tsunami, and so forth. In the context of insurance, a few heavy losses hitting a portfolio and then converting into claims usually represent the greatest part of the indemnities paid by insurance companies. The aforementioned losses, also called `extremes’, are quantitatively modelled by the tails of the associated probability distributions. From the quantitative modelling standpoint, relying on probabilistic models with improper tails is of course daunting. For instance, periods of financial stress may appear with higher frequency than the actuaries expect, and insurance losses may occur with worse severity. Therefore, studying the probabilistic behavior in the tail portion of actuarial models is of utmost importance in the modern framework of quantitative risk management. For this reason, this section is devoted to the introduction of a few mathematical notions that characterize the tail weight of random variables (r.v.’s). The applications of these notions will benefit us in the construction and selection of appropriate models with desired mathematical properties in the tail portion, that are suitable for a given task. Formally, define \\(X\\) to be the (random) obligations that arise from a collection (portfolio) of insurance contracts. We are particularly interested in studying the right tail of the distribution of \\(X\\). Speaking plainly, a r.v. is said to be heavier-tailed if higher probabilities are assigned to larger values. Unwelcome outcomes are more likely to occur for an insurance portfolio that is described by a loss r.v. possessing heavier (right) tail. Tail weight can be an absolute or a relative concept. Specifically, for the former, we may consider a r.v. to be heavy-tailed if certain mathematical properties of the probability distribution are met. For the latter, we can say the tail of one distribution is heavier than the other if some tail measures are larger. In the statistics and probability literature, there are several quantitative approaches have been proposed to compare and classify tail weight. Among most of these approaches, the survival functions serve as the building block. In what follows, we are going to introduce two simple yet useful tail classification methods, in which the basic idea is to study the quantities that are closely related to the survival function of \\(X\\). 10.1.0.1 Classification Based on Moments One possible way of classifying the tail weight of distribution is by assessing the existence of raw moments. Since our major interest lies in the right tails of distributions, we henceforth assume the obligation/loss r.v. \\(X\\) to be positive. At the outset, let us recall that the \\(k-\\)th raw moment of \\(X\\), for \\(k\\in\\mathcal{R}_+\\), can be computed via \\[\\begin{aligned} \\mu_k^{&#39;} &amp;= k \\int_0^{\\infty} x^{k-1} S(x) dx, \\\\ \\end{aligned}\\] where \\(S(\\cdot)\\) denotes the survival function of \\(X\\). It is a simple matter to see that the existence of the raw moments depends on the asymptotic behavior of the survival function at infinity. Namely, the faster the survival function decays to zero, the higher the order of finite moment the associated r.v. possesses. Hence the maximal order of finite moment, denoted by \\(k^{\\ast}:=\\sup\\{k\\in \\mathcal{R}_+|\\mu_k^{&#39;}&lt;\\infty \\}\\), can be considered as an indicator of tail weight. This observation leads us to moment-based tail weight classification, which is defined formally next. Definition 1. For a positive loss random variable \\(X\\), if all the positive raw moments exist, namely the maximal order of finite moment \\(k^{\\ast}=\\infty\\), then \\(X\\) is said to be light-tailed based on the moment-method. If the \\(k^{\\ast}=a \\in (0,\\infty)\\), then \\(X\\) is said to be heavy-tailed based on the moment-method. Moreover, for two positive loss random variables \\(X_1\\) and \\(X_2\\) with maximal orders of moment \\(k^{&#39;}_1\\) and \\(k^{&#39;}_2\\) respectively, we say \\(X_1\\) has a heavier (right) tail than \\(X_2\\) if \\(k^{\\ast}_1\\leq k^{\\ast}_2\\). It is noteworthy that the first part of the aforementioned definition is an absolute concept of tail weight, while the second part is a relative concept that compares the weight of (right) tails between two distributions. Next, we are going to present a few examples that illustrate the applications of the moment-based method. Some of these examples are borrowed from Klugman et al., 2012. Example 1. Let \\(X\\sim Gamma(\\alpha,\\theta)\\), with \\(\\alpha&gt;0\\) and \\(\\theta&gt;0\\), then for all \\(k\\in \\mathcal{R}_+\\), \\[\\begin{aligned} \\mu_k^{&#39;} &amp;= \\int_0^{\\infty} x^k \\frac{x^{\\alpha-1} e^{-x/\\theta}}{\\Gamma(\\alpha) \\theta^{\\alpha}} dx \\\\ &amp;= \\int_0^{\\infty} (y\\theta)^k \\frac{(y\\theta)^{\\alpha-1} e^{-y}}{\\Gamma(\\alpha) \\theta^{\\alpha}} \\theta dy \\\\ &amp;= \\frac{\\theta^k}{\\Gamma(\\alpha)} \\Gamma(\\alpha+k) &lt; \\infty.\\end{aligned}\\] Since all the positive moments exist, i.e., \\(k^{\\ast}=\\infty\\), in accordance with the moment-based classification method in Definition 1, the gamma distribution is light-tailed. Example 2. Let \\(X\\sim Weibull(\\theta,\\tau)\\), with \\(\\theta&gt;0\\) and \\(\\tau&gt;0\\), then for all \\(k\\in \\mathcal{R}_+\\), \\[\\begin{aligned} \\mu_k^{&#39;} &amp;= \\int_0^{\\infty} x^k \\frac{\\tau x^{\\tau-1} }{\\theta^{\\tau}} e^{-(x/\\theta)^{\\tau}}dx \\\\ &amp;= \\int_0^{\\infty} \\frac{ y^{k/\\tau} }{\\theta^{\\tau}} e^{-y/\\theta^{\\tau}}dy \\\\ &amp;= \\theta^{k} \\Gamma(1+k/\\tau) &lt; \\infty.\\end{aligned}\\] Again, due to the existence of all the positive moments, the Weibull distribution is light-tailed. We notice in passing that the gamma and Weibull distributions have been used quite intensively in actuarial practice nowadays. Applications of these two distributions are vast which include, but are not limited to, insurance claim severity modelling, solvency assessment, loss reserving, aggregate risk approximation, reliability engineering and failure analysis. We have thus far seem two examples of using the moment-based method to analyze light-tailed distributions. We document a heavy-tailed example in what follows. Example 3. Let \\(X\\sim Pareto(\\alpha,\\theta)\\), with \\(\\alpha&gt;0\\) and \\(\\theta&gt;0\\), then for \\(k\\in \\mathcal{R}_+\\) \\[\\begin{aligned} \\mu_k^{&#39;} &amp;= \\int_0^{\\infty} x^k \\frac{\\alpha \\theta^{\\alpha}}{(x+\\theta)^{\\alpha+1}} dx \\\\ &amp;= \\alpha \\theta^{\\alpha} \\int_{\\theta}^{\\infty} (y-\\theta)^k {y^{-(\\alpha+1)}} dy.\\end{aligned}\\] Consider a similar integration: \\[\\begin{aligned} g_k:=\\int_{\\theta}^{\\infty} {y^{k-\\alpha-1}} dy=\\left\\{ \\begin{array}{ll} &lt;\\infty, &amp; \\hbox{for } k&lt;\\alpha\\\\ =\\infty, &amp; \\hbox{for } k\\geq \\alpha \\end{array} \\right. .\\end{aligned}\\] Meanwhile, \\[\\lim_{y\\rightarrow \\infty} \\frac{(y-\\theta)^k {y^{-(\\alpha+1)}}}{y^{k-\\alpha-1}}=\\lim_{y\\rightarrow \\infty} (1-\\theta/y)^{k}=1.\\] Application of the limit comparison theorem for improper integrals yields \\(\\mu_k^{&#39;}\\) is finite if and only if \\(g_k\\) is finite. Hence we can conclude that the raw moments of Pareto r.v.’s exist only up to \\(k&lt;\\alpha\\), i.e., \\(k^{\\ast}=\\alpha\\), and thus the distribution is heavy-tailed. What is more, the maximal order of finite moments depends only on the shape parameter \\(\\alpha\\) and it is an increasing function of \\(\\alpha\\). In other words, based on the moments method, the tail weight of the Pareto r.v.’s is solely manipulated by \\(\\alpha\\) – the smaller the value of \\(\\alpha\\), the heavier the tail weight becomes. Since \\(k^{\\ast}&lt;\\infty\\), the tail of Pareto distribution is heavier than those of the gamma and Weibull distributions. We are going to conclude this current section by an open discussion on the limitations of the moment-based method. Despite its simple implementation and intuitive interpretation, there are certain circumstances in which the application of the moment-based method is not suitable. First, for more complicated probabilistic models, the \\(k\\)-th raw moment may not be straightforward to derive and/or the identification of the maximal order of finite moment can be very challenging. Second, the moment-based method does not well comply with main body of the well established heavy tail theory in literature. Specifically, the existence of moment generating functions (MGF’s) is arguably the most popular method for classifying heavy tail verse light tail within the community of academic actuaries. However, for some r.v’s such as the log normal r.v.’s, their MGF’s do not exist even that all the positive moments are finite. In these cases, applications of the moment-based and the MFG-based methods can lead to different tail weight assessment. Third, when we need to compare the tail weight between two light-tailed distributions both having all positive moments exist, the moment-based method is no longer informative. 10.1.0.2 Comparison Based on Limiting Tail Behavior In order to resolve the shortfalls of the moment-based method discussed in the previous section, an alternative approach for comparing tail weight is to directly study the limiting behavior of the survival functions. Definition 2. For two r.v.’s \\(X\\) and \\(Y\\), and let \\[ \\gamma:=\\lim_{t\\rightarrow \\infty}\\frac{S_X(t)}{S_Y(t)}. \\] We say that \\(X\\) has a heavier right tail than \\(Y\\) if \\(\\gamma=\\infty\\); \\(X\\) and \\(Y\\) are proportionally equivalent in the right tail, if \\(\\gamma =c\\in \\mathcal{R}_+\\); \\(X\\) has a lighter right tail than \\(Y\\) if \\(\\gamma=0\\). Example 4. Let \\(X\\sim Pareto(\\alpha, \\theta)\\) and \\(Y\\sim Weibull(\\tau, \\theta)\\), for \\(\\alpha&gt;0\\), \\(\\tau&gt;0\\), and \\(\\theta&gt;0\\), we have \\[\\begin{aligned} \\lim_{t\\rightarrow \\infty}\\frac{S_X(t)}{S_Y(t)} &amp;= \\lim_{t\\rightarrow \\infty}\\frac{(1+t/\\theta)^{-\\alpha}}{\\exp\\{-(t/\\theta)^{\\tau}\\}} \\\\ &amp;= \\lim_{t\\rightarrow \\infty}\\frac{\\exp\\{t/\\theta^{\\tau} \\}}{(1+t^{1/\\tau}/\\theta)^{\\alpha}} \\\\ &amp;= \\lim_{t\\rightarrow \\infty}\\frac{\\sum_{i=0}^{\\infty}\\left(\\frac{t}{\\theta^{\\tau}}\\right)^{i}/i!}{(1+t^{1/\\tau}/\\theta)^{\\alpha}}\\\\ &amp;= \\lim_{t\\rightarrow \\infty} \\sum_{i=0}^{\\infty} \\left(t^{-i/\\alpha}+\\frac{t^{(1/\\tau-i/\\alpha)}}{\\theta} \\right)^{-\\alpha}/\\theta^{\\tau i}i!\\\\ &amp;= \\infty. \\end{aligned}\\] Therefore, the Pareto distribution has a heavier tail than the Weibull distribution. One may also realize that exponentials go to infinity faster than polynomials, thus the aforementioned limit must be infinite. For some distributions, the survival functions do not admit explicite expressions. In such cases, we may find the following alternative formula useful: \\[\\begin{aligned} \\lim_{t\\to \\infty} \\frac{S_X(t)}{S_Y(t)} &amp;= \\lim_{t \\to \\infty} \\frac{S_X^{&#39;}(t)}{S_Y^{&#39;}(t)} \\\\ &amp;= \\lim_{t \\to \\infty} \\frac{-f_X(t)}{-f_Y(t)} = \\lim_{t\\to \\infty} \\frac{f_X(t)}{f_Y(t)}.\\end{aligned}\\] given that the density functions exist. Example 5. Let \\(X\\sim Pareto(\\alpha, \\theta)\\) and \\(Y\\sim Gamma(\\alpha, \\theta)\\), for \\(\\alpha&gt;0\\) and \\(\\theta&gt;0\\), we have \\[\\begin{aligned} \\lim_{t\\to \\infty} \\frac{f_{X}(t)}{f_{Y}(t)} &amp;= \\lim_{t \\to \\infty} \\frac{\\alpha \\theta^{\\alpha} (t+ \\theta)^{-\\alpha-1}}{t^{\\tau-1} e^{-t/\\lambda} \\lambda^{-\\tau} \\Gamma(\\tau)^{-1}} \\\\ &amp;= c \\lim_{t\\to \\infty} \\frac{e^{t/\\lambda}}{(t+\\theta)^{\\alpha+1} t^{\\tau-1}} \\\\ &amp;= \\infty,\\end{aligned}\\] as exponentials go to infinity faster than polynomials. 10.2 Measures of Risk A risk measure is a mapping from the r.v. representing the loss associated with the risks to the real line. A risk measure gives a single number that is intended to quantify the risk. For example, the standard deviation is a risk measure. Notation: \\(\\rho(X)\\). We briefly mention: VaR: Value at Risk; TVaR: Tail Value at Risk. 10.2.0.1 Value at Risk Say \\(F_X(x)\\) represents the cdf of outcomes over a fixed period of time, e.g. one year, of a portfolio of risks. We consider positive values of \\(X\\) as losses. Definition 3.11: let \\(X\\) denote a loss r.v., then the Value-at-Risk of \\(X\\) at the \\(100p\\%\\) level, denoted \\(VaR_p(X)\\) or \\(\\pi_p\\), is the \\(100p\\) percentile (or quantile) of the distribution of \\(X\\). E.g. for continuous distributions we have \\[\\begin{aligned} P(X&gt; \\pi_p) &amp;= 1-p.\\end{aligned}\\] VaR has become the standard risk measure used to evaluate exposure to risk. VaR is the amount of capital required to ensure, with a high degree of certainty, that the enterprise does not become technically insolvent. Which degree of certainty? 95\\(\\%\\)? in Solvency II \\(99.5\\%\\) (or: ruin probability of 1 in 200). VaR is not subadditive. Subadditivity of a risk measure \\(\\rho(.)\\) requires \\[\\begin{aligned} \\rho(X+Y) \\leq \\rho(X)+\\rho(Y).\\end{aligned}\\] Intuition behind subadditivity: combining risks is less riskier than holding them separately. Example: let \\(X\\) and \\(Y\\) be i.i.d. r.v.’s which are \\(\\text{Bern}(0.02)\\) distributed. Then, \\(P(X\\leq 0) = 0.98\\) and \\(P(Y\\leq 0)=0.98\\). Thus, \\(F_X^{-1}(0.975)=F_Y^{-1}(0.975)=0\\). For the sum, \\(X+Y\\), we have \\(P[X+Y=0]=0.98 \\cdot 0.98=0.9604\\). Thus, \\(F_{X+Y}^{-1}(0.975)&gt;0\\). VaR is not subadditive, since \\(\\text{VaR}(X+Y)\\) in this case is larger than \\(\\text{VaR}(X)+\\text{VaR}(Y)\\). Another drawback of VaR: it is a single quantile risk measure of a predetermined level \\(p\\); no information about the thickness of the upper tail of the distribution function from \\(\\text{VaR}_p\\) on; whereas stakeholders are interested in both frequency and severity of default. Therefore: study other risk measures, e.g. Tail Value at Risk (TVaR). 10.2.0.2 Tail Value at Risk Definition 3.12: let \\(X\\) denote a loss r.v., then the Tail Value at Risk of \\(X\\) at the \\(100p\\%\\) security level, \\(\\text{TVaR}(p)\\), is the expected loss given that the loss exceeds the \\(100p\\) percentile (or: quantile) of the distribution of \\(X\\). We have (assume continuous distribution) \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= E(X|X&gt;\\pi_p) \\\\ &amp;= \\frac{\\int_{\\pi_p}^{\\infty} x\\cdot f(x) dx}{1-F(\\pi_p)}.\\end{aligned}\\] We can rewrite this as the usual definition of TVaR \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= \\frac{\\int_{\\pi_p}^{\\infty} x dF_X(x)}{1-p} \\\\ &amp;= \\frac{\\int_p^1 \\text{VaR}_u(X) du}{1-p},\\end{aligned}\\] using the substitution \\(F_X(x) = u\\) and thus \\(x=F_X^{-1}(u)\\). From the definition \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= \\frac{\\int_p^1 \\text{VaR}_u(X) du}{1-p},\\end{aligned}\\] we understand TVaR is the arithmetic average of the quantiles of \\(X\\), from level \\(p\\) on; TVaR is averaging high level VaR; TVaR tells us much more about the tail of the distribution than does VaR alone. Finally, TVaR can also be written as \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= E(X|X&gt;\\pi_p) \\\\ &amp;= \\frac{\\int_{\\pi_p}^{\\infty} x f(x)dx}{1-p} \\\\ &amp;= \\pi_p + \\frac{\\int_{\\pi_p}^{\\infty} (x-\\pi_p) f(x) dx}{1-p} \\\\ &amp;= \\text{VaR}_p(X) + e(\\pi_p),\\end{aligned}\\] with \\(e(\\pi_p)\\) the mean excess loss function evaluated at the \\(100p\\)th percentile. We can understand these connections as follows. (Assume continuous r.v.’s) The relation \\[\\begin{aligned} \\text{CTE}_p(X) &amp;= \\text{TVaR}_{F_X(\\pi_p)}(X),\\end{aligned}\\] then follows immediately by combining the other two expressions. TVaR is a coherent risk measure, see e.g. Foundations of Risk Measurement course. Thus, \\(\\text{TVaR}(X+Y) \\leq \\text{TVaR}(X)+\\text{TVaR}(Y)\\). When using this risk measure, we never encounter a situation where combining risks is viewed as being riskier than keeping them separate. KPW Example 3.18 (Tail comparisons) Consider three loss distributions for an insurance company. Losses for the next year are estimated to be on average 100 million with standard deviation 223.607 million. You are interested in finding high quantiles of the distribution of losses. Using the normal, Pareto, and Weibull distributions, obtain the VaR at the 90%, 99%, and 99.99% security levels. Solution Normal distribution has a lighter tail than the others, and thus smaller quantiles. Pareto and Weibull with \\(\\tau&lt;1\\) have heavy tails, and thus relatively larger extreme quantiles. Example 3.18 (Tail comparisons) Consider three loss distributions for an insurance company. Losses for the next year are estimated to be on average 100 million with standard deviation 223.607 million. You are interested in finding high quantiles of the distribution of losses. Using the normal, Pareto, and Weibull distributions, obtain the VaR at the 99%, 99.9%, and 99.99% security levels. &gt; qnorm(c(0.9,0.99,0.999),mu,sigma) [1] 386.5639 620.1877 790.9976 &gt; qpareto(c(0.9,0.99,0.999),alpha,s) [1] 226.7830 796.4362 2227.3411 &gt; qweibull(c(0.9,0.99,0.999),tau,theta) [1] 265.0949 1060.3796 2385.8541 We learn from Example 3.18 that results vary widely depending on the choice of distribution. Thus, the selection of an appropriate loss model is highly important. To obtain numerical values of VaR or TVaR: estimate from the data directly; or use distributional formulas, and plug in parameter estimates. When estimating VaR directly from the data: use R to get quantile from the empirical distribution; R has 9 ways to estimate a VaR at level \\(p\\) from a sample of size \\(n\\), differing in the way the interpolation between order statistics close to \\(np\\) . When estimating TVaR directly from the data: take average of all observations that exceed the threshold (i.e.\\(\\pi_p\\)); Caution: we need a large number of observations (and a large number of observations \\(&gt; \\pi_p\\)) in order to get reliable estimates. When not may observations in excess of the threshold are available: construct a loss model; calculate values of VaR and TVaR directly from the fitted distribution. For example \\[\\begin{aligned} \\text{TVaR}_p(X) &amp;= E(X|X&gt;\\pi_p) \\\\ &amp;= \\pi_p + \\frac{\\int_{\\pi_p}^{\\infty} (x-\\pi_p) f(x) dx}{1-p} \\\\ &amp;= \\pi_p + \\frac{\\int_{-\\infty}^{\\infty} (x-\\pi_p) f(x) dx -\\int_{-\\infty}^{\\pi_p} (x-\\pi_p) f(x) dx }{1-p} \\\\ &amp;= \\pi_p + \\frac{E(X)-\\int_{-\\infty}^{\\pi_p} xf(x) dx -\\pi_p (1-F(\\pi_p))}{1-p} \\\\ &amp;= \\pi_p + \\frac{E(X) - E[\\min{(X,\\pi_p)}]}{1-p} = \\pi_p + \\frac{E(X)-E(X \\wedge \\pi_p)}{1-p},\\end{aligned}\\] see Appendix A for those expressions. 10.3 Reinsurance Recall that reinsurance is simply insurance purchased by an insurer. Insurance purchased by non-insurers is sometimes known as primary insurance to distinguish it from reinsurance. Reinsurance differs from personal insurance purchased by individuals, such as auto and homeowners insurance, in contract flexibility. Like insurance purchased by major corporations, reinsurance programs are generally tailored more closely to the buyer. For contrast, in personal insurance buyers typically cannot negotiate on the contract terms although they may have a variety of different options (contracts) from which to choose. The two broad types are proportional and non-proportional reinsurance. A proportional reinsurance contract is an agreement between a reinsurer and a ceding company (also known as the reinsured) in which the reinsurer assumes a given percent of losses and premium. A reinsurance contract is also known as a treaty. Non-proportional agreements are simply everything else. As examples of non-proportional agreements, this chapter focuses on stop-loss and excess of loss contracts. For all types of agreements, we split the total risk \\(S\\) into the portion taken on by the reinsurer, \\(Y_{reinsurer}\\), and that retained by the insurer, \\(Y_{insurer}\\), that is, \\(S= Y_{insurer}+Y_{reinsurer}\\). The mathematical structure of a basic reinsurance treaty is the same as the coverage modifications of personal insurance introduced in Chapter 3. For a proportional reinsurance, the transformation \\(Y_{insurer} = c S\\) is identical to a coinsurance adjustment in personal insurance. For stop-loss reinsurance, the transformation \\(Y_{reinsurer} = \\max(0,S-M)\\) is the same as an insurer’s payment with a deductible \\(M\\) and \\(Y_{insurer} = \\min(S,M) = S \\wedge M\\) is equivalent to what a policyholder pays with deductible \\(M\\). For practical applications of the mathematics, in personal insurance the focus is generally upon the expectation as this is a key ingredient used in pricing. In constrast, for reinsurance the focus is on the entire distribution of the risk, as the extreme events are a primary concern of the financial stability of the insurer and reinsurer. This chapter describes the foundational and most basic of reinsurance treaties: Section 10.1 for proportional and Section 10.2 for non-proportional. Section 10.3 gives a flavor of more complex contracts. 10.3.1 Proportional Reinsurance The simplest example of a proportional treaty is called quota share. In a quota share treaty, the reinsurer receives a flat percent, say 50%, of the premium for the book of business reinsured. In exchange, the reinsurer pays 50% of losses, including allocated loss adjustment expenses The reinsurer also pays the ceding company a ceding commission which is designed to reflect the differences in underwriting expenses incurred. The amounts paid by the direct insurer and the reinsurer are summarized as \\[ Y_{insurer} = c S \\ \\ \\text{and} \\ \\ \\ Y_{reinsurer} = (1-c) S. \\] Note that \\(Y_{insurer}+Y_{reinsurer}=S\\). Example. Distribution of Losses under Quota Share To develop intuition for the effect of quota-share agreement on the distribution of losses, the following is a short R demonstration using simulation. Note the relative shapes of the distributions of total losses, the retained portion (of the insurer), and the reinsurer’s portion. Click Here to see the R Code set.seed(2018) theta = 1000 alpha = 3 nSim = 10000 library(actuar) S &lt;- rpareto(nSim, shape = alpha, scale = theta) par(mfrow=c(1,3)) plot(density(S), xlim=c(0,3*theta), main=&quot;Total Loss&quot;, xlab=&quot;Losses&quot;) plot(density(0.75*S), xlim=c(0,3*theta), main=&quot;Insurer (75%)&quot;, xlab=&quot;Losses&quot;) plot(density(0.25*S), xlim=c(0,3*theta), main=&quot;Reinsurer (25%)&quot;, xlab=&quot;Losses&quot;) 10.3.1.1 Quota Share is Desirable for Reinsurers The quota share contract is particularly desirable for the reinsurer. To see this, suppose that an insurer and reinsurer wish to enter a contract to share total losses \\(S\\) such that \\[Y_{insurer}=g(S) \\ \\ \\ \\text{and} \\ \\ \\ \\ Y_{reinsurer}=S-g(S),\\] for some generic function \\(g(\\cdot)\\) (known as the retention function). Suppose further that the insurer only cares about the variability of retained claims and is indifferent to the choice of \\(g\\) as long as \\(Var~Y_{insurer}\\) stays the same and equals, say, \\(Q\\). Then, the following result shows that the quota share reinsurance treaty minimizes the reinsurer’s uncertainty as measured by \\(Var~Y_{reinsurer}\\). Proposition. Suppose that \\(Var~Y_{insurer}=Q.\\) Then, \\(Var ((1-c)S) \\le Var(g(S))\\) for all \\(g(.)\\). Click Here to see the Justification of the Proposition Proof of the Proposition. With \\(Y_{reinsurer} = S - Y_{insurer}\\) and the law of total variation \\[ \\begin{array}{ll} Var (Y_{reinsurer}) &amp;= Var (S-Y_{insurer}) \\\\ &amp;= Var (S) + Var (Y_{insurer}) - 2 Cov (S,Y_{insurer}) \\\\ &amp;=Var (S) + Q - 2 Corr (S,Y_{insurer}) \\times \\sqrt{Q} \\sqrt{Var (S)} \\end{array} \\] In this expression, we see that \\(Q\\) and \\(Var(S)\\) do not change with the choice of \\(g\\). Thus, we can minimize \\(Var (Y_{reinsurer})\\) by maximizing the correlation \\(Corr (S,Y_{insurer})\\). If we use a quota share reinsurance agreement, then \\(Corr (S,Y_{insurer})=Corr (S,(1-c)S)=1\\), the maximum possible correlation. This establishes the proposition. \\(\\Box\\)` The proposition is intuitively appealing - with quota share insurance, the reinsurer shares the responsibility for very large claims in the tail of the distribution. This is in contrast to non-proportional agreements where reinsurers take responsibility for the very large claims. 10.3.1.2 Optimizing Quota Share Agreements for Insurers Now assume \\(n\\) risks in the porfolio, \\(X_1, \\ldots, X_n,\\) so that the portfolio sum is \\(S= X_1 + \\cdots + X_n\\). For simplicity, we focus on the case of independent risks. Let us consider a variation of the basic quota share agreement where the amount retained by the insurer may vary with each risk, say \\(c_i\\). Thus, the insurer’s portion of the portfolio risk is \\(Y_{insurer} = \\sum_{i=1}^n c_i X_i\\). What is the best choice of the proportions \\(c_i\\)? To formalize this question, we seek to find those values of \\(c_i\\) that minimize \\(Var ~Y_{insurer}\\) subject to the constraint that \\(E ~Y_{insurer} = K.\\) The requirement that \\(E ~Y_{insurer} = K\\) suggests that the insurers wishes to retain a revenue in at least the amount of the constant \\(K\\). Subject to this revenue constraint, the insurer wishes to minimize uncertainty of the retained risks as measured by the variance. Click Here to see the Optimal Retention Proportions The Optimal Retention Proportions Minimizing \\(Var ~Y_{insurer}\\) subject to \\(E ~Y_{insurer} = K\\) is a constrained optimization problem - we can use the method of Lagrange multipliers, a calculus technique, to solve this. To this end, define the Lagrangian \\[ \\begin{array}{ll} L &amp;= Var (Y_{insurer}) - \\lambda (E ~Y_{insurer} - K) \\\\ &amp;= \\sum_{i=1}^n c_i^2 ~Var ~X_i - \\lambda (\\sum_{i=1}^n c_i ~E ~X_i - K) \\end{array} \\] Taking a partial derivative with respect to \\(\\lambda\\) and setting this equal simply means that the constraint, \\(E ~Y_{insurer} = K\\), is enforced and we have to choose the proportions \\(c_i\\) to satisfy this constraint. Moreover, taking the partial derivative with respect to each proportion \\(c_i\\) yields \\[ \\frac{\\partial}{\\partial c_i} L = 2 c_i ~Var~ X_i - \\lambda ~E ~X_i = 0 \\] so that \\[ c_i = \\frac{\\lambda}{2} \\frac{E ~X_i}{Var ~X_i} . \\] From the math, it turns out that the constant for the \\(i\\)th risk, \\(c_i\\) is proportional to \\(\\frac{E ~X_i}{Var ~X_i}\\). This is intuitively appealing. Other things being equal, a higher revenue as measured by \\(E ~X_i\\) means a higher value of \\(c_i\\). In the same way, a higher value of uncertainty as measured by \\(Var ~X_i\\) means a lower value of \\(c_i\\). The proportional scaling factor is determined by the revenue requirement \\(E ~Y_{insurer} = K\\). The following example helps to develop a feel for this relationship. Click Here to see an Example with Three Pareto Risks Example. Consider three risks that have a Pareto distribution. The graph, and supporting code, give values of \\(c_1\\), \\(c_2\\), and \\(c_3\\) for a required revenue \\(K\\). Note that these values increase linearly with \\(K\\). theta1 = 1000;theta2 = 2000;theta3 = 3000; alpha1 = 3;alpha2 = 3;alpha3 = 4; library(actuar) propnfct &lt;- function(alpha,theta){ mu &lt;- mpareto(shape=alpha, scale=theta, order=1) var &lt;- mpareto(shape=alpha, scale=theta, order=2) - mu^2 ratio &lt;- mu/var ratio } c1 &lt;- propnfct(alpha1, theta1) c2 &lt;- propnfct(alpha2, theta2) c3 &lt;- propnfct(alpha3, theta3) summeans = mpareto(shape=alpha1, scale=theta1, order=1)+ mpareto(shape=alpha2, scale=theta2, order=1)+ mpareto(shape=alpha3, scale=theta3, order=1) temp = c1*mpareto(shape=alpha1, scale=theta1, order=1)+ c2*mpareto(shape=alpha2, scale=theta2, order=1)+ c3*mpareto(shape=alpha3, scale=theta3, order=1) KVec = seq(100,summeans,length.out=20) c1Vec &lt;- c2Vec &lt;-c3Vec &lt;- 0*KVec for (j in 1:20) { c1Vec[j] = c1 * KVec[j]/temp c2Vec[j] = c2 * KVec[j]/temp c3Vec[j] = c3 * KVec[j]/temp } plot(KVec,c1Vec, type=&quot;l&quot;, ylab=&quot;proportion&quot;, xlab=&quot;required revenue&quot;, ylim=c(0,1)) lines(KVec,c2Vec) lines(KVec,c3Vec) text(1200,.8,expression(c[1])) text(2000,.75,expression(c[2])) text(1500,.3,expression(c[3])) 10.3.2 Non-Proportional Reinsurance 10.3.2.1 The Optimality of Stop Loss Insurance Under this arrangement, the insurer sets a retention level \\(M (&gt;0)\\) and pays in full total claims for which \\(S \\le M\\). Thus, the insurer retains an amount \\(M\\) of the risk. Further, for claims for which \\(S &gt; M\\), the direct insurer pays \\(M\\) and the reinsurer pays the remaining amount \\(S-M\\). Summarizing, the amounts paid by the direct insurer and the reinsurer are \\[ Y_{insurer} = \\begin{cases} S &amp; \\text{for } S \\le M\\\\ M &amp; \\text{for } S &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\min(S,M) = S \\wedge M \\] and \\[ Y_{reinsurer} = \\begin{cases} 0 &amp; \\text{for } S \\le M\\\\ S- M &amp; \\text{for } S &gt;M \\\\ \\end{cases} \\ \\ \\ \\ = \\max(0,S-M) . \\] As before, note that \\(Y_{insurer}+Y_{reinsurer}=S\\). The stop loss type of contract is particularly desirable for the insurer. Similar to earlier, suppose that an insurer and reinsurer wish to enter a contract so that \\(Y_{insurer}=g(S)\\) and \\(Y_{reinsurer}=S-g(S)\\) for some generic retention function \\(g(\\cdot)\\). Suppose further that the insurer only cares about the variability of retained claims and is indifferent to the choice of \\(g\\) as long as \\(Var~Y_{insurer}\\) can be minimized. Again, we impose the constraint that \\(E ~Y_{insurer} = K\\); the insurer needs to retain a revenue \\(K\\). Subject to this revenue constraint, the insurer wishes to minimize uncertainty of the retained risks (as measured by the variance). Then, the following result shows that the stop loss reinsurance treaty minimizes the reinsurer’s uncertainty as measured by \\(Var~Y_{reinsurer}\\). Proposition. Suppose that \\(E~Y_{insurer}=K.\\) Then, \\(Var (S \\wedge M) \\le Var(g(S))\\) for all \\(g(.)\\). Click Here to see the Justification of the Proposition Proof of the Proposition. Add and subtract a constant \\(M\\) and expand the square to get \\[ \\begin{array}{ll} Var~ g(S) &amp;= E (g(S) - K)^2 = E (g(S) -M +M- K)^2 \\\\ &amp;= E (g(S) -M)^2 + (M- K)^2 +2 E (g(S) -M)(M- K) \\\\ &amp;= E (g(S) -M)^2 - (M- K)^2 , \\end{array} \\] because \\(E ~g(S)= K.\\) Now, for any retention function, we have \\(g(S) \\le S\\), that is, the insurer’s retained claims are less than or equal to total claims. Using the notation \\(g_{SL}(S) = S \\wedge M\\) for stop loss insurance, we have \\[ \\begin{array}{ll} M- g_{SL}(S) &amp;= M-(S \\wedge M) \\\\ &amp;= (M-S) \\wedge 0 \\\\ &amp;\\le (M-g(S)) \\wedge 0 . \\end{array} \\] Squaring each side yields \\[(M- g_{SL}(S))^2 \\le (M-g(S))^2 \\wedge 0 \\le (M-g(S))^2.\\] Returning to our expression for the variance, we have \\[ \\begin{array}{ll} Var~ g_{SL}(S) &amp;= E (g_{SL}(S) -M)^2 - (M- K)^2 \\\\ &amp;\\le E (g_{SL}(S) -M)^2 - (M- K)^2 = Var~ g(S) , \\end{array} \\] for any retention function \\(g\\). This establishes the proposition. \\(\\Box\\)` The proposition is intuitively appealing - with stop loss insurance, the reinsurer takes the responsibility for very large claims in the tail of the distribution, not the insurer. 10.3.2.2 Excess of Loss A closely related form of non-proportional reinsurance is the excess of loss coverage. Under this contract, we assume that the total risk \\(S\\) can be thought of as composed as \\(n\\) separate risks \\(X_1, \\ldots, X_n\\) and that each of these risks are subject to upper limit, say, \\(M_i\\). So the insurer retains \\[ Y_{i,insurer} = X_i \\wedge M_i \\ \\ \\ \\ Y_{insurer} = \\sum_{i=1}^n Y_{i,insurer} \\] and the reinsurer is responsible for the excess, \\(Y_{reinsurer}=S - Y_{insurer}\\). The retention limits may vary by risk or may be the same for all risks, \\(M_i =M\\), for all \\(i\\). 10.3.2.3 Optimal Choice for Excess of Loss Retention Limits What is the best choice of the excess of loss retention limits \\(M_i\\)? To formalize this question, we seek to find those values of \\(M_i\\) that minimize \\(Var ~Y_{insurer}\\) subject to the constraint that \\(E ~Y_{insurer} = K.\\) Subject to this revenue constraint, the insurer wishes to minimize uncertainty of the retained risks (as measured by the variance). Click Here to see the Optimal Retention Proportions The Optimal Retention Limits Minimizing \\(Var ~Y_{insurer}\\) subject to \\(E ~Y_{insurer} = K\\) is a constrained optimization problem - we can use the method of Lagrange multipliers, a calculus technique, to solve this. As before, define the Lagrangian \\[ \\begin{array}{ll} L &amp;= Var (Y_{insurer}) - \\lambda (E ~Y_{insurer} - K) \\\\ &amp;= \\sum_{i=1}^n ~Var (X_i \\wedge M_i) - \\lambda (\\sum_{i=1}^n ~E(X_i \\wedge M_i)- K) \\end{array} \\] We first recall the relationships \\[ E~S \\wedge M = \\int_0^M ~(1- F(S))dx \\] and \\[ E~(S \\wedge M)^2 = 2\\int_0^M ~x(1- F(x))dx \\] Taking a partial derivative with respect to \\(\\lambda\\) and setting this equal simply means that the constraint, \\(E ~Y_{insurer} = K\\), is enforced and we have to choose the limits \\(M_i\\) to satisfy this constraint. Moreover, taking the partial derivative with respect to each limit \\(M_i\\) yields \\[ \\begin{array}{ll} \\frac{\\partial}{\\partial M_i} L &amp;= \\frac{\\partial}{\\partial M_i} ~Var~ (X_i \\wedge M_i) - \\lambda \\frac{\\partial}{\\partial M_i} ~E ~(X_i \\wedge M_i) \\\\ &amp;= \\frac{\\partial}{\\partial M_i} \\left(E~ (X_i \\wedge M_i)^2 -(E ~(X_i \\wedge M_i))^2\\right) - \\lambda (1-F_i(M_i)) \\\\ &amp;= 2 M_i (1-F_i(M_i)) - 2 E ~(X_i \\wedge M_i) (1-F_i(M_i))- \\lambda (1-F_i(M_i)). \\end{array} \\] Setting \\(\\frac{\\partial}{\\partial M_i} L =0\\) and solving for \\(\\lambda\\), we get \\[ \\lambda = 2 (M_i - E ~(X_i \\wedge M_i)) . \\] From the math, it turns out that the retention limit less the expected insurer’s claims, \\(M_i - E ~(X_i \\wedge M_i)\\), is the same for all risks. This is intuitively appealing, …. Click Here to see an Example with Three Pareto Risks Example. Consider three risks that have a Pareto distribution, each having a different set of parameters (so they are independent but non-identical). We first optimize the Lagrangian using the R package alabama for Augmented Lagrangian Adaptive Barrier Minimization Algorithm. Then, we show that the optimal retention limits \\(M_1\\), \\(M_2\\), and \\(M_3\\) resulting retention limit minus expected insurer’s claims, \\(M_i - E ~(X_i \\wedge M_i)\\), is the same for all risks, as we derived theoretically. Finally, we graphically compare the distribution of total risks to that retained by the insurer and by the reinsurer. theta1 = 1000;theta2 = 2000;theta3 = 3000; alpha1 = 3; alpha2 = 3; alpha3 = 4; Pmin &lt;- 2000 library(actuar) VarFct &lt;- function(M){ M1=M[1];M2=M[2];M3=M[3] mu1 &lt;- levpareto(limit=M1,shape=alpha1, scale=theta1, order=1) var1 &lt;- levpareto(limit=M1,shape=alpha1, scale=theta1, order=2)-mu1^2 mu2 &lt;- levpareto(limit=M2,shape=alpha2, scale=theta2, order=1) var2 &lt;- levpareto(limit=M2,shape=alpha2, scale=theta2, order=2)-mu2^2 mu3 &lt;- levpareto(limit=M3,shape=alpha3, scale=theta3, order=1) var3 &lt;- levpareto(limit=M3,shape=alpha3, scale=theta3, order=2)-mu3^2 varFct &lt;- var1 +var2+var3 meanFct &lt;- mu1+mu2+mu3 c(meanFct,varFct) } f &lt;- function(M){VarFct(M)[2]} h &lt;- function(M){VarFct(M)[1] - Pmin} library(alabama) par0=rep(1000,3) op &lt;- auglag(par=par0,fn=f,hin=h,control.outer=list(trace=FALSE)) M1star = op$par[1];M2star = op$par[2];M3star = op$par[3] M1star -levpareto(M1star,shape=alpha1, scale=theta1,order=1) [1] 1344.135 M2star -levpareto(M2star,shape=alpha2, scale=theta2,order=1) [1] 1344.133 M3star -levpareto(M3star,shape=alpha3, scale=theta3,order=1) [1] 1344.133 set.seed(2018) nSim = 10000 library(actuar) Y1 &lt;- rpareto(nSim, shape = alpha1, scale = theta1) Y2 &lt;- rpareto(nSim, shape = alpha2, scale = theta2) Y3 &lt;- rpareto(nSim, shape = alpha3, scale = theta3) YTotal &lt;- Y1 + Y2 + Y3 Yinsur &lt;- pmin(Y1,M1star)+pmin(Y2,M2star)+pmin(Y3,M3star) Yreinsur &lt;- YTotal - Yinsur par(mfrow=c(1,3)) plot(density(YTotal), xlim=c(0,10000), main=&quot;Total Loss&quot;, xlab=&quot;Losses&quot;) plot(density(Yinsur), xlim=c(0,10000), main=&quot;Insurer&quot;, xlab=&quot;Losses&quot;) plot(density(Yreinsur), xlim=c(0,10000), main=&quot;Reinsurer&quot;, xlab=&quot;Losses&quot;) 10.3.3 Additional Reinsurance Treaties 10.3.3.1 Surplus Share Proportional Treaty Another proportional treaty is known as surplus share; this type of contract is common in commercial property insurance. A surplus share treaty allows the reinsured to limit its exposure on any one risk to a given amount (the retained line). The reinsurer assumes a part of the risk in proportion to the amount that the insured value exceeds the retained line, up to a given limit (expressed as a multiple of the retained line, or number of lines). For example, let the retained line be $100,000 and let the given limit be 4 lines ($400,000). Then, if \\(S\\) is the loss, the reinsurer’s portion is \\(\\min(400000, (S-100000)_+)\\). 10.3.3.2 Layers of Coverage One can also extend non-proportional stop loss treaties by introducing additional parties to the contract. For example, instead of simply an insurer and reinsurer or an insurer and a policyholder, think about the situation with all three parties, a policyholder, insurer, and reinsurer, who agree on how to share a risk. More generally, we consider \\(k\\) parties. If \\(k=4\\), it could be an insurer and three different reinsurers. Example Suppose that there are \\(k=3\\) parties. The first party is responsible for the first 100 of claims, the second responsible for claims from 100 to 3000, and the third responsible for claims above 3000. If there are four claims in the amounts 50, 600, 1800 and 4000, then they would be allocated to the parties as follows: Layer Claim 1 Claim 2 Claim 3 Claim 4 Total (0, 100] 50 100 100 100 350 (100, 3000] 0 500 1700 2900 5100 (3000, \\(\\infty\\)) 0 0 0 1000 1000 Total 50 600 1800 4000 6450 To handle the general situation with \\(k\\) groups, partition the positive real line into \\(k\\) intervals using the cut-points \\[0 = M_0 &lt; M_1 &lt; \\cdots &lt; M_{k-1} &lt; M_k = \\infty.\\] Note that the \\(j\\)th interval is \\((M_{j-1}, M_j]\\). Now let \\(Y_j\\) be the amount of risk shared by the \\(j\\)th party. To illustrate, if a loss \\(x\\) is such that \\(M_{j-1} &lt;x \\le M_j\\), then \\[\\left(\\begin{array}{c} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_j \\\\Y_{j+1} \\\\ \\vdots \\\\Y_k \\end{array}\\right) =\\left(\\begin{array}{c} M_1-M_0 \\\\ M_2-M_1 \\\\ \\vdots \\\\ x-M_{j-1} \\\\ 0 \\\\ \\vdots \\\\0 \\end{array}\\right)\\] More succinctly, we can write \\[Y_j = \\min(S,M_j) - \\min(S,M_{j-1}) .\\] With the expression \\(Y_j = \\min(S,M_j) - \\min(S,M_{j-1})\\), we see that the \\(j\\)th party is responsible for claims in the interval \\((M_{j-1}, M_j].\\) With this, it is easy to check that \\(S = Y_1 + Y_2 + \\cdots + Y_k.\\) As emphasized in the following example, we also remark that the parties need not be different. Example - Suppose that a policyholder is responsible for the first 500 of claims and all claims in excess of 100,000. The insurer takes claims between 100 and 100,000. Then, we would use \\(M_1 = 100\\), \\(M_2 =100000\\). The policyholder is responsible for \\(Y_1 =\\min(S,100)\\) and \\(Y_3 = S - \\min(S,100000) = \\max(0, S-100000)\\). For additional reading, wee the Wisconsin Property Fund site for more info on layers of reinsurance, https://sites.google.com/a/wisc.edu/local-government-property-insurance-fund/home/reinsurance. 10.3.3.3 Portfolio Management Example Many other variations of the foundational contracts are possible. For one more illustration, consider the following. Example. You are the Chief Risk Officer of a telecommunications firm. Your firm has several property and liabililty risks; we will consider: \\(X_1\\) - buildings, modeled using a gamma distribution with mean 200 and scale parameter 100. \\(X_2\\) - motor vehicles, modeled using a gamma distribution with mean 400 and scale parameter 200. \\(X_3\\) - directors and executive officers risk, modeled using a Pareto distribution with mean 1000 and scale parameter 1000. \\(X_4\\) - cyber risks, modeled using a Pareto distribution with mean 1000 and scale parameter 2000. Denote the total risk as \\[S = X_1 + X_2 + X_3 + X_4 .\\] For simplicity, you assume that these risks are independent. To manage the risk, you seek some insurance protection. You wish to manage internally small building and motor vehicles amounts, up to \\(M_1\\) and \\(M_2\\), respectively. You seek insurance to cover all other risks. Specifically, the insurer’s portion is \\[ Y_{insurer} = (X_1 - M_1)_+ + (X_2 - M_2)_+ + X_3 + X_4 ,\\] so that your retained risk is \\(Y_{retained}= S- Y_{insurer} = \\min(X_1,M_1) + \\min(X_2,M_2)\\). Using deductibles \\(M_1=\\) 100 and \\(M_2=\\) 200: Determine the expected claim amount of (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount. Determine the 80th, 90th, 95th, and 99th percentiles for (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount. Compare the distributions by plotting the densities for (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount. R Code for Example Solution In preparation, here is the code needed to set the parameters. # For the gamma distributions, use alpha1 &lt;- 2; theta1 &lt;- 100 alpha2 &lt;- 2; theta2 &lt;- 200 # For the Pareto distributions, use alpha3 &lt;- 2; theta3 &lt;- 1000 alpha4 &lt;- 3; theta4 &lt;- 2000 # Limits M1 &lt;- 100 M2 &lt;- 200 With these parameters, we can now simulate realizations of the portfolio risks. # Simulate the risks nSim &lt;- 10000 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim,alpha1,scale = theta1) X2 &lt;- rgamma(nSim,alpha2,scale = theta2) # For the Pareto Distribution, use library(actuar) X3 &lt;- rpareto(nSim,scale=theta3,shape=alpha3) X4 &lt;- rpareto(nSim,scale=theta4,shape=alpha4) # Portfolio Risks S &lt;- X1 + X2 + X3 + X4 Yretained &lt;- pmin(X1,M1) + pmin(X2,M2) Yinsurer &lt;- S - Yretained (a) Here is the code for the expected claim amounts. # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(S)))) colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(ExpVec,digits=2) Retained Insurer Total [1,] 269.05 2274.41 2543.46 (b) Here is the code for the quantiles. # Quantiles quantMat &lt;- rbind( quantile(Yretained, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(Yinsurer, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(S , probs=c(0.80, 0.90, 0.95, 0.99))) rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(quantMat,digits=2) 80% 90% 95% 99% Retained 300.00 300.00 300.00 300.00 Insurer 3075.67 4399.80 6172.69 11859.02 Total 3351.35 4675.04 6464.20 12159.02 (c) Here is the code for the density plots of the retained, insurer, and total portfolio risk. par(mfrow=c(1,3)) plot(density(Yretained), xlim=c(0,500), main=&quot;Retained Portfolio Risk&quot;, xlab=&quot;Loss (Note the different horizontal scale)&quot;) plot(density(Yinsurer), xlim=c(0,15000), main=&quot;Insurer Portfolio Risk&quot;, xlab=&quot;Loss&quot;) plot(density(S), xlim=c(0,15000), main=&quot;Total Portfolio Risk&quot;, xlab=&quot;Loss&quot;) "],
["C-LossReserves.html", "Chapter 11 Loss Reserving", " Chapter 11 Loss Reserving This is a placeholder file "],
["C-BonusMalus.html", "Chapter 12 Experience Rating using Bonus-Malus", " Chapter 12 Experience Rating using Bonus-Malus This is a placeholder file Bonus-Malus Bonus-malus system, which is used interchangeably as “no-fault discount”, “merit rating”, “experience rating” or “no-claim discount” in different countries, is based on penalizing insureds who are responsible for one or more claims by a premium surcharge, and awarding insureds with a premium discount if they do not have any claims (Frangos and Vrontos, 2001). Insurers use bonus-malus systems for two main purposes; firstly, to encourage drivers to drive more carefully in a year without any claims, and secondly, to ensure insureds to pay premiums proportional to their risks which are based on their claims experience. NCD and Experience Rating No Claim Discount (NCD) system is an experience rating system commonly used in motor insurance. NCD system represents an attempt to categorize insureds into homogeneous groups who pay premiums based on their claims experience. Depending on the rules in the scheme, new policyholders may be required to pay full premium initially, and obtain discounts in the future years as a results of claim-free years. Hunger for Bonus An NCD system rewards policyholders for not making any claims during a year, or in other words, it grants a bonus to a careful driver. This bonus principle may affect policy holders’ decisions whether to claim or not to claim, especially when involving accidents with slight damages, which is known as ‘hunger for bonus’ phenomenon (Philipson, 1960). The option of ‘hunger for bonus’ implemented on insureds under an NCD system may reduce insurers’ claim costs, and may be able to offset the expected decrease in premium income. "],
["data-systems.html", "Chapter 13 Data Systems 13.1 Data 13.2 Data Analysis Preliminary 13.3 Data Analysis Techniques 13.4 Some R Functions 13.5 Summary 13.6 Further Resources and Contributors", " Chapter 13 Data Systems Chapter Preview. This chapter covers the learning areas on data and systems outlined in the IAA (International Actuarial Association) Education Syllabus published in September 2015. 13.1 Data 13.1.1 Data Types and Sources In terms of how data are collected, data can be divided into two types (Hox and Boeije 2005): primary data and secondary data. Primary data are original data that are collected for a specific research problem. Secondary data are data originally collected for a different purpose and reused for another research problem. A major advantage of using primary data is that the theoretical constructs, the research design, and the data collection strategy can be tailored to the underlying research question to ensure that the data collected indeed help to solve the problem. A disadvantage of using primary data is that data collection can be costly and time-consuming. Using secondary data has the advantage of lower cost and faster access to relevant information. However, using secondary data may not be optimal for the research question under consideration. In terms of the degree of organization of the data, data can be also divided into two types (Inmon and Linstedt 2014; O’Leary 2013; Hashem et al. 2015; Abdullah and Ahmad 2013; Pries and Dunnigan 2015): structured data and unstructured data. Structured data have a predictable and regularly occurring format. In contrast, unstructured data are unpredictable and have no structure that is recognizable to a computer. Structured data consists of records, attributes, keys, and indices and are typically managed by a database management system (DBMS) such as IBM DB2, Oracle, MySQL, and Microsoft SQL Server. As a result, most units of structured data can be located quickly and easily. Unstructured data have many different forms and variations. One common form of unstructured data is text. Accessing unstructured data is clumsy. To find a given unit of data in a long text, for example, sequentially search is usually performed. In terms of how the data are measured, data can be classified as qualitative or quantitative. Qualitative data is data about qualities, which cannot be actually measured. As a result, qualitative data is extremely varied in nature and includes interviews, documents, and artifacts (Miles, Hberman, and Sdana 2014). Quantitative data is data about quantities, which can be measured numerically with numbers. In terms of the level of measurement, quantitative data can be further classified as nominal, ordinal, interval, or ratio (Gan 2011). Nominal data, also called categorical data, are discrete data without a natural ordering. Ordinal data are discrete data with a natural order. Interval data are continuous data with a specific order and equal intervals. Ratio data are interval data with a natural zero. There exist a number of data sources. First, data can be obtained from university-based researchers who collect primary data. Second, data can be obtained from organizations that are set up for the purpose of releasing secondary data for general research community. Third, data can be obtained from national and regional statistical institutes that collect data. Finally, companies have corporate data that can be obtained for research purpose. While it might be difficult to obtain data to address a specific research problem or answer a business question, it is relatively easy to obtain data to test a model or an algorithm for data analysis. In nowadays, readers can obtain datasets from the Internet easily. The following is a list of some websites to obtain real-world data: UCI Machine Learning Repository This website (url: http://archive.ics.uci.edu/ml/index.php) maintains more than 400 datasets that can be used to test machine learning algorithms. Kaggle The Kaggle website (url: https://www.kaggle.com/) include real-world datasets used for data science competition. Readers can download data from Kaggle by registering an account. DrivenData DrivenData aims at bringing cutting-edge practices in data science to solve some of the world’s biggest social challenges. In its website (url: https://www.drivendata.org/), readers can participate data science competitions and download datasets. Analytics Vidhya This website (url: https://datahack.analyticsvidhya.com/contest/all/) allows you to participate and download datasets from practice problems and hackathon problems. KDD Cup KDD Cup is the annual Data Mining and Knowledge Discovery competition organized by ACM Special Interest Group on Knowledge Discovery and Data Mining. This website (url: http://www.kdd.org/kdd-cup) contains the datasets used in past KDD Cup competitions since 1997. U.S. Government’s open data This website (url: https://www.data.gov/) contains about 200,000 datasets covering a wide range of areas including climate, education, energy, and finance. AWS Public Datasets In this website (url: https://aws.amazon.com/datasets/), Amazon provides a centralized repository of public datasets, including some huge datasets. 13.1.2 Data Structures and Storage As mentioned in the previous subsection, there are structured data as well as unstructured data. Structured data are highly organized data and usually have the following tabular format: \\[\\begin{matrix} \\begin{array}{lllll} \\hline &amp; V_1 &amp; V_2 &amp; \\cdots &amp; V_d \\ \\\\\\hline \\textbf{x}_1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ \\textbf{x}_2 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ \\textbf{x}_n &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\\\ \\hline \\end{array} \\end{matrix} \\] In other words, structured data can be organized into a table consists of rows and columns. Typically, each row represents a record and each column represents an attribute. A table can be decomposed into several tables that can be stored in a relational database such as the Microsoft SQL Server. The SQL (Structured Query Language) can be used to access and modify the data easily and efficiently. Unstructured data do not follow a regular format (Abdullah and Ahmad 2013). Examples of unstructured data include documents, videos, and audio files. Most of the data we encounter are unstructured data. In fact, the term ``big data’’ was coined to reflect this fact. Traditional relational databases cannot meet the challenges on the varieties and scales brought by massive unstructured data nowadays. NoSQL databases have been used to store massive unstructured data. There are three main NoSQL databases (Chen et al. 2014): key-value databases, column-oriented databases, and document-oriented databases. Key-value databases use a simple data model and store data according to key-values. Modern key-value databases have higher expandability and smaller query response time than relational databases. Examples of key-value databases include Dynamo used by Amazon and Voldemort used by LinkedIn. Column-oriented databases store and process data according to columns rather than rows. The columns and rows are segmented in multiple nodes to achieve expandability. Examples of column-oriented databases include BigTable developed by Google and Cassandra developed by FaceBook. Document databases are designed to support more complex data forms than those stored in key-value databases. Examples of document databases include MongoDB, SimpleDB, and CouchDB. MongoDB is an open-source document-oriented database that stores documents as binary objects. SimpleDB is a distributed NoSQL database used by Amazon. CouchDB is an another open-source document-oriented database. 13.1.3 Data Quality Accurate data are essential to useful data analysis. The lack of accurate data may lead to significant costs to organizations in areas such as correction activities, lost customers, missed opportunities, and incorrect decisions (Olson 2003). Data has quality if it satisfies its intended use, that is, the data is accurate, timely, relevant, complete, understood, and trusted (Olson 2003). As a result, we first need to know the specification of the intended uses and then judge the suitability for those uses in order to assess the quality of the data. Unintended uses of data can arise from a variety of reasons and lead to serious problems. Accuracy is the single most important component of high-quality data. Accurate data have the following properties (Olson 2003): The data elements are not missing and have valid values. The values of the data elements are in the right ranges and have the right representations. Inaccurate data arise from different sources. In particular, the following areas are common areas where inaccurate data occur: Initial data entry. Mistakes (including deliberate errors) and system errors can occur during the initial data entry. Flawed data entry processes can result in inaccurate data. Data decay. Data decay, also known as data degradation, refers to the gradual corruption of computer data due to an accumulation of non-critical failures in a storage device. Data moving and restructuring. Inaccurate data can also arise from data extracting, cleaning, transforming, loading, or integrating. Data using. Faulty reporting and lack of understanding can lead to inaccurate data. Reverification and analysis are two approaches to find inaccurate data elements. To ensure that the data elements are 100% accurate, we must use reverification. However, reverification can be time-consuming and may not be possible for some data. Analytical techniques can also be used to identify inaccurate data elements. There are five types of analysis that can be used to identify inaccurate data (Olson 2003): data element analysis, structural analysis, value correlation, aggregation correlation, and value inspection Companies can create a data quality assurance program to create high-quality databases. For more information about data quality issues management and data profiling techniques, readers are referred to (Olson 2003). 13.1.4 Data Cleaning Raw data usually need to be cleaned before useful analysis can be conducted. In particular, the following areas need attention when preparing data for analysis (Janert 2010): Missing values It is common to have missing values in raw data. Depending on the situations, we can discard the record, discard the variable, or impute the missing values. Outliers Raw data may contain unusual data points such as outliers. We need to handle outliers carefully. We cannot just remove outliers without knowing the reason for their existence. Sometimes the outliers are caused by clerical errors. Sometimes outliers are the effect we are looking for. Junk Raw data may contain junks such as nonprintable characters. Junks are typically rare and not easy to get noticed. However, junks can cause serious problems in downstream applications. Format Raw data may be formated in a way that is inconvenient for subsequent analysis. For example, components of a record may be split into multiple lines in a text file. In such cases, lines corresponding to a single record should be merged before loading to a data analysis software such as R. Duplicate records Raw data may contain duplicate records. Duplicate records should be recognized and removed. This task may not be trivial depending on what you consider ``duplicate.’’ Merging datasets Raw data may come from different sources. In such cases, we need to merge the data from different sources to ensure compatibility. For more information about how to handle data in R, readers are referred to (Forte 2015) and (Buttrey and Whitaker 2017). 13.2 Data Analysis Preliminary Data analysis involves inspecting, cleansing, transforming, and modeling data to discover useful information to suggest conclusions and make decisions. Data analysis has a long history. In 1962, statistician John Tukey defined data analysis as: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. — (Tukey 1962) Recently, Judd and coauthors defined data analysis as the following equation(Judd, McClelland, and Ryan 2017): \\[\\hbox{Data} = \\hbox{Model} + \\hbox{Error},\\] where Data represents a set of basic scores or observations to be analyzed, Model is a compact representation of the data, and Error is simply the amount the model fails to represent accurately. Using the above equation for data analysis, an analyst must resolve the following two conflicting goals: to add more parameters to the model so that the model represents the data better. to remove parameters from the model so that the model is simple and parsimonious. In this section, we give a high-level introduction to data analysis, including different types of methods. 13.2.1 Data Analysis Process Data analysis is part of an overall study. For example, Figure 13.1 shows the process of a typical study in behavioral and social sciences as described in (Albers 2017). The data analysis part consists of the following steps: Exploratory analysis The purpose of this step is to get a feel of the relationships with the data and figure out what type of analysis for the data makes sense. Statistical analysis This step performs statistical analysis such as determining statistical significance and effect size. Make sense of the results This step interprets the statistical results in the context of the overall study. Determine implications This step interprets the data by connecting it to the study goals and the larger field of this study. The goal of the data analysis as described above focuses on explaining some phenomenon (See Section 13.2.5). Figure 13.1: The process of a typical study in behavioral and social sciences. Shmueli (2010) described a general process for statistical modeling, which is shown in Figure 13.2. Depending on the goal of the analysis, the steps differ in terms of the choice of methods, criteria, data, and information. Figure 13.2: The process of statistical modeling. 13.2.2 Exploratory versus Confirmatory There are two phases of data analysis (Good 1983): exploratory data analysis (EDA) and confirmatory data analysis (CDA). Table 13.1 summarizes some differences between EDA and CDA. EDA is usually applied to observational data with the goal of looking for patterns and formulating hypotheses. In contrast, CDA is often applied to experimental data (i.e., data obtained by means of a formal design of experiments) with the goal of quantifying the extent to which discrepancies between the model and the data could be expected to occur by chance (Gelman 2004). \\[\\begin{matrix} \\begin{array}{lll} \\hline &amp; \\textbf{EDA} &amp; \\textbf{CDA} \\\\\\hline \\text{Data} &amp; \\text{Observational data} &amp; \\text{Experimental data}\\\\[3mm] \\text{Goal} &amp; \\text{Pattern recognition,} &amp; \\text{Hypothesis testing,} \\\\ &amp; \\text{formulate hypotheses} &amp; \\text{estimation, prediction} \\\\[3mm] \\text{Techniques} &amp; \\text{Descriptive statistics,} &amp; \\text{Traditional statistical tools of} \\\\ &amp; \\text{visualization, clustering} &amp; \\text{inference, significance, and}\\\\ &amp; &amp; \\text{confidence} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 13.1: Comparison of exploratory data analysis and confirmatory data analysis. Techniques for EDA include descriptive statistics (e.g., mean, median, standard deviation, quantiles), distributions, histograms, correlation analysis, dimension reduction, and cluster analysis. Techniques for CDA include the traditional statistical tools of inference, significance, and confidence. 13.2.3 Supervised versus Unsupervised Methods for data analysis can be divided into two types (Abbott 2014; Igual and Segu 2017): supervised learning methods and unsupervised learning methods. Supervised learning methods work with labeled data, which include a target variable. Mathematically, supervised learning methods try to approximate the following function: \\[ Y = f(X_1, X_2, \\ldots, X_p), \\] where \\(Y\\) is a target variable and \\(X_1\\), \\(X_2\\), \\(\\ldots\\), \\(X_p\\) are explanatory variables. Other terms are also used to mean a target variable. Table 13.2 gives a list of common names for different types of variables (E. W. Frees 2009). When the target variable is a categorical variable, supervised learning methods are called classification methods. When the target variable is continuous, supervised learning methods are called regression methods. \\[\\begin{matrix} \\begin{array}{ll} \\hline \\textbf{Target Variable} &amp; \\textbf{Explanatory Variable}\\\\\\hline \\text{Dependent variable} &amp; \\text{Independent variable}\\\\ \\text{Response} &amp; \\text{Treatment} \\\\ \\text{Output} &amp; \\text{Input} \\\\ \\text{Endogenous variable} &amp; \\text{Exogenous variable} \\\\ \\text{Predicted variable} &amp; \\text{Predictor variable} \\\\ \\text{Regressand} &amp; \\text{Regressor} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 13.2: Common names of different variables. Unsupervised learning methods work with unlabeled data, which include explanatory variables only. In other words, unsupervised learning methods do not use target variables. As a result, unsupervised learning methods are also called descriptive modeling methods. 13.2.4 Parametric versus Nonparametric Methods for data analysis can be parametric or nonparametric (Abbott 2014). Parametric methods assume that the data follow a certain distribution. Nonparametric methods do not assume distributions for the data and therefore are called distribution-free methods. Parametric methods have the advantage that if the distribution of the data is known, properties of the data and properties of the method (e.g., errors, convergence, coefficients) can be derived. A disadvantage of parametric methods is that analysts need to spend considerable time on figuring out the distribution. For example, analysts may try different transformation methods to transform the data so that it follows a certain distribution. Since nonparametric methods make fewer assumptions, nonparametric methods have the advantage that they are more flexible, more robust, and applicable to non-quantitative data. However, a drawback of nonparametric methods is that the conclusions drawn from nonparametric methods are not as powerful as those drawn from parametric methods. 13.2.5 Explanation versus Prediction There are two goals in data analysis (Breiman 2001; Shmueli 2010): explanation and prediction. In some scientific areas such as economics, psychology, and environmental science, the focus of data analysis is to explain the causal relationships between the input variables and the response variable. In other scientific areas such as natural language processing and bioinformatics, the focus of data analysis is to predict what the responses are going to be given the input variables. Shmueli (2010) discussed in detail the distinction between explanatory modeling and predictive modeling, which reflect the process of using data and methods for explaining or predicting, respectively. Explanatory modeling is commonly used for theory building and testing. However, predictive modeling is rarely used in many scientific fields as a tool for developing theory. Explanatory modeling is typically done as follows: State the prevailing theory. State causal hypotheses, which are given in terms of theoretical constructs rather than measurable variables. A causal diagram is usually included to illustrate the hypothesized causal relationship between the theoretical constructs. Operationalize constructs. In this step, previous literature and theoretical justification are used to build a bridge between theoretical constructs and observable measurements. Collect data and build models alongside the statistical hypotheses, which are operationalized from the research hypotheses. Reach research conclusions and recommend policy. The statistical conclusions are converted into research conclusions. Policy recommendations are often accompanied. Shmueli (2010) defined predictive modeling as the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations. Predictions include point predictions, interval predictions, regions, distributions, and rankings of new observations. Predictive model can be any method that produces predictions. 13.2.6 Data Modeling versus Algorithmic Modeling Breiman (2001) discussed two cultures in the use of statistical modeling to reach conclusions from data: the data modeling culture and the algorithmic modeling culture. In the data modeling culture, the data are assumed to be generated by a given stochastic data model. In the algorithmic modeling culture, the data mechanism is treated as unknown and algorithmic models are used. Data modeling gives the statistics field many successes in analyzing data and getting information about the data mechanisms. However, Breiman (2001) argued that the focus on data models in the statistical community has led to some side effects such as Produced irrelevant theory and questionable scientific conclusions. Kept statisticians from using algorithmic models that might be more suitable. Restricted the ability of statisticians to deal with a wide range of problems. Algorithmic modeling was used by industrial statisticians long time ago. However, the development of algorithmic methods was taken up by a community outside statistics (Breiman 2001). The goal of algorithmic modeling is predictive accuracy. For some complex prediction problems, data models are not suitable. These prediction problems include speech recognition, image recognition, handwriting recognition, nonlinear time series prediction, and financial market prediction. The theory in algorithmic modeling focuses on the properties of algorithms, such as convergence and predictive accuracy. 13.2.7 Big Data Analysis Unlike traditional data analysis, big data analysis employs additional methods and tools that can extract information rapidly from massive data. In particular, big data analysis uses the following processing methods (Chen et al. 2014): Bloom filter A bloom filter is a space-efficient probabilistic data structure that is used to determine whether an element belongs to a set. It has the advantages of high space efficiency and high query speed. A drawback of using bloom filter is that there is a certain misrecognition rate. Hashing Hashing is a method that transforms data into fixed-length numerical values through a hash function. It has the advantages of rapid reading and writing. However, sound hash functions are difficult to find. Indexing Indexing refers to a process of partitioning data in order to speed up reading. Hashing is a special case of indexing. Tries A trie, also called digital tree, is a method to improve query efficiency by using common prefixes of character strings to reduce comparison on character strings to the greatest extent. Parallel computing Parallel computing uses multiple computing resources to complete a computation task. Parallel computing tools include MPI (Message Passing Interface), MapReduce, and Dryad. Big data analysis can be conducted in the following levels (Chen et al. 2014): memory-level, business intelligence (BI) level, and massive level. Memory-level analysis is conducted when the data can be loaded to the memory of a cluster of computers. Current hardware can handle hundreds of gigabytes (GB) of data in memory. BI level analysis can be conducted when the data surpass the memory level. It is common for BI level analysis products to support data over terabytes (TB). Massive level analysis is conducted when the data surpass the capabilities of products for BI level analysis. Usually Hadoop and MapReduce are used in massive level analysis. 13.2.8 Reproducible Analysis As mentioned in Section 13.2.1, a typical data analysis workflow includes collecting data, analyzing data, and reporting results. The data collected are saved in a database or files. The data are then analyzed by one or more scripts, which may save some intermediate results or always work on the raw data. Finally a report is produced to describe the results, which include relevant plots, tables, and summaries of the data. The workflow may subject to the following potential issues (Mailund 2017, Chapter 2): The data are separated from the analysis scripts. The documentation of the analysis is separated from the analysis itself. If the analysis is done on the raw data with a single script, then the first issue is not a major problem. If the analysis consists of multiple scripts and a script saves intermediate results that are read by the next script, then the scripts describe a workflow of data analysis. To reproduce an analysis, the scripts have to be executed in the right order. The workflow may cause major problems if the order of the scripts is not documented or the documentation is not updated or lost. One way to address the first issue is to write the scripts so that any part of the workflow can be run completely automatically at any time. If the documentation of the analysis is synchronized with the analysis, then the second issue is not a major problem. However, the documentation may become completely useless if the scripts are changed but the documentation is not updated. Literate programming is an approach to address the two issues mentioned above. In literate programming, the documentation of a program and the code of the program are written together. To do literate programming in R, one way is to use the R Markdown and the \\(\\texttt{knitr}\\) package. 13.2.9 Ethical Issues Analysts may face ethical issues and dilemmas during the data analysis process. In some fields, for example, ethical issues and dilemmas include participant consent, benefits, risk, confidentiality, and data ownership (Miles, Hberman, and Sdana 2014). For data analysis in actuarial science and insurance in particular, we face the following ethical matters and issues (Miles, Hberman, and Sdana 2014): Worthness of the project Is the project worth doing? Will the project contribute in some significant way to a domain broader than my career? If a project is only opportunistic and does not have a larger significance, then it might be pursued with less care. The result may be looked good but not right. Competence Do I or the whole team have the expertise to carry out the project? Incompetence may lead to weakness in the analytics such as collecting large amounts of data poorly and drawing superficial conclusions. Benefits, costs, and reciprocity Will each stakeholder gain from the project? Are the benefit and the cost equitable? A project will likely to fail if the benefit and the cost for a stakeholder do not match. Privacy and confidentiality How do we make sure that the information is kept confidentially? Where raw data and analysis results are stored and how will have access to them should be documented in explicit confidentiality agreements. 13.3 Data Analysis Techniques Techniques for data analysis are drawn from different but overlapping fields such as statistics, machine learning, pattern recognition, and data mining. Statistics is a field that addresses reliable ways of gathering data and making inferences based on them (Bandyopadhyay and Forster 2011; Bluman 2012). The term machine learning was coined by Samuel in 1959 (Samuel 1959). Originally, machine learning refers to the field of study where computers have the ability to learn without being explicitly programmed. Nowadays, machine learning has evolved to the broad field of study where computational methods use experience (i.e., the past information available for analysis) to improve performance or to make accurate predictions (C. M. Bishop 2007; Clarke, Fokoue, and Zhang 2009; Mohri, Rostamizadeh, and Talwalkar 2012; Kubat 2017). There are four types of machine learning algorithms (See Table 13.3 depending on the type of the data and the type of the learning tasks. \\[\\begin{matrix} \\begin{array}{rll} \\hline &amp; \\textbf{Supervised} &amp; \\textbf{Unsupervised} \\\\\\hline \\textbf{Discrete Label} &amp; \\text{Classification} &amp; \\text{Clustering} \\\\ \\textbf{Continuous Label} &amp; \\text{Regression} &amp; \\text{Dimension reduction} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 13.3: Types of machine learning algorithms. Originating in engineering, pattern recognition is a field that is closely related to machine learning, which grew out of computer science. In fact, pattern recognition and machine learning can be considered to be two facets of the same field (C. M. Bishop 2007). Data mining is a field that concerns collecting, cleaning, processing, analyzing, and gaining useful insights from data (Aggarwal 2015). 13.3.1 Exploratory Techniques Exploratory data analysis techniques include descriptive statistics as well as many unsupervised learning techniques such as data clustering and principal component analysis. 13.3.2 Descriptive Statistics In the mass noun sense, descriptive statistics is an area of statistics that concerns the collection, organization, summarization, and presentation of data (Bluman 2012). In the count noun sense, descriptive statistics are summary statistics that quantitatively describe or summarize data. \\[\\begin{matrix} \\begin{array}{ll} \\hline &amp; \\textbf{Descriptive Statistics} \\\\\\hline \\text{Measures of central tendency} &amp; \\text{Mean, median, mode, midrange}\\\\ \\text{Measures of variation} &amp; \\text{Range, variance, standard deviation} \\\\ \\text{Measures of position} &amp; \\text{Quantile} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 13.4: Some commonly used descriptive statistics. Table 13.4 lists some commonly used descriptive statistics. In R, we can use the function \\(\\texttt{summary}\\) to calculate some of the descriptive statistics. For numeric data, we can visualize the descriptive statistics using a boxplot. In addition to these quantitative descriptive statistics, we can also qualitatively describe shapes of the distributions (Bluman 2012). For example, we can say that a distribution is positively skewed, symmetric, or negatively skewed. To visualize the distribution of a variable, we can draw a histogram. 13.3.2.1 Principal Component Analysis Principal component analysis (PCA) is a statistical procedure that transforms a dataset described by possibly correlated variables into a dataset described by linearly uncorrelated variables, which are called principal components and are ordered according to their variances. PCA is a technique for dimension reduction. If the original variables are highly correlated, then the first few principal components can account for most of the variation of the original data. To describe PCA, let \\(X_1,X_2,\\ldots,X_d\\) be a set of variables. The first principal component is defined to be the normalized linear combination of the variables that has the largest variance, that is, the first principal component is defined as \\[Z_1=w_{11} X_1 + w_{12} X_2 + \\cdots + w_{1d} X_d,\\] where \\(\\textbf{w}_1=(w_{11}, w_{12}, \\ldots, w_{1d})&#39;\\) is a vector of loadings such that \\(\\mathrm{Var~}{(Z_1)}\\) is maximized subject to the following constraint: \\[\\textbf{w}_1&#39;\\textbf{w}_1 = \\sum_{j=1}^d w_{1j}^2 = 1.\\] For \\(i=2,3,\\ldots,d\\), the \\(i\\)th principal component is defined as \\[Z_i=w_{i1} X_1 + w_{i2} X_2 + \\cdots + w_{id} X_d,\\] where \\(\\textbf{w}_i=(w_{i1}, w_{i2}, \\ldots, w_{id})&#39;\\) is a vector of loadings such that \\(\\mathrm{Var~}{(Z_i)}\\) is maximized subject to the following constraints: \\[\\textbf{w}_i&#39;\\textbf{w}_i=\\sum_{j=1}^d w_{ij}^2 = 1,\\] \\[\\mathrm{cov~}{(Z_i, Z_j)} = 0,\\quad j=1,2,\\ldots,i-1.\\] The principal components of the variables are related to the eigenvectors and eigenvectors of the covariance matrix of the variables. For \\(i=1,2,\\ldots,d\\), let \\((\\lambda_i, \\textbf{e}_i)\\) be the \\(i\\)th eigenvalue-eigenvector pair of the covariance matrix \\({\\Sigma}\\) such that \\(\\lambda_1\\ge \\lambda_2\\ge \\ldots\\ge \\lambda_d\\ge 0\\) and the eigenvectors are normalized. Then the \\(i\\)th principal component is given by \\[Z_{i} = \\textbf{e}_i&#39; \\textbf{X} =\\sum_{j=1}^d e_{ij} X_j,\\] where \\(\\textbf{X}=(X_1,X_2,\\ldots,X_d)&#39;\\). It can be shown that \\(\\mathrm{Var~}{(Z_i)} = \\lambda_i\\). As a result, the proportion of variance explained by the \\(i\\)th principal component is calculated as \\[\\dfrac{\\mathrm{Var~}{(Z_i)}}{ \\sum_{j=1}^{d} \\mathrm{Var~}{(Z_j)}} = \\dfrac{\\lambda_i}{\\lambda_1+\\lambda_2+\\cdots+\\lambda_d}.\\] For more information about PCA, readers are referred to (Mirkin 2011). 13.3.3 Cluster Analysis Cluster analysis (aka data clustering) refers to the process of dividing a dataset into homogeneous groups or clusters such that points in the same cluster are similar and points from different clusters are quite distinct (Gan, Ma, and Wu 2007; Gan 2011). Data clustering is one of the most popular tools for exploratory data analysis and has found applications in many scientific areas. During the past several decades, many clustering algorithms have been proposed. Among these clustering algorithms, the \\(k\\)-means algorithm is perhaps the most well-known algorithm due to its simplicity. To describe the \\(k\\)-means algorithm, let \\(X=\\{\\textbf{x}_1,\\textbf{x}_2,\\ldots,\\textbf{x}_n\\}\\) be a dataset containing \\(n\\) points, each of which is described by \\(d\\) numerical features. Given a desired number of clusters \\(k\\), the \\(k\\)-means algorithm aims at minimizing the following objective function: \\[P(U,Z) = \\sum_{l=1}^k\\sum_{i=1}^n u_{il} \\Vert \\textbf{x}_i-\\textbf{z}_l\\Vert^2,\\] where \\(U=(u_{il})_{n\\times k}\\) is an \\(n\\times k\\) partition matrix, \\(Z=\\{\\textbf{z}_1,\\textbf{z}_2,\\ldots,\\textbf{z}_k\\}\\) is a set of cluster centers, and \\(\\Vert\\cdot\\Vert\\) is the \\(L^2\\) norm or Euclidean distance. The partition matrix \\(U\\) satisfies the following conditions: \\[u_{il}\\in \\{0,1\\},\\quad i=1,2,\\ldots,n,\\:l=1,2,\\ldots,k,\\] \\[\\sum_{l=1}^k u_{il}=1,\\quad i=1,2,\\ldots,n.\\] The \\(k\\)-means algorithm employs an iterative procedure to minimize the objective function. It repeatedly updates the partition matrix \\(U\\) and the cluster centers \\(Z\\) alternately until some stop criterion is met. When the cluster centers \\(Z\\) are fixed, the partition matrix \\(U\\) is updated as follows: \\[\\begin{aligned}u_{il}=\\left\\{ \\begin{array}{ll} 1, &amp; \\text{if } \\Vert \\textbf{x}_i - \\textbf{z}_l\\Vert = \\min_{1\\le j\\le k} \\Vert \\textbf{x}_i - \\textbf{z}_j\\Vert;\\\\ 0, &amp; \\text{if otherwise,} \\end{array} \\right. \\end{aligned}\\] When the partition matrix \\(U\\) is fixed, the cluster centers are updated as follows: \\[z_{lj} = \\dfrac{\\sum_{i=1}^n u_{il} x_{ij} } { \\sum_{i=1}^n u_{il}},\\quad l=1,2,\\ldots,k,\\: j=1,2,\\ldots,d,\\] where \\(z_{lj}\\) is the \\(j\\)th component of \\(\\textbf{z}_l\\) and \\(x_{ij}\\) is the \\(j\\)th component of \\(\\textbf{x}_i\\). For more information about \\(k\\)-means, readers are referred to (Gan, Ma, and Wu 2007) and (Mirkin 2011). 13.3.4 Confirmatory Techniques Confirmatory data analysis techniques include the traditional statistical tools of inference, significance, and confidence. 13.3.4.1 Linear Models Linear models, also called linear regression models, aim at using a linear function to approximate the relationship between the dependent variable and independent variables. A linear regression model is called a simple linear regression model if there is only one independent variable. When more than one independent variables are involved, a linear regression model is called a multiple linear regression model. Let \\(X\\) and \\(Y\\) denote the independent and the dependent variables, respectively. For \\(i=1,2,\\ldots,n\\), let \\((x_i, y_i)\\) be the observed values of \\((X,Y)\\) in the \\(i\\)th case. Then the simple linear regression model is specified as follows (E. W. Frees 2009): \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,2,\\ldots,n,\\] where \\(\\beta_0\\) and \\(\\beta_1\\) are parameters and \\(\\epsilon_i\\) is a random variable representing the error for the \\(i\\)th case. When there are multiple independent variables, the following multiple linear regression model is used: \\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik} + \\epsilon_i,\\] where \\(\\beta_0\\), \\(\\beta_1\\), \\(\\ldots\\), \\(\\beta_k\\) are unknown parameters to be estimated. Linear regression models usually make the following assumptions: \\(x_{i1},x_{i2},\\ldots,x_{ik}\\) are nonstochastic variables. \\(\\mathrm{Var~}(y_i)=\\sigma^2\\), where \\(\\mathrm{Var~}(y_i)\\) denotes the variance of \\(y_i\\). \\(y_1,y_2,\\ldots,y_n\\) are independent random variables. For the purpose of obtaining tests and confidence statements with small samples, the following strong normality assumption is also made: \\(\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n\\) are normally distributed. 13.3.4.2 Generalized Linear Models The generalized linear model (GLM) is a wide family of regression models that include linear regression models as special cases. In a GLM, the mean of the response (i.e., the dependent variable) is assumed to be a function of linear combinations of the explanatory variables, i.e., \\[\\mu_i = E[y_i],\\] \\[\\eta_i = \\textbf{x}_i&#39;\\boldsymbol{\\beta} = g(\\mu_i),\\] where \\(\\textbf{x}_i=(1,x_{i1}, x_{i2}, \\ldots, x_{ik})&#39;\\) is a vector of regressor values, \\(\\mu_i\\) is the mean response for the \\(i\\)th case, and \\(\\eta_i\\) is a systematic component of the GLM. The function \\(g(\\cdot)\\) is known and is called the link function. The mean response can vary by observations by allowing some parameters to change. However, the regression parameters \\(\\boldsymbol{\\beta}\\) are assumed to be the same among different observations. GLMs make the following assumptions: \\(x_{i1},x_{i2},\\ldots,x_{in}\\) are nonstochastic variables. \\(y_1,y_2,\\ldots,y_n\\) are independent. The dependent variable is assumed to follow a distribution from the linear exponential family. The variance of the dependent variable is not assumed to be constant but is a function of the mean, i.e., \\[\\mathrm{Var~}{(y_i)} = \\phi \\nu(\\mu_i),\\] where \\(\\phi\\) denotes the dispersion parameter and \\(\\nu(\\cdot)\\) is a function. As we can see from the above specification, the GLM provides a unifying framework to handle different types of dependent variables, including discrete and continuous variables. For more information about GLMs, readers are referred to (Jong and Heller 2008) and (E. W. Frees 2009). 13.3.4.3 Tree-based Models Decision trees, also known as tree-based models, involve dividing the predictor space (i.e., the space formed by independent variables) into a number of simple regions and using the mean or the mode of the region for prediction (Breiman et al. 1984). There are two types of tree-based models: classification trees and regression trees. When the dependent variable is categorical, the resulting tree models are called classification trees. When the dependent variable is continuous, the resulting tree models are called regression trees. The process of building classification trees is similar to that of building regression trees. Here we only briefly describe how to build a regression tree. To do that, the predictor space is divided into non-overlapping regions such that the following objective function \\[f(R_1,R_2,\\ldots,R_J) = \\sum_{j=1}^J \\sum_{i=1}^n I_{R_j}(\\textbf{x}_i)(y_i - \\mu_j)^2\\] is minimized, where \\(I\\) is an indicator function, \\(R_j\\) denotes the set of indices of the observations that belong to the \\(j\\)th box, \\(\\mu_j\\) is the mean response of the observations in the \\(j\\)th box, \\(\\textbf{x}_i\\) is the vector of predictor values for the \\(i\\)th observation, and \\(y_i\\) is the response value for the \\(i\\)th observation. In terms of predictive accuracy, decision trees generally do not perform to the level of other regression and classification models. However, tree-based models may outperform linear models when the relationship between the response and the predictors is nonlinear. For more information about decision trees, readers are referred to (Breiman et al. 1984) and (Mitchell 1997). 13.4 Some R Functions R is an open-source software for statistical computing and graphics. The R software can be downloaded from the R project website at . In this section, we give some R function for data analysis, especially the data analysis tasks mentioned in previous sections. \\[\\begin{matrix} \\begin{array}{lll} \\hline \\text{Data Analysis Task} &amp; \\text{R package} &amp; \\text{R Function} \\\\\\hline \\text{Descriptive Statistics} &amp; \\texttt{base} &amp; \\texttt{summary}\\\\ \\text{Principal Component Analysis} &amp; \\texttt{stats} &amp; \\texttt{prcomp} \\\\ \\text{Data Clustering} &amp; \\texttt{stats} &amp; \\texttt{kmeans}, \\texttt{hclust} \\\\ \\text{Fitting Distributions} &amp; \\texttt{MASS} &amp; \\texttt{fitdistr} \\\\ \\text{Linear Regression Models} &amp; \\texttt{stats} &amp; \\texttt{lm} \\\\ \\text{Generalized Linear Models} &amp; \\texttt{stats} &amp; \\texttt{glm} \\\\ \\text{Regression Trees} &amp; \\texttt{rpart} &amp; \\texttt{rpart} \\\\ \\text{Survival Analysis} &amp; \\texttt{survival} &amp; \\texttt{survfit} \\\\ \\hline \\end{array} \\end{matrix} \\] Table 13.5: Some R functions for data analysis. Table 13.5 lists a few R functions for different data analysis tasks. Readers can read the R documentation for examples of using these functions. There are also other R functions from other packages to do similar things. However, the functions listed in this table provide good start points for readers to conduct data analysis in R. For analyzing large datasets in R in an efficient way, readers are referred to (Daroczi 2015). 13.5 Summary In this chapter, we gave a high-level overview of data analysis. The overview is divided into three major parts: data, data analysis, and data analysis techniques. In the first part, we introduced data types, data structures, data storages, and data sources. In particular, we provided several websites where readers can obtain real-world datasets to horn their data analysis skills. In the second part, we introduced the process of data analysis and various aspects of data analysis. In the third part, we introduced some commonly used techniques for data analysis. In addition, we listed some R packages and functions that can be used to perform various data analysis tasks. 13.6 Further Resources and Contributors Bibliography "],
["dependence-modeling.html", "Chapter 14 Dependence Modeling 14.1 Variable Types 14.2 Classic Measures of Scalar Associations 14.3 Introduction to Copula 14.4 Application Using Copulas 14.5 Types of Copulas 14.6 Why is Dependence Modeling Important? Technical Supplement A. Other Classic Measures of Scalar Associations", " Chapter 14 Dependence Modeling Chapter Preview. In practice, there are many types of variables that one encounter and the first step in dependence modeling is identifying the type of variable you are dealing with to help direct you to the appropriate technique.This chapter introduces readers to variable types and techniques for modeling dependence or association of multivariate distributions. Section 14.1 provides an overview of the types of variables. Section 14.2 then elaborates basic measures for modeling the dependence between variables. Section 14.3 introduces a novel approach to modeling dependence using Copulas which is reinforced with practical illustrations in Section 14.4. The types of Copula families and basic properties of Copula functions is explained Section 14.5. The chapter concludes by explaining why the study of dependence modeling is important in Section 14.6. 14.1 Variable Types In this section, you learn how to: Classify variables as qualitative or quantitative. Describe multivariate variables. People, firms, and other entities that we want to understand are described in a dataset by numerical characteristics. As these characteristics vary by entity, they are commonly known as variables. To manage insurance systems, it will be critical to understand the distribution of each variable and how they are associated with one another. It is common for data sets to have many variables (high dimensional) and so it useful to begin by classifying them into different types. As will be seen, these classifications are not strict; there is overlap among the groups. Nonetheless, the grouping summarized in Table 14.1 and explained in the remainder of this section provide a solid first step in framing a data set. \\[ {\\small \\begin{matrix} \\begin{array}{l|l} \\hline \\textbf{Variable Type} &amp; \\textbf{Example} \\\\\\hline Qualitative &amp; \\\\ \\text{Binary} &amp; \\text{Sex} \\\\ \\text{Categorical (Unordered, Nominal)} &amp; \\text{Territory (e.g., state/province) in which an insured resides} \\\\ \\text{Ordered Category (Ordinal)} &amp; \\text{Claimant satisfaction (five point scale ranging from 1=dissatisfied} \\\\ &amp; ~~~ \\text{to 5 =satisfied)} \\\\\\hline Quantitative &amp; \\\\ \\text{Continuous} &amp; \\text{Policyholder&#39;s age, weight, income} \\\\ \\text{Discrete} &amp; \\text{Amount of deductible} \\\\ \\text{Count} &amp; \\text{Number of insurance claims} \\\\ \\text{Combinations of} &amp; \\text{Policy losses, mixture of 0&#39;s (for no loss)} \\\\ ~~~ \\text{Discrete and Continuous} &amp; ~~~\\text{and positive claim amount} \\\\ \\text{Interval Variable} &amp; \\text{Driver Age: 16-24 (young), 25-54 (intermediate),} \\\\ &amp; ~~~\\text{55 and over (senior)} \\\\ \\text{Circular Data} &amp; \\text{Time of day measures of customer arrival} \\\\ \\hline Multivariate ~ Variable &amp; \\\\ \\text{High Dimensional Data} &amp; \\text{Characteristics of a firm purchasing worker&#39;s compensation} \\\\ &amp; ~~~\\text{insurance (location of plants, industry, number of employees,} \\\\ &amp;~~~\\text{and so on)} \\\\ \\text{Spatial Data} &amp; \\text{Longitude/latitude of the location an insurance hailstorm claim} \\\\ \\text{Missing Data} &amp; \\text{Policyholder&#39;s age (continuous/interval) and &quot;-99&quot; for} \\\\ &amp;~~~ \\text{&quot;not reported,&quot; that is, missing} \\\\ \\text{Censored and Truncated Data} &amp; \\text{Amount of insurance claims in excess of a deductible} \\\\ \\text{Aggregate Claims} &amp; \\text{Losses recorded for each claim in a motor vehicle policy.} \\\\ \\text{Stochastic Process Realizations} &amp; \\text{The time and amount of each occurrence of an insured loss} \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.1 : Variable types In data analysis, it is important to understand what type of variable you are working with. For example, Consider a pair of random variables (Coverage,Claim) from the LGPIF data introduced in chapter 1 as displayed in Figure 14.1 below. We would like to know whether the distribution of Coverage depends on the distribution of Claim or whether they are statistically independent. We would also want to know how the Claim distribution depends on the EntityType variable. Because the EntityType variable belongs to a different class of variables, modeling the dependence between Claim and Coverage may require a different technique from that of Claim and EntityType. Figure 14.1: Scatter plot of (Coverage,Claim) from LGPIF data 14.1.1 Qualitative Variables In this sub-section, you learn how to: Classify qualitative variables as nominal or ordinal Describe binary variable A qualitative, or categorical, variable is one for which the measurement denotes membership in a set of groups, or categories. For example, if you were coding which area of the country an insured resides, you might use a 1 for the northern part, 2 for southern, and 3 for everything else. This location variable is an example of a nominal variable, one for which the levels have no natural ordering. Any analysis of nominal variables should not depend on the labeling of the categories. For example, instead of using a 1,2,3 for north, south, other, I should arrive at the same set of summary statistics if I used a 2,1,3 coding instead, interchanging north and south. In contrast, an ordinal variable is a type of categorical variable for which an ordering does exist. For example, with a survey to see how satisfied customers are with our claims servicing department, we might use a five point scale that ranges from 1 meaning dissatisfied to a 5 meaning satisfied. Ordinal variables provide a clear ordering of levels of a variable but the amount of separation between levels is unknown. A binary variable is a special type of categorical variable where there are only two categories commonly taken to be a 0 and a 1. For example, we might code a variable in a dataset to be a 1 if an insured is female and a 0 if male. 14.1.2 Quantitative Variables In this sub-section, you learn how to: Differentiate between continuous and discrete variable Use a combination of continuous and discrete variable Describe circular data Unlike a qualitative variable, a quantitative variable is one in which numerical level is a realization from some scale so that the distance between any two levels of the scale takes on meaning. A continuous variable is one that can take on any value within a finite interval. For example, it is common to represent a policyholder’s age, weight, or income, as a continuous variable. In contrast, a discrete variable is one that takes on only a finite number of values in any finite interval. Like an ordinal variable, these represent distinct categories that are ordered. Unlike an ordinal variable, the numerical difference between levels takes on economic meaning. A special type of discrete variable is a count variable, one with values on the nonnegative integers. For example, we will be particularly interested in the number of claims arising from a policy during a given period. Some variables are inherently a combination of discrete and continuous components. For example, when we analyze the insured loss of a policyholder, we will encounter a discrete outcome at zero, representing no insured loss, and a continuous amount for positive outcomes, representing the amount of the insured loss. Another interesting variation is an interval variable, one that gives a range of possible outcomes. Circular data represent an interesting category typically not analyzed by insurers. As an example of circular data, suppose that you monitor calls to your customer service center and would like to know when is the peak time of the day for calls to arrive. In this context, one can think about the time of the day as a variable with realizations on a circle, e.g., imagine an analog picture of a clock. For circular data, the distance between observations at 00:15 and 00:45 are just as close as observations 23:45 and 00:15 (here, we use the convention HH:MM means hours and minutes). 14.1.3 Multivariate Variables In this sub-section, you learn how to: Differentiate between univariate and multivariate data Handle missing variables Insurance data typically are multivariate in the sense that we can take many measurements on a single entity. For example, when studying losses associated with a firm’s worker’s compensation plan, we might want to know the location of its manufacturing plants, the industry in which it operates, the number of employees, and so forth. The usual strategy for analyzing multivariate data is to begin by examining each variable in isolation of the others. This is known as a univariate approach. In contrast, for some variables, it makes little sense to only look at one dimensional aspects. For example, insurers typically organize spatial data by longitude and latitude to analyze the location of weather related insurance claims due hailstorms. Having only a single number, either longitude or latitude, provides little information in understanding geographical location. Another special case of a multivariate variable, less obvious, involves coding for missing data. When data are missing, it is better to think about the variable as two dimensions, one to indicate whether or not the variable is reported and the second providing the age (if reported). In the same way, insurance data are commonly censored and truncated. We refer you to Chapter 4 for more on censored and truncated data. Aggregate claims can also be coded as another special type of multivariate variable. We refer you to Chapter 5 for more Aggregate claims. Perhaps the most complicated type of multivariate variable is a realization of a stochastic process. You will recall that a stochastic process is little more than a collection of random variables. For example, in insurance, we might think about the times that claims arrive to an insurance company in a one year time horizon. This is a high dimensional variable that theoretically is infinite dimensional. Special techniques are required to understand realizations of stochastic processes that will not be addressed here. 14.2 Classic Measures of Scalar Associations In this section, you learn how to: Estimate correlation using Pearson method Use rank based measures like Spearman, Kendall to estimate correlation Measure dependence using odds ratio,Pearson chi-square and likelihood ratio test statistic Use normal-based correlations to quantify associations involving ordinal variables 14.2.1 Association Measures for Quantitative Variables For this section, consider a pair of random variables \\((X,Y)\\) having joint distribution function \\(F(\\cdot)\\) and a random sample \\((X_i,Y_i), i=1, \\ldots, n\\). For the continuous case, suppose that \\(F(\\cdot)\\) is absolutely continuous with absolutely continuous marginals. 14.2.1.1 Pearson Define the sample covariance function \\(Cov(X,Y) = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})\\), where \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the sample means of \\(X\\) and \\(Y\\), respectively. Then, the product-moment (Pearson) correlation can be written as \\[\\begin{equation*} r = \\frac{Cov(X,Y)}{\\sqrt{Cov(X,X) Cov(Y,Y)}}. \\end{equation*}\\] The correlation statistic \\(r\\) is widely used to capture association between random variables. It is a (nonparametric) estimator of the correlation parameter \\(\\rho\\), defined to be the covariance divided by the product of standard deviations. In this sense, it captures association for any pair of random variables. This statistic has several important features. Unlike regression estimators, it is symmetric between random variables, so the correlation between \\(X\\) and \\(Y\\) equals the correlation between \\(Y\\) and \\(X\\). It is unchanged by linear transformations of random variables (up to sign changes) so that we can multiply random variables or add constants as is helpful for interpretation. The range of the statistic is \\([-1,1]\\) which does not depend on the distribution of either \\(X\\) or \\(Y\\). Further, in the case of independence, the correlation coefficient \\(r\\) is 0. However, it is well known that zero correlation does not imply independence, except for normally distributed random variables. The correlation statistic \\(r\\) is also a (maximum likelihood) estimator of the association parameter for bivariate normal distribution. So, for normally distributed data, the correlation statistic \\(r\\) can be used to assess independence. For additional interpretations of this well-known statistic, readers will enjoy (Lee Rodgers and Nicewander 1998). You can obtain the correlation statistic \\(r\\) using the cor() function in R and selecting the pearson method. This is demonstrated below by using the Coverage rating variable in millions of dollars and Claim amount variable in dollars from the LGPIF data introduced in chapter 1. R Code for Pearson Correlation Statistic ### Pearson correlation between Claim and Coverage ### r&lt;-cor(Claim,Coverage, method = c(&quot;pearson&quot;)) round(r,2) Output: [1] 0.31 ### Pearson correlation between Claim and log(Coverage) ### r&lt;-cor(Claim,log(Coverage), method = c(&quot;pearson&quot;)) round(r,2) Output: [1] 0.1 From R output above, \\(r=0.31\\) , which indicates a positive association between Claim and Coverage. This means that as the coverage amount of a policy increases we expect claim to increase. 14.2.2 Rank Based Measures 14.2.2.1 Spearman The Pearson correlation coefficient does have the drawback that it is not invariant to nonlinear transforms of the data. For example, the correlation between \\(X\\) and \\(\\ln Y\\) can be quite different from the correlation between \\(X\\) and \\(Y\\). As we see from the R code for Pearson correlation statistic above, the correlation statistic \\(r\\) between Coverage rating variable in logarithmic millions of dollars and Claim amounts variable in dollars is \\(0.1\\) as compared to \\(0.31\\) when we calculate the correlation between Coverage rating variable in millions of dollars and Claim amounts variable in dollars. This limitation is one reason for considering alternative statistics. Alternative measures of correlation are based on ranks of the data. Let \\(R(X_j)\\) denote the rank of \\(X_j\\) from the sample \\(X_1, \\ldots, X_n\\) and similarly for \\(R(Y_j)\\). Let \\(R(X) = \\left(R(X_1), \\ldots, R(X_n)\\right)&#39;\\) denote the vector of ranks, and similarly for \\(R(Y)\\). For example, if \\(n=3\\) and \\(X=(24, 13, 109)\\), then \\(R(X)=(2,1,3)\\). A comprehensive introduction of rank statistics can be found in, for example, (Hettmansperger 1984). Also, ranks can be used to obtain the empirical distribution function, refer to section 4.1.1 for more on the empirical distribution function. With this, the correlation measure of (Spearman 1904) is simply the product-moment correlation computed on the ranks: \\[\\begin{equation*} r_S = \\frac{Cov(R(X),R(Y))}{\\sqrt{Cov(R(X),R(X))Cov(R(Y),R(Y))}} = \\frac{Cov(R(X),R(Y))}{(n^2-1)/12} . \\end{equation*}\\] You can obtain the Spearman correlation statistic \\(r_S\\) using the cor() function in R and selecting the spearman method. From below, the Spearman correlation between the Coverage rating variable in millions of dollars and Claim amount variable in dollars is \\(0.41\\). R Code for Spearman Correlation Statistic ### Spearman correlation between Claim and Coverage ### rs&lt;-cor(Claim,Coverage, method = c(&quot;spearman&quot;)) round(rs,2) Output: [1] 0.41 ### Spearman correlation between Claim and log(Coverage) ### rs&lt;-cor(Claim,log(Coverage), method = c(&quot;spearman&quot;)) round(rs,2) Output: [1] 0.41 To show that the Spearman correlation statistic is invariate under strictly increasing transformations , from the R Code for Spearman correlation statistic above, \\(r_S=0.41\\) between the Coverage rating variable in logarithmic millions of dollars and Claim amount variable in dollars. 14.2.2.2 Kendall An alternative measure that uses ranks is based on the concept of concordance. An observation pair \\((X,Y)\\) is said to be concordant (discordant) if the observation with a larger value of \\(X\\) has also the larger (smaller) value of \\(Y\\). Then \\(\\Pr(concordance) = \\Pr[ (X_1-X_2)(Y_1-Y_2) &gt;0 ]\\) , \\(\\Pr(discordance) = \\Pr[ (X_1-X_2)(Y_1-Y_2) &lt;0 ]\\) and \\[\\begin{eqnarray*} \\tau(X,Y)= \\Pr(concordance) - \\Pr(discordance) = 2\\Pr(concordance) - 1 + \\Pr(tie). \\end{eqnarray*}\\] To estimate this, the pairs \\((X_i,Y_i)\\) and \\((X_j,Y_j)\\) are said to be concordant if the product \\(sgn(X_j-X_i)sgn(Y_j-Y_i)\\) equals 1 and discordant if the product equals -1. Here, \\(sgn(x)=1,0,-1\\) as \\(x&gt;0\\), \\(x=0\\), \\(x&lt;0\\), respectively. With this, we can express the association measure of (Kendall 1938), known as Kendall’s tau, as \\[\\begin{equation*} \\begin{array}{rl} \\tau &amp;= \\frac{2}{n(n-1)} \\sum_{i&lt;j}sgn(X_j-X_i)sgn(Y_j-Y_i)\\\\ &amp;= \\frac{2}{n(n-1)} \\sum_{i&lt;j}sgn(R(X_j)-R(X_i))sgn(R(Y_j)-R(Y_i)) \\end{array}. \\end{equation*}\\] Interestingly, (Hougaard 2000), page 137, attributes the original discovery of this statistic to (Fechner 1897), noting that Kendall’s discovery was independent and more complete than the original work. You can obtain the Kendall’s tau, using the cor() function in R and selecting the kendall method. From below, \\(\\tau=0.32\\) between the Coverage rating variable in millions of dollars and Claim amount variable in dollars. R Code for Kendall’s Tau ### Kendall&#39;s tau correlation between Claim and Coverage ### tau&lt;-cor(Claim,Coverage, method = c(&quot;kendall&quot;)) round(tau,2) Output: [1] 0.32 ### Kendall&#39;s tau correlation between Claim and log(Coverage) ### tau&lt;-cor(Claim,log(Coverage), method = c(&quot;kendall&quot;)) round(tau,2) Output: [1] 0.32 Also,to show that the Kendall’s tau is invariate under strictly increasing transformations , \\(\\tau=0.32\\) between the Coverage rating variable in logarithmic millions of dollars and Claim amount variable in dollars. 14.2.3 Nominal Variables 14.2.3.1 Bernoulli Variables To see why dependence measures for continuous variables may not be the best for discrete variables, let us focus on the case of Bernoulli variables that take on simple binary outcomes, 0 and 1. For notation, let \\(\\pi_{jk} = \\Pr(X=j, Y=k)\\) for \\(j,k=0,1\\) and let \\(\\pi_X=\\Pr(X=1)\\) and similarly for \\(\\pi_Y\\). Then, the population version of the product-moment (Pearson) correlation can be easily seen to be \\[\\begin{eqnarray*} \\rho = \\frac{\\pi_{11} - \\pi_X \\pi_Y}{\\sqrt{\\pi_X(1-\\pi_X)\\pi_Y(1-\\pi_Y)}} . \\end{eqnarray*}\\] Unlike the case for continuous data, it is not possible for this measure to achieve the limiting boundaries of the interval \\([-1,1]\\). To see this, students of probability may recall the Fr\\(\\acute{e}\\)chet-H\\(\\ddot{o}\\)effding bounds for a joint distribution that turn out to be \\(\\max\\{0, \\pi_X+\\pi_Y-1\\} \\le \\pi_{11} \\le \\min\\{\\pi_X,\\pi_Y\\}\\) for this joint probability. This limit on the joint probability imposes an additional restriction on the Pearson correlation. As an illustration, assume equal probabilities \\(\\pi_X =\\pi_Y = \\pi &gt; 1/2\\). Then, the lower bound is \\[\\begin{eqnarray*} \\frac{2\\pi - 1 - \\pi^2}{\\pi(1-\\pi)} = -\\frac{1-\\pi}{\\pi} . \\end{eqnarray*}\\] For example, if \\(\\pi=0.8\\), then the smallest that the Pearson correlation could be is -0.25. More generally, there are bounds on \\(\\rho\\) that depend on \\(\\pi_X\\) and \\(\\pi_Y\\) that make it difficult to interpret this measure. As noted by (Y. M. Bishop, Fienberg, and Holland 1975) (page 382), squaring this correlation coefficient yields the Pearson chi-square statistic. Despite the boundary problems described above, this feature makes the Pearson correlation coefficient a good choice for describing dependence with binary data. The other is the odds ratio, described as follows. As an alternative measure for Bernoulli variables, the odds ratio is given by \\[\\begin{eqnarray*} OR(\\pi_{11}) = \\frac{\\pi_{11} \\pi_{00}}{\\pi_{01} \\pi_{10}} = \\frac{\\pi_{11} \\left( 1+\\pi_{11}-\\pi_1 -\\pi_2\\right)}{(\\pi_1-\\pi_{11})(\\pi_2- \\pi_{11})} . \\end{eqnarray*}\\] Pleasant calculations show that \\(OR(z)\\) is \\(0\\) at the lower Fr\\(\\acute{e}\\)chet-H\\(\\ddot{o}\\)effding bound \\(z= \\max\\{0, \\pi_1+\\pi_2-1\\}\\) and is \\(\\infty\\) at the upper bound \\(z=\\min\\{\\pi_1,\\pi_2\\}\\). Thus, the bounds on this measure do not depend on the marginal probabilities \\(\\pi_X\\) and \\(\\pi_Y\\), making it easier to interpret this measure. As noted by (Yule 1900), odds ratios are invariant to the labeling of 0 and 1. Further, they are invariant to the marginals in the sense that one can rescale \\(\\pi_1\\) and \\(\\pi_2\\) by positive constants and the odds ratio remains unchanged. Specifically, suppose that \\(a_i\\), \\(b_j\\) are sets of positive constants and that \\[\\begin{eqnarray*} \\pi_{ij}^{new} &amp;=&amp; a_i b_j \\pi_{ij} \\end{eqnarray*}\\] and \\(\\sum_{ij} \\pi_{ij}^{new}=1.\\) Then, \\[\\begin{eqnarray*} OR^{new} = \\frac{(a_1 b_1 \\pi_{11})( a_0 b_0 \\pi_{00})}{(a_0 b_1 \\pi_{01})( a_1 b_0\\pi_{10})} = \\frac{\\pi_{11} \\pi_{00}}{\\pi_{01} \\pi_{10}} =OR^{old} . \\end{eqnarray*}\\] For additional help with interpretation, Yule proposed two transforms for the odds ratio, the first in (Yule 1900), \\[\\begin{eqnarray*} \\frac{OR-1}{OR+1}, \\end{eqnarray*}\\] and the second in (Yule 1912), \\[\\begin{eqnarray*} \\frac{\\sqrt{OR}-1}{\\sqrt{OR}+1}. \\end{eqnarray*}\\] Although these statistics provide the same information as is the original odds ration \\(OR\\), they have the advantage of taking values in the interval \\([-1,1]\\), making them easier to interpret. In a later section, we will also see that the marginal distributions have no effect on the Fr\\(\\acute{e}\\)chet-H\\(\\ddot{o}\\)effding of the tetrachoric correlation, another measure of association, see also, (Joe 2014), page 48. \\[ {\\small \\begin{matrix} \\begin{array}{|l|rr|r|} \\hline &amp; \\text{Fire5} &amp; &amp; \\\\ \\text{NoClaimCredit} &amp; 0 &amp; 1 &amp; \\text{Total} \\\\ \\hline 0 &amp; 1611 &amp; 2175 &amp; 3786 \\\\ 1 &amp; 897 &amp; 956 &amp; 1853 \\\\ \\hline \\text{Total} &amp; 2508 &amp; 3131 &amp; 5639 \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.2 : 2 \\(\\times\\) 2 table of counts for Fire5 and NoClaimCredit From Table 14.2, \\(OR(\\pi_{11})=\\frac{1611(956)}{897(2175)}=0.79\\). You can obtain the \\(OR(\\pi_{11})\\), using the oddsratio() function from the epitools library in R. From the output below, \\(OR(\\pi_{11})=0.79\\) for the binary variables NoClaimCredit and Fier5 from the LGPIF data. R Code for Odds Ratios library(epitools) oddsratio(NoClaimCredit, Fire5,method = c(&quot;wald&quot;))$measure Output: [1] 0.79 14.2.3.2 Categorical Variables More generally, let \\((X,Y)\\) be a bivariate pair having \\(ncat_X\\) and \\(ncat_Y\\) numbers of categories, respectively. For a two-way table of counts, let \\(n_{jk}\\) be the number in the \\(j\\)th row, \\(k\\) column. Let \\(n_{j\\cdot}\\) be the row margin total and \\(n_{\\cdot k}\\) be the column margin total. Define Pearson chi-square statistic as \\[\\begin{eqnarray*} chi^2 = \\sum_{jk} \\frac{(n_{jk}- n_{j\\cdot}n_{\\cdot k}/n)^2}{n_{j\\cdot}n_{\\cdot k}/n} . \\end{eqnarray*}\\] The likelihood ratio test statistic is \\[\\begin{eqnarray*} G^2 = 2 \\sum_{jk} n_{jk} \\ln\\frac{n_{jk}}{n_{j\\cdot}n_{\\cdot k}/n} . \\end{eqnarray*}\\] Under the assumption of independence, both \\(chi^2\\) and \\(G^2\\) have an asymptotic chi-square distribution with \\((ncat_X-1)(ncat_Y-1)\\) degrees of freedom. To help see what these statistics are estimating, let \\(\\pi_{jk} = \\Pr(X=j, Y=k)\\) and let \\(\\pi_{X,j}=\\Pr(X=j)\\) and similarly for \\(\\pi_{Y,k}\\). Assuming that \\(n_{jk}/n \\approx \\pi_{jk}\\) for large \\(n\\) and similarly for the marginal probabilities, we have \\[\\begin{eqnarray*} \\frac{chi^2}{n} \\approx \\sum_{jk} \\frac{(\\pi_{jk}- \\pi_{X,j}\\pi_{Y,k})^2}{\\pi_{X,j}\\pi_{Y,k}} \\end{eqnarray*}\\] and \\[\\begin{eqnarray*} \\frac{G^2}{n} \\approx 2 \\sum_{jk} \\pi_{jk} \\ln\\frac{\\pi_{jk}}{\\pi_{X,j}\\pi_{Y,k}} . \\end{eqnarray*}\\] Under the null hypothesis of independence, we have \\(\\pi_{jk} =\\pi_{X,j}\\pi_{Y,k}\\) and it is clear from these approximations that we anticipate that these statistics will be small under this hypothesis. Classical approaches, as described in (Y. M. Bishop, Fienberg, and Holland 1975) (page 374), distinguish between tests of independence and measures of associations. The former are designed to detect whether a relationship exists whereas the latter are meant to assess the type and extent of a relationship. We acknowledge these differing purposes but also less concerned with this distinction for actuarial applications. \\[ {\\small \\begin{matrix} \\begin{array}{|l|rr|} \\hline &amp; \\text{NoClaimCredit} &amp; \\\\ \\text{EntityType} &amp; 0 &amp; 1 \\\\ \\hline \\text{City} &amp; 644 &amp; 149 \\\\ \\text{County} &amp; 310 &amp; 18 \\\\ \\text{Misc} &amp; 336 &amp; 273 \\\\ \\text{School} &amp; 1103 &amp; 494 \\\\ \\text{Town} &amp; 492 &amp; 479 \\\\ \\text{Village} &amp; 901 &amp; 440 \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.3 : Two-way table of counts for EntityType and NoClaimCredit You can obtain the Pearson chi-square statistic, using the chisq.test() function from the MASS library in R. Here, we test whether the EntityType variable is independent of NoClaimCredit variable using Table 14.3. R Code for Pearson Chi-square Statistic library(MASS) table = table(EntityType, NoClaimCredit) chisq.test(table) Output: ------------------------------------ Test statistic df P value ---------------- ---- -------------- 344.2 5 3.15e-72 * * * ------------------------------------ Table: Pearson&#39;s Chi-squared test As the p-value is less than the .05 significance level, we reject the null hypothesis that the EntityType is independent of NoClaimCredit. Furthermore, you can obtain the likelihood ratio test statistic , using the likelihood.test() function from the Deducer library in R. From below, we test whether the EntityType variable is independent of NoClaimCredit variable from the LGPIF data. Same conclusion is drawn as the Pearson chi-square test. R Code for Likelihood Ratio Test Statistic library(Deducer) likelihood.test(EntityType, NoClaimCredit) Output: ----------------------------------------- Test statistic X-squared df P value ---------------- -------------- --------- 378.7 5 0 * * * ----------------------------------------- Table: Log likelihood ratio (G-test) test of independence without correction 14.2.3.3 Ordinal Variables As the analyst moves from the continuous to the nominal scale, there are two main sources of loss of information (Y. M. Bishop, Fienberg, and Holland 1975) (page 343). The first is breaking the precise continuous measurements into groups. The second is losing the ordering of the groups. So, it is sensible to describe what we can do with variables that in discrete groups but where the ordering is known. As described in Section 14.1.1, ordinal variables provide a clear ordering of levels of a variable but distances between levels are unknown. Associations have traditionally been quantified parametrically using normal-based correlations and nonparametrically using Spearman correlations with tied ranks. 14.2.3.4 Parametric Approach Using Normal Based Correlations Refer to page 60, Section 2.12.7 of (Joe 2014). Let \\((y_1,y_2)\\) be a bivariate pair with discrete values on \\(m_1, \\ldots, m_2\\). For a two-way table of ordinal counts, let \\(n_{st}\\) be the number in the \\(s\\)th row, \\(t\\) column. Let \\((n_{m_1*}, \\ldots, n_{m_2*})\\) be the row margin total and \\((n_{*m_1}, \\ldots, n_{*m_2})\\) be the column margin total. Let \\(\\hat{\\xi}_{1s} = \\Phi^{-1}((n_{m_1}+\\cdots+n_{s*})/n)\\) for \\(s=m_1, \\ldots, m_2\\) be a cutpoint and similarly for \\(\\hat{\\xi}_{2t}\\). The polychoric correlation, based on a two-step estimation procedure, is \\[\\begin{eqnarray*} \\begin{array}{cr} \\hat{\\rho_N} &amp;=\\text{argmax}_{\\rho} \\sum_{s=m_1}^{m_2} \\sum_{t=m_1}^{m_2} n_{st} \\log\\left\\{ \\Phi_2(\\hat{\\xi}_{1s}, \\hat{\\xi}_{2t};\\rho) -\\Phi_2(\\hat{\\xi}_{1,s-1}, \\hat{\\xi}_{2t};\\rho) \\right.\\\\ &amp; \\left. -\\Phi_2(\\hat{\\xi}_{1s}, \\hat{\\xi}_{2,t-1};\\rho) +\\Phi_2(\\hat{\\xi}_{1,s-1}, \\hat{\\xi}_{2,t-1};\\rho) \\right\\} \\end{array} \\end{eqnarray*}\\] It is called a tetrachoric correlation for binary variables. \\[ {\\small \\begin{matrix} \\begin{array}{|l|rr|} \\hline &amp; \\text{NoClaimCredit} &amp; \\\\ \\text{AlarmCredit} &amp; 0 &amp; 1 \\\\ \\hline 1 &amp; 1669 &amp; 942 \\\\ 2 &amp; 121 &amp; 118 \\\\ 3 &amp; 195 &amp; 132 \\\\ 4 &amp; 1801 &amp; 661 \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.4 : Two-way table of counts for AlarmCredit and NoClaimCredit You can obtain the polychoric or tetrachoric correlation using the polychoric() or tetrachoric() function from the psych library in R. The polychoric correlation is illustrated using Table 14.4. \\(\\hat{\\rho_N}=-0.14\\), which means that there is a negative relationship between AlarmCredit and NoClaimCredit. R Code for Polychoric Correlation library(psych) AlarmCredit&lt;-as.numeric(ifelse(Insample$AC00==1,&quot;1&quot;, ifelse(Insample$AC05==1,&quot;2&quot;, ifelse(Insample$AC10==1,&quot;3&quot;, ifelse(Insample$AC15==1,&quot;4&quot;,0))))) x &lt;- table(AlarmCredit,NoClaimCredit) rhoN&lt;-polychoric(x,correct=FALSE)$rho round(rhoN,2) Output: [1] -0.14 14.2.3.5 Interval Variables As described in Section 14.1.2, interval variables provide a clear ordering of levels of a variable and the numerical distance between any two levels of the scale can be readily interpretable. For example, a claims count variable is an interval variable. For measuring association, both the continuous variable and ordinal variable approaches make sense. The former takes advantage of knowledge of the ordering although assumes continuity. The latter does not rely on the continuity but also does not make use of the information given by the distance between scales. For applications, one type is a count variable, a random variable on the discrete integers. Another is a mixture variable, on that has discrete and continuous components. 14.2.3.6 Discrete and Continuous Variables The polyserial correlation is defined similarly, when one variable (\\(y_1\\)) is continuous and the other (\\(y_2\\)) ordinal. Define \\(z\\) to be the normal score of \\(y_1\\). The polyserial correlation is \\[\\begin{eqnarray*} \\hat{\\rho_N} = \\text{argmax}_{\\rho} \\sum_{i=1}^n \\log\\left\\{ \\phi(z_{i1})\\left[ \\Phi(\\frac{\\hat{\\xi}_{2,y_{i2}} - \\rho z_{i1}} {(1-\\rho^2)^{1/2}}) -\\Phi(\\frac{\\hat{\\xi}_{2,y_{i2-1}} - \\rho z_{i1}} {(1-\\rho^2)^{1/2}}) \\right] \\right\\} \\end{eqnarray*}\\] The biserial correlation is defined similarly, when one variable is continuous and the other binary. \\[ {\\small \\begin{matrix} \\begin{array}{|l|r|r|} \\hline \\text{NoClaimCredit} &amp; \\text{Mean} &amp;\\text{Total} \\\\ &amp; \\text{Claim} &amp;\\text{Claim} \\\\ \\hline 0 &amp; 22,505 &amp; 85,200,483 \\\\ 1 &amp; 6,629 &amp; 12,282,618 \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.5 : Summary of Claim by NoClaimCredit You can obtain the polyserial or biserial correlation using the polyserial() or biserial() function from the psych library in R. Table 14.5 gives the summary of Claim by NoClaimCredit and the biserial correlation is illustrated using R code below. The \\(\\hat{\\rho_N}=-0.04\\) which means that there is a negative correlation between Claim and NoClaimCredit. R Code for Biserial Correlation library(psych) rhoN&lt;-biserial(Claim,NoClaimCredit) round(rhoN,2) Output: [1] -0.04 14.3 Introduction to Copula Copula functions are widely used in statistics and actuarial science literature for dependency modeling. In this section, you learn how to: Describe the multivariate distribution function in terms of a Copula function A \\(copula\\) is a multivariate distribution function with uniform marginals. Specifically, let \\(U_1, \\ldots, U_p\\) be \\(p\\) uniform random variables on \\((0,1)\\). Their distribution function \\[{C}(u_1, \\ldots, u_p) = \\Pr(U_1 \\leq u_1, \\ldots, U_p \\leq u_p),\\] is a copula. We seek to use copulas in applications that are based on more than just uniformly distributed data. Thus, consider arbitrary marginal distribution functions \\({F}_1(y_1)\\),…,\\({F}_p(y_p)\\).Then, we can define a multivariate distribution function using the copula such that \\[{F}(y_1, \\ldots, y_p)= {C}({F}_1(y_1), \\ldots, {F}_p(y_p)).\\] Here, \\(F\\) is a multivariate distribution function in this equation. Sklar (1959) showed that \\(any\\) multivariate distribution function \\(F\\), can be written in the form of this equation, that is, using a copula representation. Sklar also showed that, if the marginal distributions are continuous, then there is a unique copula representation. In this chapter we are just focusing copula modeling with continuous variables. For discrete case, readers can see (Joe 2014);(Genest and Neslohva 2007). For bivariate case; \\(d=2\\) , the distribution function of two random variables can be written by the bivariate copula function: \\[{C}(u_1, \\, u_2) = \\Pr(U_1 \\leq u_1, \\, U_2 \\leq u_2),\\] \\[{F}(y_1, \\, y_2)= {C}({F}_1(y_1), \\, {F}_p(y_2)).\\] To give an example for bivariate copula, we can look at Frank’s (1979) copula. The equation is \\[{C}(u_1,u_2) = \\frac{1}{\\theta} \\ln \\left( 1+ \\frac{ (\\exp(\\theta u_1) -1)(\\exp(\\theta u_2) -1)} {\\exp(\\theta) -1} \\right).\\] This is a bivariate distribution function with its domain on the unit square \\([0,1]^2.\\) Here \\(\\theta\\) is dependence parameter and the range of dependence is controlled by the parameter \\(\\theta\\). Positive association increases as \\(\\theta\\) increases and this positive association can be summarized with Spearman’s rho (\\(\\rho\\)) and Kendall’s tau (\\(\\tau\\)).Frank copula is one of the commonly used copula functions in the copula literature. We will learn more details about copula types in Section 14.5. 14.4 Application Using Copulas In this section, you learn how to: Discover dependence structure between random variables Model the dependence with a copula function This section analyzes the insurance losses and expenses data with the statistical programming “R”. This data set is included in copula package. Model fitting process is started by marginal modeling of two variables (\\(loss\\) and \\(expense\\)). Then we model the joint distribution of these marginal outcomes. 14.4.1 Data Description We start with getting a sample (\\(n = 1500\\)) from the whole data. We consider first two variables of the data; losses and expenses . losses : general liability claims from Insurance Services Office, Inc. (ISO) expenses : ALAE, specifically attributable to the settlement of individual claims (e.g. lawyer’s fees, claims investigation expenses) To visualize the relationship between losses and expenses (ALAE), scatterplots in figure 14.2 are created on the real dollar scale and on the log scale. Figure 14.2: Scatter plot of Loss and ALAE R Code for Scatterplots library(copula) data(loss) # loss data Lossdata &lt;- loss attach(Lossdata) loss &lt;- Lossdata$loss par(mfrow=c(1, 2)) plot(loss,alae, cex=.5) # real dollar scale plot(log(loss),log(alae),cex=.5) # log scale par(mfrow=c(1, 2)) 14.4.2 Marginal Models We need to check the histograms of losses and expenses before going through the joint modeling. The histograms show that both losses and expenses are right-skewed and fat-tailed. For marginal distributions of losses and expenses, we consider a Pareto-type distribution, namely a Pareto type II with distribution function: \\[ F(y)=1- \\left( 1 + \\frac{y}{\\theta} \\right) ^{-\\alpha}\\] where \\(\\theta\\) is the scale parameter and \\(\\alpha\\) is the shape parameter. The marginal distributions of losses and expenses are fitted with regression. \\(vglm\\) function is used for the estimation from VGAM package. Firstly, we fit the marginal distribution of expenses . R Code for Pareto Fitting library(VGAM) fit = vglm(alae ~ 1, paretoII(location=0, lscale=&quot;loge&quot;, lshape=&quot;loge&quot;)) # fit the model by vlgm function coef(fit, matrix=TRUE) # extract fitted model coefficients, matrix=TRUE gives logarithm of estimated parameters instead of default normal scale estimates Coef(fit) Output: loge(scale) loge(shape) (Intercept) 9.624673 0.7988753 scale shape (Intercept) 15133.603598 2.223039 We repeat the procedure above to fit the marginal distribution of the loss variable. Because the loss data also seems right-skewed and heavy-tail data, we model the marginal distribution with Pareto II distribution. R Code for Pareto Fitting fitloss = vglm(loss ~ 1, paretoII, trace=TRUE) Coef(fit) summary(fit) Output: scale shape 15133.603598 2.223039 To visualize the fitted distribution of expenses and loss variables, we use the estimated parameters and plot the corresponding distribution function and density function. For more details on marginal model selection, we refer you to Chapter 4 . 14.4.3 Probability Integral Transformation Probability integral transformation shows us any continuous variable can be mapped to a \\(U(0,1)\\) random variable via its distribution function. Given the fitted Pareto II distribution, the variable expenses is transformed to the variable \\(u_1\\), which follows a uniform distribution on \\([0,1]\\): \\[u_1 = 1 - \\left( 1 + \\frac{ALAE}{\\hat{\\theta}} \\right)^{-\\hat{\\alpha}}.\\] After applying the probability integral transformation to expenses variable, we plot the histogram of Transformed Alae in figure 14.3. Figure 14.3: Histogram of Transformed Alae After fitting process,the variable loss is also transformed to the variable \\(u_2\\), which follows a uniform distribution on \\([0,1]\\). We plot the histogram of Transformed Loss . As an alternative, the variable loss is transformed to \\(normal\\) \\(scores\\) with the quantile function of standard normal distribution. As we see in figure 14.4, normal scores of the variable loss are approximately marginally standard normal. Figure 14.4: Left: Histogram of Transformed Loss. Right:Histogram of normal scores of Loss R Code for The Histogram of Transformed Alae u1 = 1 - (1 + (alae/b))^(-s) # or u1=pparetoII(alae, location=0, scale=b, shape=s) hist(u1, main=&quot;&quot;, xlab=&quot;Histogram of Transformed alae&quot;) scaleloss = Coef(fitloss)[1] shapeloss = Coef(fitloss)[2] u2 = 1 - (1 + (loss/scaleloss))^(-shapeloss) par(mfrow=c(1, 2)) hist(u2, main=&quot;&quot;, xlab=&quot;Histogram of Transformed Loss&quot;) hist(qnorm(u2), main=&quot;&quot;, xlab=&quot;Histogram of qnorm(Loss)&quot;) 14.4.4 Joint Modeling with Copula Function Before jointly modeling losses and expenses, we draw the scatterplot of transformed variables \\((u_1, u_2)\\) and the scatterplot of normal scores in figure 14.5. Then we calculate the Spearman’s rho correlation between these two uniform random variables. Figure 14.5: Left: Scatter plot for transformed variables. Right:Scatter plot for normal scores R Code for Scatter Plots and Correlation par(mfrow=c(1, 2)) plot(u1,u2, cex=0.5, xlim=c(-0.1,1.1), ylim=c(-0.1,1.1), xlab=&quot;Transformed Alae&quot;, ylab=&quot;Transformed Loss&quot;) plot(qnorm(u1),qnorm(u2)) cor(u1,u2, method=&quot;spearman&quot;) Output: [1] 0.451872 Scatter plots and Spearman’s rho correlation value (0.451) shows us there is a positive dependency between these two uniform random variables.It is more clear to see the relationship with normal scores in the second graph. To learn more details about normal scores, readers can see (Joe 2014). \\((U_1, U_2)\\), (\\(U_1 = F_1(ALAE)\\) and \\(U_2=F_2(LOSS)\\)), is fitted to Frank’s copula with Maximum likelihood method. R Code for Modeling with Frank Copula uu = cbind(u1,u2) frank.cop &lt;- archmCopula(&quot;frank&quot;, param= c(5), dim = 2) fit.ml &lt;- fitCopula(frank.cop, uu, method=&quot;ml&quot;, start=c(0.4)) summary(fit.ml) Output: Call: fitCopula(copula, data = data, method = &quot;ml&quot;, start = ..2) Fit based on &quot;maximum likelihood&quot; and 1500 2-dimensional observations. Copula: frankCopula param 3.114 The maximized loglikelihood is 172.6 Convergence problems: code is 52 see ?optim. Call: fitCopula(copula, data = data, method = &quot;ml&quot;, start = ..2) Fit based on &quot;maximum likelihood&quot; and 1500 2-dimensional observations. Frank copula, dim. d = 2 Estimate Std. Error param 3.114 NA The maximized loglikelihood is 172.6 Convergence problems: code is 52 see ?optim. Number of loglikelihood evaluations: function gradient 45 45 The fitted model implies that losses and expenses are positively dependent and their dependence is significant. We use the fitted parameter to update the Frank’s copula. The Spearman’s correlation corresponding to the fitted copula parameter(3.114) is calculated with the rho function. In this case, the Spearman’s correlation coefficient is 0.462, which is very close to the sample Spearman’s correlation coefficient; 0.452. R Code for Spearman’s Correlation Using Frank’s Copula (param = fit.ml@estimate) frank.cop &lt;- archmCopula(&quot;frank&quot;, param= param, dim = 2) rho(frank.cop) Output : [1] 0.4622722 To visualize the fitted Frank’s copula, the distribution function and density function perspective plots are drawn in figure 14.6. Figure 14.6: Left: Plot for distribution function for Franks Copula. Right:Plot for density function for Franks Copula R Code for Frank’s Copula Plots par(mar=c(3.2,3,.2,.2),mfrow=c(1,2)) persp(frank.cop, pCopula, theta=50, zlab=&quot;C(u,v)&quot;, xlab =&quot;u&quot;, ylab=&quot;v&quot;, cex.lab=1.3) persp(frank.cop, dCopula, theta=0, zlab=&quot;c(u,v)&quot;, xlab =&quot;u&quot;, ylab=&quot;v&quot;, cex.lab=1.3) Frank’s copula models positive dependence for this data set, with \\(\\theta=3.114\\). For Frank’s copula, the dependence is related to values of \\(\\theta\\). That is: \\(\\theta=0\\): independent copula \\(\\theta&gt;0\\): positive dependence \\(\\theta&lt;0\\): negative dependence 14.5 Types of Copulas We will go on reviewing copula functions with more details. In this section, you learn how to: Define the basic families of the copula functions Calculate the association coefficients by the help of copula functions There are several families of copulas have been described in the literature. Two main families of the copula families are Archimedian copulas and Elliptical copulas. 14.5.1 Elliptical Copulas Elliptical copulas are constructed from elliptical distributions.This copula decompose (multivariate) elliptical distributions into their univariate elliptical marginal distributions by Sklar’s theorem (Hofert et al. 2017). Properties of elliptical copulas are typically obtained from the properties of corresponding elliptical distributions(Hofert et al. 2017). The normal distribution is a special type of elliptical distribution. To introduce the elliptical class of copulas, we start with the familiar multivariate normal distribution with probability density function \\[\\phi_N (\\mathbf{z})= \\frac{1}{(2 \\pi)^{p/2}\\sqrt{\\det \\boldsymbol \\Sigma}} \\exp\\left( -\\frac{1}{2} \\mathbf{z}^{\\prime} \\boldsymbol \\Sigma^{-1}\\mathbf{z}\\right).\\] Here, \\(\\boldsymbol \\Sigma\\) is a correlation matrix, with ones on the diagonal. Let \\(\\Phi\\) and \\(\\phi\\) denote the standard normal distribution and density functions. We define the Gaussian (normal) copula density function as \\[{c}_N(u_1, \\ldots, u_p) = \\phi_N \\left(\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_p) \\right) \\prod_{j=1}^p \\frac{1}{\\phi(\\Phi^{-1}(u_j))}.\\] As with other copulas, the domain is the unit cube \\([0,1]^p\\). Specifically, a \\(p\\)-dimensional vector \\({z}\\) has an \\({elliptical}\\) \\({distribution}\\) if the density can be written as \\[h_E (\\mathbf{z})= \\frac{k_p}{\\sqrt{\\det \\boldsymbol \\Sigma}} g_p \\left( \\frac{1}{2} (\\mathbf{z}- \\boldsymbol \\mu)^{\\prime} \\boldsymbol \\Sigma^{-1}(\\mathbf{z}- \\boldsymbol \\mu) \\right).\\] We will use elliptical distributions to generate copulas. Because copulas are concerned primarily with relationships, we may restrict our considerations to the case where \\(\\mu = \\mathbf{0}\\) and \\(\\boldsymbol \\Sigma\\) is a correlation matrix. With these restrictions, the marginal distributions of the multivariate elliptical copula are identical; we use \\(H\\) to refer to this marginal distribution function and \\(h\\) is the corresponding density. This marginal density is \\(h(z) = k_1 g_1(z^2/2).\\) We are now ready to define the \\(elliptical\\) \\(copula\\), a function defined on the unit cube \\([0,1]^p\\) as \\[{c}_E(u_1, \\ldots, u_p) = h_E \\left(H^{-1}(u_1), \\ldots, H^{-1}(u_p) \\right) \\prod_{j=1}^p \\frac{1}{h(H^{-1}(u_j))}.\\] In the Elliptical copula family, the function \\(g_p\\) is known as a ``generator’’ in that it can be used to generate alternative distributions. \\[ \\small\\begin{array}{lc} \\hline &amp; Generator \\\\ Distribution &amp; \\mathrm{g}_p(x) \\\\ \\hline \\text{Normal distribution} &amp; e^{-x}\\\\ \\text{t-distribution with r degrees of freedom} &amp; (1+2x/r)^{-(p+r)/2}\\\\ \\text{Cauchy} &amp; (1+2x)^{-(p+1)/2}\\\\ \\text{Logistic} &amp; e^{-x}/(1+e^{-x})^2\\\\ \\text{Exponential power} &amp; \\exp(-rx^s)\\\\ \\hline \\end{array} \\] Table 14.6 : Distribution and Generator Functions (\\(\\mathrm{g}_p(x)\\)) for Selected Elliptical Copulas Most empirical work focuses on the normal copula and \\(t\\)-copula. \\(t\\)-copulas are useful for modeling the dependency in the tails of bivariate distributions,especially in financial risk analysis applications. The \\(t\\)-copulas with same association parameter in varying the degrees of freedom parameter show us different tail dependency structures. For more information on about \\(t\\)-copulas readers can see (Joe 2014),(Hofert et al. 2017). 14.5.2 Archimedian Copulas This class of copulas are constructed from a \\(generator\\) function,which is \\(\\mathrm{g}(\\cdot)\\) is a convex, decreasing function with domain [0,1] and range \\([0, \\infty)\\) such that \\(\\mathrm{g}(0)=0\\). Use \\(\\mathrm{g}^{-1}\\) for the inverse function of \\(\\mathrm{g}\\). Then the function \\[\\mathrm{C}_{\\mathrm{g}}(u_1, \\ldots, u_p) = \\mathrm{g}^{-1} \\left( \\mathrm{g}(u_1)+ \\cdots + \\mathrm{g}(u_p) \\right)\\] is said to be an copula. The function ``\\(\\mathrm{g}\\)’’ is known as the of the copula \\(\\mathrm{C}_{\\mathrm{g}}\\). For bivariate case; \\(d=2\\) , Archimedean copula function can be written by the function \\[\\mathrm{C}_{\\mathrm{g}}(u_1, \\, u_2) = \\mathrm{g}^{-1} \\left( \\mathrm{g}(u_1) + \\mathrm{g}(u_2) \\right).\\] Some important special cases of Archimedean copulas are Frank copula, Clayton/Cook-Johnson copula, Gumbel/Hougaard copula.This copula classes are derived from different generator functions. We can remember that we mentioned about Frank’s copula with details in Section 14.3 and in Section 14.4. Here we will continue to express the equations for Clayton copula and Gumbel/Hougaard copula. 14.5.2.1 Clayton Copula For \\(d=2\\) , the Clayton copula is parameterized by \\(\\theta \\in [-1,\\infty)\\) is defined by \\[C_{\\theta}^C(u)=\\max\\{u_1^{-\\theta}+u_2^{-\\theta}-1,0\\}^{1/\\theta}, \\quad u\\in[0,1]^2.\\] This is a bivariate distribution function of Clayton copula defined in unit square \\([0,1]^2.\\) The range of dependence is controlled by the parameter \\(\\theta\\) as the same as Frank copula. 14.5.2.2 Gumbel-Hougaard copula The Gumbel-Hougaarg copula is parametrized by \\(\\theta \\in [1,\\infty)\\) and defined by \\[C_{\\theta}^{GH}(u)=\\exp\\left(-\\left(\\sum_{i=1}^2 (-\\log u_i)^{\\theta}\\right)^{1/\\theta}\\right), \\quad u\\in[0,1]^2.\\] Readers seeking deeper background on Archimedean copulas can see (Joe 2014);(E. Frees and Valdez 1998); (Genest and Mackay 1986). 14.5.3 Properties of Copulas 14.5.3.1 Bounds on Association Like all multivariate distribution functions, copulas are bounded. The Fr\\(&#39;{e}\\)chet-Hoeffding bounds are \\[\\max( u_1 +\\cdots+ u_p + p -1, 0) \\leq \\mathrm{C}(u_1, \\ldots, u_p) \\leq \\min (u_1, \\ldots,u_p).\\] To see the right-hand side of the equation, note that \\[\\mathrm{C}(u_1,\\ldots, u_p) = \\Pr(U_1 \\leq u_1, \\ldots, U_p \\leq u_p) \\leq \\Pr(U_j \\leq u_j)\\], for \\(j=1,\\ldots,p\\). The bound is achieved when \\(U_1 = \\cdots = U_p\\). To see the left-hand side when \\(p=2\\), consider \\(U_2=1-U_1\\). In this case, if \\(1-u_2 &lt; u_1\\) then \\(\\Pr(U_1 \\leq u_1, U_2 \\leq u_2) = \\Pr ( 1-u_2 \\leq U_1 &lt; u_1) =u_1+u_2-1.\\) (Nelson 1997) The product copula is \\(\\mathrm{C}(u_1,u_2)=u_1u_2\\) is the result of assuming independence between random variables. The lower bound is achieved when the two random variables are perfectly negatively related (\\(U_2=1-U_1\\)) and the upper bound is achieved when they are perfectly positively related (\\(U_2=U_1\\)). We can see The Frechet-Hoeffding bounds for two random variables in the figure 14.7. Figure 14.7: Perfect Positive and Perfect negative dependence plots R Code for Frechet-Hoeffding Bounds for Two Random Variables library(copula) n&lt;-100 set.seed(1980) U&lt;-runif(n) par(mfrow=c(1, 2)) plot(cbind(U,1-U), xlab=quote(U[1]), ylab=quote(U[2]),main=&quot;Perfect Negative Dependency&quot;) # W for d=2 plot (cbind(U,U), xlab=quote(U[1]),ylab=quote(U[2]),main=&quot;Perfect Positive Dependency&quot;) #M for d=2 14.5.3.2 Measures of Association Schweizer and Wolff (1981) established that the copula accounts for all the dependence between two random variables, \\(Y_1\\) and \\(Y_2\\), in the following sense. Consider m\\(_1\\) and m\\(_2\\), strictly increasing functions. Thus, the manner in which \\(Y_1\\) and \\(Y_2\\) ``move together’’ is captured by the copula, regardless of the scale in which each variable is measured. Schweizer and Wolff also showed the two standard nonparametric measures of association could be expressed solely in terms of the copula function. Spearman’s correlation coefficient is given by \\[= 12 \\int \\int \\left\\{\\mathrm{C}(u,v) - uv \\right\\} du dv.\\] And Kendall’s tau is given by \\[= 4 \\int \\int \\mathrm{C}(u,v)d\\mathrm{C}(u,v) - 1 .\\] For these expressions, we assume that \\(Y_1\\) and \\(Y_2\\) have a jointly continuous distribution function. Further, the definition of Kendall’s tau uses an independent copy of (\\(Y_1\\), \\(Y_2\\)), labeled (\\(Y_1^{\\ast}\\), \\(Y_2^{\\ast}\\)), to define the measure of ``concordance.’’ the widely used Pearson correlation depends on the margins as well as the copula. Because it is affected by non-linear changes of scale. 14.5.3.3 Tail Dependency There are some applications in which it is useful to distinguish by the part of the distribution in which the association is strongest. For example, in insurance it is helpful to understand association among the largest losses, that is, association in the right tails of the data. To capture this type of dependency, we use the right-tail concentration function. The function is \\[R(z) = \\frac{\\Pr(U_1 &gt;z, U_2 &gt; z)}{1-z} =\\Pr(U_1 &gt; z | U_2 &gt; z) =\\frac{1 - 2z + \\mathrm{C}(z,z)}{1-z} .\\] From this equation ,\\(R(z)\\) will equal to \\(z\\) under independence. Joe (1997) uses the term `“upper tail dependence parameter” for \\(R = \\lim_{z \\rightarrow 1} R(z)\\). Similarly, the left-tail concentration function is \\[L(z) = \\frac{\\Pr(U_1 \\leq z, U_2 \\leq z)}{z}=\\Pr(U_1 \\leq z | U_2 \\leq z) =\\frac{ \\mathrm{C}(z,z)}{1-z}.\\] Tail dependency concentration function captures the probability of two random variables both catching up extreme values. We calculate the left and right tail concentration functions for four different types of copulas; Normal, Frank,Gumbel and t copula. After getting tail concentration functions for each copula, we show concentration function’s values for these four copulas in Table 14.7. As in (???), we show \\(L(z)\\) for \\(z\\leq 0.5\\) and \\(R(z)\\) for \\(z&gt;0.5\\) in the tail dependence plot in figure 14.8. We interpret the tail dependence plot, to mean that both the Frank and Normal copula exhibit no tail dependence whereas the \\(t\\) and the Gumbel may do so. The \\(t\\) copula is symmetric in its treatment of upper and lower tails. \\[ {\\small \\begin{matrix} \\begin{array}{|l|rr|} \\hline \\text{Copula} &amp; \\text{Lower} &amp; \\text{Upper} \\\\ \\hline \\text{Frank} &amp; 0 &amp; 0 \\\\ \\text{Gumbel} &amp; 0 &amp; 0.74 \\\\ \\text{Normal} &amp; 0 &amp; 0 \\\\ \\text{t} &amp; 0.10 &amp; 0.10 \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.7 : Tail concentration function values for different copulas R Code for Tail Copula Functions for Different Copulas library(copula) U1 = seq(0,0.5, by=0.002) U2 = seq(0.5,1, by=0.002) U = rbind(U1, U2) TailFunction &lt;- function(Tailcop) { lowertail &lt;- pCopula(cbind(U1,U1), Tailcop)/U1 uppertail &lt;- (1-2*U2 +pCopula(cbind(U2,U2), Tailcop))/(1-U2) jointtail &lt;- rbind(lowertail,uppertail) } Tailcop1 &lt;- archmCopula(family = &quot;frank&quot;, param= c(0.05), dim = 2) Tailcop2 &lt;- archmCopula(family = &quot;gumbel&quot;,param = 3) Tailcop3 &lt;- ellipCopula(&quot;normal&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;) Tailcop4 &lt;- ellipCopula(&quot;t&quot;, param = c(0.25),dim = 2, dispstr = &quot;un&quot;, df=5) jointtail1 &lt;- TailFunction(Tailcop1) jointtail2 &lt;- TailFunction(Tailcop2) jointtail3 &lt;- TailFunction(Tailcop3) jointtail4 &lt;- TailFunction(Tailcop4) tailIndex(Tailcop1) tailIndex(Tailcop2) tailIndex(Tailcop3) tailIndex(Tailcop4) Figure 14.8: Tail dependence plots R Code for Tail Dependence Plots for Different Copulas plot(U,jointtail1, cex=.2, xlim=c(0,1),ylab=&quot;Tail Dependence&quot;, ylim=c(0,1)) lines(U,jointtail2, type=&quot;p&quot;,lty=1, cex=.2) lines(U,jointtail3, type=&quot;p&quot;,lty=1, cex=.2) lines(U,jointtail4, type=&quot;p&quot;,lty=1, cex=.2) text(0.75, 0.1, &quot;Frank&quot;, cex=1.3) #1 text(0.1, 0.8, &quot;Gumbel&quot;, cex=1.3) #2 text(0.25, 0.1, &quot;normal&quot;, cex=1.3) #3 arrows(.17, 0.1, .07, 0.12,code=2, angle=20, length=0.1) text(0.9, 0.4, &quot;t with 5 df&quot;, cex=1.3) #4 14.6 Why is Dependence Modeling Important? Dependence Modeling is important because it enables us to understand the dependence structure by defining the relationship between variables in a dataset. In insurance, ignoring dependence modeling may not impact pricing but could lead to misestimation of required capital to cover losses. For instance, from Section 14.4 , it is seen that there was a positive relationship between Loss and Expense. This means that, if there is a large loss then we expect expenses to be large as well and ignoring this relationship could lead to misestimation of reserves. To illustrate the importance of dependence modeling, we refer you back to Portfolio Management example in Chapter 6 that assumed that the property and liability risks are independent. Here, we incorporate dependence by allowing the 4 lines of business to depend on one another through a Gaussian copula. In Table 14.8, we show that dependence affects the portfolio quantiles (\\(VaR_q\\)), although not the expect value. For instance , the \\(VaR_{0.99}\\) for total risk which is the amount of capital required to ensure, with a \\(99\\%\\) degree of certainty that the firm does not become technically insolvent is higher when we incorporate dependence. This leads to less capital being allocated when dependence is ignored and can cause unexpected solvency problems. \\[ {\\small \\begin{matrix} \\begin{array}{|l|rrrr|} \\hline \\text{Independent} &amp;\\text{Expected} &amp; VaR_{0.9} &amp; VaR_{0.95} &amp; VaR_{0.99} \\\\ &amp;\\text{Value} &amp; &amp; &amp; \\\\ \\hline \\text{Retained} &amp; 269 &amp; 300 &amp; 300 &amp; 300 \\\\ \\text{Insurer} &amp; 2,274 &amp; 4,400 &amp; 6,173 &amp; 11,859 \\\\ \\text{Total} &amp; 2,543 &amp; 4,675 &amp; 6,464 &amp; 12,159 \\\\ \\hline \\text{Gaussian Copula}&amp;\\text{Expected}&amp; VaR_{0.9} &amp; VaR_{0.95} &amp; VaR_{0.99} \\\\ &amp;\\text{Value} &amp; &amp; &amp; \\\\ \\hline \\text{Retained} &amp; 269 &amp; 300 &amp; 300 &amp; 300 \\\\ \\text{Insurer} &amp; 2,340 &amp; 4,988 &amp; 7,339 &amp; 14,905 \\\\ \\text{Total} &amp; 2,609 &amp; 5,288 &amp; 7,639 &amp; 15,205 \\\\ \\hline \\end{array} \\end{matrix}} \\] Table 14.8 : Results for portfolio expected value and quantiles (\\(VaR_q\\)) R Code for Simulation Using Gaussian Copula # For the gamma distributions, use alpha1 &lt;- 2; theta1 &lt;- 100 alpha2 &lt;- 2; theta2 &lt;- 200 # For the Pareto distributions, use alpha3 &lt;- 2; theta3 &lt;- 1000 alpha4 &lt;- 3; theta4 &lt;- 2000 # Deductibles d1 &lt;- 100 d2 &lt;- 200 # Simulate the risks nSim &lt;- 10000 #number of simulations set.seed(2017) #set seed to reproduce work X1 &lt;- rgamma(nSim,alpha1,scale = theta1) X2 &lt;- rgamma(nSim,alpha2,scale = theta2) # For the Pareto Distribution, use library(VGAM) X3 &lt;- rparetoII(nSim,scale=theta3,shape=alpha3) X4 &lt;- rparetoII(nSim,scale=theta4,shape=alpha4) # Portfolio Risks S &lt;- X1 + X2 + X3 + X4 Sretained &lt;- pmin(X1,d1) + pmin(X2,d2) Sinsurer &lt;- S - Sretained # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S)))) colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(ExpVec,digits=2) # Quantiles quantMat &lt;- rbind( quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(Sinsurer, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(S , probs=c(0.80, 0.90, 0.95, 0.99))) rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(quantMat,digits=2) plot(density(S), main=&quot;Density of Total Portfolio Risk S&quot;, xlab=&quot;S&quot;) ### Normal Copula ## library(VGAM) library(copula) library(GB2) library(statmod) library(numDeriv) set.seed(2017) parm&lt;-c(0.5,0.5,0.5,0.5,0.5,0.5) nc &lt;- normalCopula(parm, dim = 4, dispstr = &quot;un&quot;) mcc &lt;- mvdc(nc, margins = c(&quot;gamma&quot;, &quot;gamma&quot;,&quot;paretoII&quot;,&quot;paretoII&quot;), paramMargins = list(list(scale = theta1, shape=alpha1), list(scale = theta2, shape=alpha2), list(scale = theta3, shape=alpha3), list(scale = theta4, shape=alpha4))) X &lt;- rMvdc(nSim, mvdc = mcc) X1&lt;-X[,1] X2&lt;-X[,2] X3&lt;-X[,3] X4&lt;-X[,4] # Portfolio Risks S &lt;- X1 + X2 + X3 + X4 Sretained &lt;- pmin(X1,d1) + pmin(X2,d2) Sinsurer &lt;- S - Sretained # Expected Claim Amounts ExpVec &lt;- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S)))) colnames(ExpVec) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(ExpVec,digits=2) # Quantiles quantMat &lt;- rbind( quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(Sinsurer, probs=c(0.80, 0.90, 0.95, 0.99)), quantile(S , probs=c(0.80, 0.90, 0.95, 0.99))) rownames(quantMat) &lt;- c(&quot;Retained&quot;, &quot;Insurer&quot;,&quot;Total&quot;) round(quantMat,digits=2) plot(density(S), main=&quot;Density of Total Portfolio Risk S&quot;, xlab=&quot;S&quot;) Technical Supplement A. Other Classic Measures of Scalar Associations A.1. Blomqvist’s Beta (Blomqvist 1950) developed a measure of dependence now known as Blomqvist’s beta, also called the median concordance coefficient and the medial correlation coefficient. Using distribution functions, this parameter can be expressed as \\[\\begin{equation*} \\beta = 4F\\left(F^{-1}_X(1/2),F^{-1}_Y(1/2) \\right) - 1. \\end{equation*}\\] That is, first evaluate each marginal at its median (\\(F^{-1}_X(1/2)\\) and \\(F^{-1}_Y(1/2)\\), respectively). Then, evaluate the bivariate distribution function at the two medians. After rescaling (multiplying by 4 and subtracting 1), the coefficient turns out to have a range of \\([-1,1]\\), where 0 occurs under independence. Like Spearman’s rho and Kendall’s tau, an estimator based on ranks is easy to provide. First write \\(\\beta = 4C(1/2,1/2)-1 = 2\\Pr((U_1-1/2)(U_2-1/2))-1\\) where \\(U_1, U_2\\) are uniform random variables. Then, define \\[\\begin{equation*} \\hat{\\beta} = \\frac{2}{n} \\sum_{i=1}^n I\\left( (R(X_{i})-\\frac{n+1}{2})(R(Y_{i})-\\frac{n+1}{2}) \\ge 0 \\right)-1 . \\end{equation*}\\] See, for example, (Joe 2014), page 57 or (Hougaard 2000), page 135, for more details. Because Blomqvist’s parameter is based on the center of the distribution, it is particularly useful when data are censored; in this case, information in extreme parts of the distribution are not always reliable. How does this affect a choice of association measures? First, recall that association measures are based on a bivariate distribution function. So, if one has knowledge of a good approximation of the distribution function, then calculation of an association measure is straightforward in principle. Second, for censored data, bivariate extensions of the univariate Kaplan-Meier distribution function estimator are available. For example, the version introduced in (Dabrowska 1988) is appealing. However, because of instances when large masses of data appear at the upper range of the data, this and other estimators of the bivariate distribution function are unreliable. This means that, summary measures of the estimated distribution function based on Spearman’s rho or Kendall’s tau can be unreliable. For this situation, Blomqvist’s beta appears to be a better choice as it focuses on the center of the distribution. (Hougaard 2000), Chapter 14, provides additional discussion. You can obtain the Blomqvist’s beta, using the betan() function from the copula library in R. From below, \\(\\beta=0.3\\) between the Coverage rating variable in millions of dollars and Claim amount variable in dollars. R Code for Blomqvist’s Beta ### Blomqvist&#39;s beta correlation between Claim and Coverage ### library(copula) n&lt;-length(Claim) U&lt;-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(Coverage))) beta&lt;-betan(U, scaling=FALSE) round(beta,2) Output: [1] 0.3 ### Blomqvist&#39;s beta correlation between Claim and log(Coverage) ### n&lt;-length(Claim) Fx&lt;-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(log(Coverage)))) beta&lt;-betan(Fx, scaling=FALSE) round(beta,2) Output: [1] 0.3 In addition,to show that the Blomqvist’s beta is invariate under strictly increasing transformations , \\(\\beta=0.3\\) between the Coverage rating variable in logarithmic millions of dollars and Claim amount variable in dollars. A.2. Nonparametric Approach Using Spearman Correlation with Tied Ranks For the first variable, the average rank of observations in the \\(s\\)th row is \\[\\begin{equation*} r_{1s} = n_{m_1*}+ \\cdots+ n_{s-1,*}+ \\frac{1}{2} \\left(1+ n_{s*}\\right) \\end{equation*}\\] and similarly \\(r_{2t} = \\frac{1}{2} \\left[(n_{*m_1}+ \\cdots+ n_{*,s-1}+1)+ (n_{*m_1}+ \\ldots+ n_{*s})\\right]\\). With this, we have Spearman’s rho with tied rank is \\[\\begin{equation*} \\hat{\\rho}_S = \\frac{\\sum_{s=m_1}^{m_2} \\sum_{t=m_1}^{m_2} n_{st}(r_{1s} - \\bar{r})(r_{2t} - \\bar{r})} {\\left[\\sum_{s=m_1}^{m_2}n_{s*}(r_{1s} - \\bar{r})^2 \\sum_{t=m_1}^{m_2} n_{*t}(r_{2t} - \\bar{r})^2 \\right]^2} \\end{equation*}\\] where the average rank is \\(\\bar{r} = (n+1)/2\\). Click to Show Proof for Special Case: Binary Data. Special Case: Binary Data. Here, \\(m_1=0\\) and \\(m_2=1\\). For the first variable ranks, we have \\(r_{10} = (1+n_{0+})/2\\) and \\(r_{11} = (n_{0+}+1+n)/2\\). Thus, \\(r_{10} -\\bar{r}= (n_{0+}-n)/2\\) and \\(r_{11}-\\bar{r} = n_{0+}/2\\). This means that we have \\(\\sum_{s=0}^{1}n_{s+}(r_{1s} - \\bar{r})^2 = n (n-n_{0+})n_{0+}/4\\) and similarly for the second variable. For the numerator, we have \\[\\begin{eqnarray*} \\sum_{s=0}^{1} \\sum_{t=0}^{1} &amp;&amp; n_{st}(r_{1s} - \\bar{r})(r_{2t} - \\bar{r})\\\\ &amp;=&amp; n_{00} \\frac{n_{0+}-n}{2} \\frac{n_{+0}-n}{2} +n_{01} \\frac{n_{0+}-n}{2} \\frac{n_{+0}}{2} +n_{10} \\frac{n_{0+}}{2} \\frac{n_{+0}-n}{2} +n_{11} \\frac{n_{0+}}{2} \\frac{n_{+0}}{2} \\\\ &amp;=&amp; \\frac{1}{4}(n_{00} (n_{0+}-n) (n_{+0}-n) +(n_{0+}-n_{00}) (n_{0+}-n)n_{+0} \\\\ &amp;&amp; ~ ~ ~ +(n_{+0}-n_{00}) n_{0+}(n_{+0}-n) +(n-n_{+0}-n_{0+}+n_{00}) n_{0+}n_{+0} ) \\\\ &amp;=&amp; \\frac{1}{4}(n_{00} n^2 - n_{0+} (n_{0+}-n)n_{+0} \\\\ &amp;&amp; ~ ~ ~ +n_{+0} n_{0+}(n_{+0}-n) +(n-n_{+0}-n_{0+}) n_{0+}n_{+0} ) \\\\ &amp;=&amp; \\frac{1}{4}(n_{00} n^2 - n_{0+}n_{+0} (n_{0+}-n +n_{+0}-n +n-n_{+0}-n_{0+}) \\\\ &amp;=&amp; \\frac{n}{4}(n n_{00} - n_{0+}n_{+0}) . \\end{eqnarray*}\\] This yields \\[\\begin{eqnarray*} \\hat{\\rho}_S &amp;=&amp; \\frac{n(n n_{00} - n_{0+}n_{+0})} {4\\sqrt{(n (n-n_{0+})n_{0+}/4)(n (n-n_{+0})n_{+0}/4)}} \\\\ &amp;=&amp; \\frac{n n_{00} - n_{0+}n_{+0}} {\\sqrt{ n_{0+} n_{+0}(n-n_{0+}) (n-n_{+0})}} \\\\ &amp;=&amp; \\frac{n_{00} - n (1-\\hat{\\pi}_X)(1- \\hat{\\pi}_Y)} {\\sqrt{\\hat{\\pi}_X(1-\\hat{\\pi}_X)\\hat{\\pi}_Y(1-\\hat{\\pi}_Y) }} \\end{eqnarray*}\\] where \\(\\hat{\\pi}_X = (n-n_{0+})/n\\) and similarly for \\(\\hat{\\pi}_Y\\). Note that this is same form as the Pearson measure. From this, we see that the joint count \\(n_{00}\\) drives this association measure. You can obtain the ties-corrected Spearman correlation statistic \\(r_S\\) using the cor() function in R and selecting the spearman method. From below \\(\\hat{\\rho}_S=-0.09\\) R Code for Ties-corrected Spearman Correlation rs_ties&lt;-cor(AlarmCredit,NoClaimCredit, method = c(&quot;spearman&quot;)) round(rs_ties,2) Output: [1] -0.09 Bibliography "],
["C-AppA.html", "Chapter 15 Appendix A: Review of Statistical Inference 15.1 Basic Concepts 15.2 Point Estimation and Properties 15.3 Interval Estimation 15.4 Hypothesis Testing", " Chapter 15 Appendix A: Review of Statistical Inference Chapter preview. The appendix gives an overview of concepts and methods related to statistical inference on the population of interest, using a random sample of observations from the population. In the appendix, Section 15.1 introduces the basic concepts related to the population and the sample used for making the inference. Section 15.2 presents the commonly used methods for point estimation of population characteristics. Section 15.3 demonstrates interval estimation that takes into consideration the uncertainty in the estimation, due to use of a random sample from the population. Section 15.4 introduces the concept of hypothesis testing for the purpose of variable and model selection. 15.1 Basic Concepts In this section, you learn the following concepts related to statistical inference. Random sampling from a population that can be summarized using a list of items or individuals within the population Sampling distributions that characterize the distributions of possible outcomes for a statistic calculated from a random sample The central limit theorem that guides the distribution of the mean of a random sample from the population Statistical inference is the process of making conclusions on the characteristics of a large set of items/individuals (i.e., the population), using a representative set of data (e.g., a random sample) from a list of items or individuals from the population that can be sampled. While the process has a broad spectrum of applications in various areas including science, engineering, health, social, and economic fields, statistical inference is important to insurance companies that use data from their existing policy holders in order to make inference on the characteristics (e.g., risk profiles) of a specific segment of target customers (i.e., the population) whom the insurance companies do not directly observe. Show An Empirical Example Using the Wisconsin Property Fund Example – Wisconsin Property Fund. Assume there are 1,377 individual claims from the 2010 experience. Minimum First Quartile Median Mean Third Quartile Maximum Standard Deviation Claims 1 788 2,250 26,620 6,171 12,920,000 368,030 Logarithmic Claims 0 6.670 7.719 7.804 8.728 16.370 1.683 ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE) ClaimLevBC10&lt;-subset(ClaimLev,Year==2010); cat(&quot;Sample size: &quot;, nrow(ClaimLevBC10), &quot;\\n&quot;) par(mfrow=c(1, 2)) hist(ClaimLevBC10$Claim, main=&quot;&quot;, xlab=&quot;Claims&quot;) hist(log(ClaimLevBC10$Claim), main=&quot;&quot;, xlab=&quot;Logarithmic Claims&quot;) Figure 15.1: Distribution of Claims ## Sample size: 1377 Using the 2010 claim experience (the sample), the Wisconsin Property Fund may be interested in assessing the severity of all claims that could potentially occur, such as 2010, 2011, and so forth (the population). This process is important in the contexts of ratemaking or claim predictive modeling. In order for such inference to be valid, we need to assume that the set of 2010 claims is a random sample that is representative of the population, the sampling distribution of the average claim amount can be estimated, so that we can quantify the bias and uncertainty in the esitmation due to use of a finite sample. 15.1.1 Random Sampling In statistics, a sampling error occurs when the sampling frame, the list from which the sample is drawn, is not an adequate approximation of the population of interest. A sample must be a representative subset of a population, or universe, of interest. If the sample is not representative, taking a larger sample does not eliminate bias, as the same mistake is repeated over again and again. Thus, we introduce the concept for random sampling that gives rise to a simple random sample that is representative of the population. We assume that the random variable \\(X\\) represents a draw from a population with a distribution function \\(F(\\cdot)\\) with mean \\(\\mathrm{E}[X]=\\mu\\) and variance \\(\\mathrm{Var}[X]=\\mathrm{E}[(X-\\mu)^2]\\), where \\(E(\\cdot)\\) denotes the expectation of a random variable. In random sampling, we make a total of \\(n\\) such draws represented by \\(X_1, \\ldots, X_n\\), each unrelated to one another (i.e., statistically independent). We refer to \\(X_1, \\ldots, X_n\\) as a random sample (with replacement) from \\(F(\\cdot)\\), taking either a parametric or nonparametric form. Alternatively, we may say that \\(X_1, \\ldots, X_n\\) are identically and independently distributed (iid) with distribution function \\(F(\\cdot)\\). 15.1.2 Sampling Distribution Using the random sample \\(X_1, \\ldots, X_n\\), we are interested in making a conclusion on a specific attribute of the population distribution \\(F(\\cdot)\\). For example, we may be interested in making an inference on the population mean, denoted \\(\\mu\\). It is natural to think of the sample mean, \\(\\bar{X}=\\sum_{i=1}^nX_i\\), as an estimate of the population mean \\(\\mu\\). We call the sample mean as a statistic calculated from the random sample \\(X_1, \\ldots, X_n\\). Other commonly used summary statistics include sample standard deviation and sample quantiles. When using a statistic (e.g., the sample mean \\(\\bar{X}\\)) to make statistical inference on the population attribute (e.g., population mean \\(\\mu\\)), the quality of inference is determined by the bias and uncertainty in the estimation, owing to the use of a sample in place of the population. Hence, it is important to study the distribution of a statistic that quantifies the bias and variability of the statistic. In particular, the distribution of the sample mean, \\(\\bar{X}\\) (or any other statistic), is called the sampling distribution. The sampling distribution depends on the sampling process, the statistic, the sample size \\(n\\) and the population distribution \\(F(\\cdot)\\). The central limit theorem gives the large-sample (sampling) distribution of the sample mean under certain conditions. 15.1.3 Central Limit Theorem In statistics, there are variations of the central limit theorem (CLT) ensuring that, under certain conditions, the sample mean will approach the population mean with its sampling distribution approaching the normal distribution as the sample size goes to infinity. We give the Lindeberg–Levy CLT that establishes the asymptotic sampling distribution of the sample mean \\(\\bar{X}\\) calculated using a random sample from a universe population having a distribution \\(F(\\cdot)\\). Lindeberg–Levy CLT. Let \\(X_1, \\ldots, X_n\\) be a random sample from a population distribution \\(F(\\cdot)\\) with mean \\(\\mu\\) and variance \\(\\sigma^2&lt;\\infty\\). The difference between the sample mean \\(\\bar{X}\\) and \\(\\mu\\), when multiplied by \\(\\sqrt{n}\\), converges in distribution to a normal distribution as the sample size goes to infinity. That is, \\[\\sqrt{n}(\\bar{X}-\\mu)\\xrightarrow[]{d}N(0,\\sigma).\\] Note that the CLT does not require a parametric form for \\(F(\\cdot)\\). Based on the CLT, we may perform statistical inference on the population mean (we infer, not deduce). The types of inference we may perform include estimation of the population, hypothesis testing on whether a null statement is true, and prediction of future samples from the population. 15.2 Point Estimation and Properties In this section, you learn how to estimate population parameters using method of moments estimation estimate population parameters based on maximum likelihood estimation The population distribution function \\(F(\\cdot)\\) can usually be characterized by a limited (finite) number of terms called parameters, in which case we refer to the distribution as a parametric distribution. In contrast, in nonparametric analysis, the attributes of the sampling distribution are not limited to a small number of parameters. For obtaining the population characteristics, there are different attributes related to the population distribution \\(F(\\cdot)\\). Such measures include the mean, median, percentiles (i.e., 95th percentile), and standard deviation. Because these summary measures do not depend on a specific parametric reference, they are nonparametric summary measures. In parametric analysis, on the other hand, we may assume specific families of distributions with specific parameters. For example, people usually think of logarithm of claim amounts to be normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). That is, we assume that the claims have a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Alternatively, insurance companies commonly assume that claim severity follows a gamma distribution with a shape parameter \\(\\alpha\\) and a scale parameter \\(\\theta\\). Here, the normal, lognormal, and gamma distributions are examples of parametric distributions. In the above examples, the quantities of \\(\\mu\\), \\(\\sigma\\), \\(\\alpha\\), and \\(\\theta\\) are known as parameters. For a given parametric distribution family, the distribution is uniquely determined by the values of the parameters. One often uses \\(\\theta\\) to denote a summary attribute of the population. In parametric models, \\(\\theta\\) can be a parameter or a function of parameters from a distribution such as the normal mean and variance parameters. In nonparametric analysis, it can take a form of a nonparametric summary such as the population mean or standard deviation. Let \\(\\hat{\\theta} =\\hat{\\theta}(X_1, \\ldots, X_n)\\) be a function of the sample that provides a proxy, or an estimate, of \\(\\theta\\). It is referred to as a statistic, a function of the sample \\(X_1, \\ldots, X_n\\). Show Wisconsin Property Fund Example - Continued Example – Wisconsin Property Fund. The sample mean 7.804 and the sample standard deviation 1.683 can be either deemed as nonparametric estimates of the population mean and standard deviation, or as parametric estimates of \\(\\mu\\) and \\(\\sigma\\) of the normal distribution concerning the logarithmic claims. Using results from the lognormal distribution, we may estimate the expected claim, the lognormal mean, as 10,106.8 ( \\(=\\exp(7.804+1.683^2/2)\\) ). For the Wisconsin Property Fund data, we may denote \\(\\hat{\\mu} =7.804\\) and \\(\\hat{\\sigma} = 1.683\\), with the hat notation denoting an estimate of the parameter based on the sample. In particular, such an estimate is referred to as a point estimate, a single approximation of the corresponding parameter. For point estimation, we introduce the two commonly used methods called the method of moments estimation and maximum likelihood estimation. 15.2.1 Method of Moments Estimation Before defining the method of moments estimation, we define the the concept of moments. Moments are population attributes that characterize the distribution function \\(F(\\cdot)\\). Given a random draw \\(X\\) from \\(F(\\cdot)\\), the expectation \\(\\mu_k=\\mathrm{E}[X^k]\\) is called the \\(k\\)th moment of \\(X\\), \\(k=1,2,3,\\cdots\\). For example, the population mean \\(\\mu\\) is the first moment. Furthermore, the expectation \\(\\mathrm{E}[(X-\\mu)^k]\\) is called a \\(k\\)th central moment. Thus, the variance is the second central moment. Using the random sample \\(X_1, \\ldots, X_n\\), we may construct the corresponding sample moment, \\(\\hat{\\mu}_k=(1/n)\\sum_{i=1}^nX_i^k\\), for estimating the population attribute \\(\\mu_k\\). For example, we have used the sample mean \\(\\bar{X}\\) as an estimator for the population mean \\(\\mu\\). Similarly, the second central moment can be estimated as \\((1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2\\). Without assuming a parametric form for \\(F(\\cdot)\\), the sample moments constitute nonparametric estimates of the corresponding population attributes. Such an estimator based on matching of the corresponding sample and population moments is called a method of moments estimator (MME). While the MME works naturally in a nonparametric model, it can be used to estimate parameters when a specific parametric family of distribution is assumed for \\(F(\\cdot)\\). Denote by \\(\\boldsymbol{\\theta}=(\\theta_1,\\cdots,\\theta_m)\\) the vector of parameters corresponding to a parametric distribution \\(F(\\cdot)\\). Given a distribution family, we commonly know the relationships between the parameters and the moments. In particular, we know the specific forms of the functions \\(h_1(\\cdot),h_2(\\cdot),\\cdots,h_m(\\cdot)\\) such that \\(\\mu_1=h_1(\\boldsymbol{\\theta}),\\,\\mu_2=h_2(\\boldsymbol{\\theta}),\\,\\cdots,\\,\\mu_m=h_m(\\boldsymbol{\\theta})\\). Given the MME \\(\\hat{\\mu}_1, \\ldots, \\hat{\\mu}_m\\) from the random sample, the MME of the parameters \\(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m\\) can be obtained by solving the equations of \\[\\hat{\\mu}_1=h_1(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m);\\] \\[\\hat{\\mu}_2=h_2(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m);\\] \\[\\cdots\\] \\[\\hat{\\mu}_m=h_m(\\hat{\\theta}_1,\\cdots,\\hat{\\theta}_m).\\] Show Wisconsin Property Fund Example - Continued Example – Wisconsin Property Fund. Assume that the claims follow a lognormal distribution, so that logarithmic claims follow a normal distribution. Specifically, assume \\(\\ln(X)\\) has a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), denoted as \\(\\ln(X) \\sim N(\\mu, \\sigma^2)\\). It is straightforward that the MME \\(\\hat{\\mu}=\\bar{X}\\) and \\(\\hat{\\sigma}=\\sqrt{(1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2}\\). For the Wisconsin Property Fund example, the method of moments estimates are \\(\\hat{\\mu} =7.804\\) and \\(\\hat{\\sigma} = 1.683\\). 15.2.2 Maximum Likelihood Estimation When \\(F(\\cdot)\\) takes a parametric form, the maximum likelihood method is widely used for estimating the population parameters \\(\\boldsymbol{\\theta}\\). Maximum likelihood estimation is based on the likelihood function, a function of the parameters given the observed sample. Denote by \\(f(x_i|\\boldsymbol{\\theta})\\) the probability function of \\(X_i\\) evaluated at \\(X_i=x_i\\) \\((i=1,2,\\cdots,n)\\), the probability mass function in the case of a discrete \\(X\\) and the probability density function in the case of a continuous \\(X\\). Then the likelihood function of \\(\\boldsymbol{\\theta}\\) associated with the observation \\((X_1,X_2,\\cdots,X_n)=(x_1,x_2,\\cdots,x_n)=\\mathbf{x}\\) can be written as \\[L(\\boldsymbol{\\theta}|\\mathbf{x})=\\prod_{i=1}^nf(x_i|\\boldsymbol{\\theta}),\\] with the corresponding log-likelihood function given by \\[l(\\boldsymbol{\\theta}|\\mathbf{x})=\\ln(L(\\boldsymbol{\\theta}|\\mathbf{x}))=\\sum_{i=1}^n\\ln f(x_i|\\boldsymbol{\\theta}).\\] The maximum likelihood estimator (MLE) of \\(\\boldsymbol{\\theta}\\) is the set of values of \\(\\boldsymbol{\\theta}\\) that maximize the likelihood function (log-likelihood function), given the observed sample. That is, the MLE \\(\\hat{\\boldsymbol{\\theta}}\\) can be written as \\[\\hat{\\boldsymbol{\\theta}}={\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x}),\\] where \\(\\Theta\\) is the parameter space of \\(\\boldsymbol{\\theta}\\), and \\({\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x})\\) is defined as the value of \\(\\boldsymbol{\\theta}\\) at which the function \\(l(\\boldsymbol{\\theta}|\\mathbf{x})\\) reachs its maximum. Given the analytical form of the likelihood function, the MLE can be obtained by taking the first derivative of the log-likelihood function with respect to \\(\\boldsymbol{\\theta}\\), and setting the values of the partial derivatives to zero. That is, the MLE are the solutions of the equations of \\[\\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_1}=0;\\] \\[\\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_2}=0;\\] \\[\\cdots\\] \\[\\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\theta}_m}=0,\\] provided that the second partial derivatives are negative. For parametric models, the MLE of the parameters can be obtained either analytically (e.g., in the case of normal distributions and linear estimators), or numerically through iterative algorithms such as the Newton-Raphson method and its adaptive versions (e.g., in the case of generalized linear models with a non-normal response variable). Normal distribution. Assume \\((X_1,X_2,\\cdots,X_n)\\) to be a random sample from the normal distribution \\(N(\\mu, \\sigma^2)\\). With an observed sample \\((X_1,X_2,\\cdots,X_n)=(x_1,x_2,\\cdots,x_n)\\), we can write the likelihood function of \\(\\mu,\\sigma^2\\) as \\[L(\\mu,\\sigma^2)=\\prod_{i=1}^n\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\left(x_i-\\mu\\right)^2}{2\\sigma^2}}\\right],\\] with the corresponding log-likelihood function given by \\[l(\\mu,\\sigma^2)=-\\frac{n}{2}[\\ln(2\\pi)+\\ln(\\sigma^2)]-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(x_i-\\mu\\right)^2.\\] By solving \\[\\frac{\\partial l(\\hat{\\mu},\\sigma^2)}{\\partial \\hat{\\mu}}=0,\\] we obtain \\(\\hat{\\mu}=\\bar{x}=(1/n)\\sum_{i=1}^nx_i\\). It is straightforward to verify that \\(\\frac{\\partial l^2(\\hat{\\mu},\\sigma^2)}{\\partial \\hat{\\mu}^2}\\left|_{\\hat{\\mu}=\\bar{x}}\\right.&lt;0\\). Since this works for arbitrary \\(x\\), \\(\\hat{\\mu}=\\bar{X}\\) is the MLE of \\(\\mu\\). Similarly, by solving \\[\\frac{\\partial l(\\mu,\\hat{\\sigma}^2)}{\\partial \\hat{\\sigma}^2}=0,\\] we obtain \\(\\hat{\\sigma}^2=(1/n)\\sum_{i=1}^n(x_i-\\mu)^2\\). Further replacing \\(\\mu\\) by \\(\\hat{\\mu}\\), we derive the MLE of \\(\\sigma^2\\) as \\(\\hat{\\sigma}^2=(1/n)\\sum_{i=1}^n(X_i-\\bar{X})^2\\). Hence, the sample mean \\(\\bar{X}\\) and \\(\\hat{\\sigma}^2\\) are both the MME and MLE for the mean \\(\\mu\\) and variance \\(\\sigma^2\\), under a normal population distribution \\(F(\\cdot)\\). More details regarding the properties of the likelihood function, and the derivation of MLE under parametric distributions other than the normal distribution are given in Appendix Chapter 16. 15.3 Interval Estimation In this section, you learn how to derive the exact sampling distribution of the MLE of the normal mean obtain the large-sample approximation of the sampling distribution using the large sample properties of the MLE construct a confidence interval of a parameter based on the large sample properties of the MLE Now that we have introduced the MME and MLE, we may perform the first type of statistical inference, interval estimation that quantifies the uncertainty resulting from the use of a finite sample. By deriving the sampling distribution of MLE, we can estimate an interval (a confidence interval) for the parameter. Under the frequentist approach (e.g., that based on maximum likelihood estimation), the confidence intervals generated from the same random sampling frame will cover the true value the majority of times (e.g., 95% of the times), if we repeat the sampling process and re-calculate the interval over and over again. Such a process requires the derivation of the sampling distribution for the MLE. 15.3.1 Exact Distribution for Normal Sample Mean Due to the additivity property of the normal distribution (i.e., a sum of normal random variables that follows a multivariate normal distribution still follows a normal distribution) and that the normal distribution belongs to the location–scale family (i.e., a location and/or scale transformation of a normal random variable has a normal distribution), the sample mean \\(\\bar{X}\\) of a random sample from a normal \\(F(\\cdot)\\) has a normal sampling distribution for any finite \\(n\\). Given \\(X_i\\sim^{iid} N(\\mu,\\sigma^2)\\), \\(i=1,\\dots,n\\), the MLE of \\(\\mu\\) has an exact distribution \\[\\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right).\\] Hence, the sample mean is an unbiased estimator of \\(\\mu\\). In addition, the uncertainty in the estimation can be quantified by its variance \\(\\sigma^2/n\\), that decreases with the sample size \\(n\\). When the sample size goes to infinity, the sample mean will approach a single mass at the true value. 15.3.2 Large-sample Properties of MLE For the MLE of the mean parameter and any other parameters of other parametric distribution families, however, we usually cannot derive an exact sampling distribution for finite samples. Fortunately, when the sample size is sufficiently large, MLEs can be approximated by a normal distribution. Due to the general maximum likelihood theory, the MLE has some nice large-sample properties. The MLE \\(\\hat{\\theta}\\) of a parameter \\(\\theta\\), is a consistent estimator. That is, \\(\\hat{\\theta}\\) converges in probability to the true value \\(\\theta\\), as the sample size \\(n\\) goes to infinity. The MLE has the asymptotic normality property, meaning that the estimator will converge in distribution to a normal distribution centered around the true value, when the sample size goes to infinity. Namely, \\[\\sqrt{n}(\\hat{\\theta}-\\theta)\\rightarrow_d N\\left(0,\\,V\\right),\\quad \\mbox{as}\\quad n\\rightarrow \\infty,\\] where \\(V\\) is the inverse of the Fisher Information. Hence, the MLE \\(\\hat{\\theta}\\) approximately follows a normal distribution with mean \\(\\theta\\) and variance \\(V/n\\), when the sample size is large. The MLE is efficient, meaning that it has the smallest asymptotic variance \\(V\\), commonly referred to as the Cramer–Rao lower bound. In particular, the Cramer–Rao lower bound is the inverse of the Fisher information defined as \\(\\mathcal{I}(\\theta)=-\\mathrm{E}(\\partial^2\\ln f(X;\\theta)/\\partial \\theta^2)\\). Hence, \\(\\mathrm{Var}(\\hat{\\theta})\\) can be estimated based on the observed Fisher information that can be written as \\(-\\sum_{i=1}^n \\partial^2\\ln f(X_i;\\theta)/\\partial \\theta^2\\). For many parametric distributions, the Fisher information may be derived analytically for the MLE of parameters. For more sophisticated parametric models, the Fisher information can be evaluated numerically using numerical integration for continuous distributions, or numerical summation for discrete distributions. 15.3.3 Confidence Interval Given that the MLE \\(\\hat{\\theta}\\) has either an exact or an approximate normal distribution with mean \\(\\theta\\) and variance \\(\\mathrm{Var}(\\hat{\\theta})\\), we may take the square root of the variance and plug-in the estimate to define \\(se(\\hat{\\theta}) = \\sqrt{\\mathrm{Var}(\\hat{\\theta})}\\). A standard error is an estimated standard deviation that quantifies the uncertainty in the estimation resulting from the use of a finite sample. Under some regularity conditions governing the population distribution, we may establish that the statistic \\[\\frac{\\hat{\\theta}-\\theta}{se(\\hat{\\theta})}\\] converges in distribution to a Student-\\(t\\) distribution with degrees of freedom (a parameter of the distribution) \\({n-p}\\), where \\(p\\) is the number of parameters in the model other than the variance. For example, for the normal distribution case, we have \\(p=1\\) for the parameter \\(\\mu\\); for a linear regression model with an independent variable, we have \\(p=2\\) for the parameters of the intercept and the independent variable. Denote by \\(t_{n-p}(1-\\alpha/2)\\) the \\(100\\times(1-\\alpha/2)\\)-th percentile of the Student-\\(t\\) distribution that satisfies \\(\\Pr\\left[t&lt; t_{n-p}\\left(1-{\\alpha}/{2}\\right) \\right]= 1-{\\alpha}/{2}\\). We have, \\[\\Pr\\left[-t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right)&lt;\\frac{\\hat{\\theta}-\\theta}{se(\\hat{\\theta})}&lt; t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right) \\right]= 1-{\\alpha},\\] from which we can derive a confidence interval for \\(\\theta\\). From the above equation we can derive a pair of statistics, \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\), that provide an interval of the form \\([\\hat{\\theta}_1, \\hat{\\theta}_2]\\). This interval is a \\(1-\\alpha\\) confidence interval for \\(\\theta\\) such that \\(\\Pr\\left(\\hat{\\theta}_1 \\le \\theta \\le \\hat{\\theta}_2\\right) = 1-\\alpha,\\) where the probability \\(1-\\alpha\\) is referred to as the confidence level. Note that the above confidence interval is not valid for small samples, except for the case of the normal mean. Normal distribution. For the normal population mean \\(\\mu\\), the MLE has an exact sampling distribution \\(\\bar{X}\\sim N(\\mu,\\sigma/\\sqrt{n})\\), in which we can estimate \\(se(\\hat{\\theta})\\) by \\(\\hat{\\sigma}/\\sqrt{n}\\). Based on the Cochran’s theorem, the resulting statistic has an exact Student-\\(t\\) distribution with degrees of freedom \\(n-1\\). Hence, we can derive the lower and upper bounds of the confidence interval as \\[\\hat{\\mu}_1 = \\hat{\\mu} - t_{n-1}\\left(1-\\frac{\\alpha}{2}\\right)\\frac{ \\hat{\\sigma}}{\\sqrt{n}}\\] and \\[\\hat{\\mu}_2 = \\hat{\\mu} + t_{n-1}\\left(1-\\frac{\\alpha}{2}\\right)\\frac{ \\hat{\\sigma}}{\\sqrt{n}}.\\] When \\(\\alpha = 0.05\\), \\(t_{n-1}(1-\\alpha/2) \\approx 1.96\\) for large values of \\(n\\). Based on the Cochran’s theorem, the confidence interval is valid regardless of the sample size. Show Wisconsin Property Fund Example - Continued Example – Wisconsin Property Fund. For the lognormal claim model, (7.715235, 7.893208) is a 95% confidence interval for \\(\\mu\\). More details regarding interval estimation based the MLE of other parameters and distribution families are given in Appendix Chapter 17. 15.4 Hypothesis Testing In this section, you learn how to understand the basic concepts in hypothesis testing including the level of significance and the power of a test perform hypothesis testing such as a Student-\\(t\\) test based on the properties of the MLE construct a likelihood ratio test for a single parameter or multiple parameters from the same statistical model use information criteria such as the Akaike’s information criterion or the Bayesian information criterion to perform model selection For the parameter(s) \\(\\boldsymbol{\\theta}\\) from a parametric distribution, an alternative type of statistical inference is called hypothesis tesing that verifies whether a hypothesis regarding the parameter(s) is true, under a given probability called the level of significance \\(\\alpha\\) (e.g., 5%). In hypothesis testing, we reject the null hypothesis, a restrictive statement concerning the parameter(s), if the probability of observing a random sample as extremal as the observed one is smaller than \\(\\alpha\\), if the null hypothesis were true. 15.4.1 Basic Concepts In a statistical test, we are usually interested in testing whether a statement regarding some parameter(s), a null hypothesis (denoted \\(H_0\\)), is true given the observed data. The null hypothesis can take a general form \\(H_0:\\theta\\in\\Theta_0\\), where \\(\\Theta_0\\) is a subset of the parameter space \\(\\Theta\\) of \\(\\theta\\) that may contain multiple parameters. For the case with a single parameter \\(\\theta\\), the null hypothesis usually takes either the form \\(H_0:\\theta=\\theta_0\\) or \\(H_0:\\theta\\leq\\theta_0\\). The opposite of the null hypothesis is called the alternative hypothesis that can be written as \\(H_a:\\theta\\neq\\theta_0\\) or \\(H_a:\\theta&gt;\\theta_0\\). The statistical test on \\(H_0:\\theta=\\theta_0\\) is called a two-sided as the alternative hypothesis contains two ineqalities of \\(H_a:\\theta&lt;\\theta_0\\) or \\(\\theta&gt;\\theta_0\\). In contrast, the statistical test on either \\(H_0:\\theta\\leq\\theta_0\\) or \\(H_0:\\theta\\geq\\theta_0\\) is called a one-sided test. A statistical test is usually constructed based on a statistic \\(T\\) and its exact or large-sample distribution. The test typically rejects a two-sided test when either \\(T &gt; c_1\\) or \\(T &lt; c_2\\), where the two constants \\(c_1\\) and \\(c_2\\) are obtained based on the sampling distribution of \\(T\\) at a probability level \\(\\alpha\\) called the level of significance. In particular, the level of significance \\(\\alpha\\) satisfies \\[\\alpha=\\Pr(\\mbox{reject }H_0|H_0\\mbox{ is true}),\\] meaning that if the null hypothesis were true, we would reject the null hypothesis only 5% of the times, if we repeat the sampling process and perform the test over and over again. Thus, the level of significance is the probability of making a type I error (error of the first kind), the error of incorrectly rejecting a true null hypothesis. For this reason, the level of significance \\(\\alpha\\) is also referred to as the type I error rate. Another type of error we may make in hypothesis testing is the type II error (error of the second kind), the error of incorrectly accepting a false null hypothesis. Similarly, we can define the type II error rate as the probability of not rejecting (accepting) a null hypothesis given that it is not true. That is, the type II error rate is given by \\[\\Pr(\\mbox{accept }H_0|H_0\\mbox{ is false}).\\] Another important quantity concerning the quality of the statistical test is called the power of the test \\(\\beta\\), defined as the probability of rejecting a false null hypothesis. The mathematical definition of the power is \\[\\beta=\\Pr(\\mbox{reject }H_0|H_0\\mbox{ is false}).\\] Note that the power of the test is typically calculated based on a specific alternative value of \\(\\theta=\\theta_a\\), given a specific sampling distribution and a given sample size. In real experimental studies, people usually calculate the required sample size in order to choose a sample size that will ensure a large chance of obtaining a statistically significant test (i.e., with a prespecified statistical power such as 85%). 15.4.2 Student-\\(t\\) test based on MLE Based on the results from Section 15.3.1, we can define a Student \\(t\\) test for testing \\(H_0:\\theta=\\theta_0\\). In particular, we define the test statistic as \\[t\\text{-stat}=\\frac{\\hat{\\theta}-\\theta_0}{se(\\hat{\\theta})},\\] which has a large-sample distribution of a Student-\\(t\\) distribution with degrees of freedom \\({n-p}\\), when the null hypothesis is true (i.e., when \\(\\theta=\\theta_0\\)). For a given level of significance \\(\\alpha\\), say 5%, we reject the null hypothesis if the event \\(t\\text{-stat}&lt;-t_{n-p}\\left(1-{\\alpha}/{2}\\right)\\) or \\(t\\text{-stat}&gt; t_{n-p}\\left(1-{\\alpha}/{2}\\right)\\) occurs (the rejection region). Under the null hypothesis \\(H_0\\), we have \\[\\Pr\\left[t\\text{-stat}&lt;-t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right)\\right]=\\Pr\\left[t\\text{-stat}&gt; t_{n-p}\\left(1-\\frac{\\alpha}{2}\\right) \\right]= \\frac{\\alpha}{2}.\\] In addition to the concept of rejection region, we may reject the test based on the \\(p\\)-value defined as \\(2\\Pr(T&gt;|t\\text{-stat}|)\\) for the aforementioned two-sided test, where the random variable \\(T\\sim T_{n-p}\\). We reject the null hypothesis if \\(p\\)-value is smaller than and equal to \\(\\alpha\\). For a given sample, a \\(p\\)-value is defined to be the smallest significance level for which the null hypothesis would be rejected. Similarly, we can construct a one-sided test for the null hypothesis \\(H_0:\\theta\\leq\\theta_0\\) (or \\(H_0:\\theta\\geq\\theta_0\\)). Using the same test statistic, we reject the null hypothesis when \\(t\\text{-stat}&gt; t_{n-p}\\left(1-{\\alpha}\\right)\\) (or \\(t\\text{-stat}&lt;- t_{n-p}\\left(1-{\\alpha}\\right)\\) for the test on \\(H_0:\\theta\\geq\\theta_0\\)). The corresponding \\(p\\)-value is defined as \\(\\Pr(T&gt;|t\\text{-stat}|)\\) (or \\(\\Pr(T&lt;|t\\text{-stat}|)\\) for the test on \\(H_0:\\theta\\geq\\theta_0\\)). Note that the test is not valid for small samples, except for the case of the test on the normal mean. One-sample \\(t\\) Test for Normal Mean. For the test on the normal mean of the form \\(H_0:\\mu=\\mu_0\\), \\(H_0:\\mu\\leq\\mu_0\\) or \\(H_0:\\mu\\geq\\mu_0\\), we can define the test statistic as \\[t\\text{-stat}=\\frac{\\bar{X}-\\mu_0}{{\\hat{\\sigma}}/{\\sqrt{n}}},\\] for which we have an exact sampling distribution \\(t\\text{-stat}\\sim T_{n-1}\\) from the Cochran’s theorem, with \\(T_{n-1}\\) denoting a Student-\\(t\\) distribution with degrees of freedom \\(n-1\\). According to the Cochran’s theorem, the test is valid for both small and large samples. Show Wisconsin Property Fund Example - Continued Example – Wisconsin Property Fund. Assume that mean logarithmic claims have historically been approximately by \\(\\mu_0 = \\ln(5000)= 8.517\\). We might want to use the 2010 data to assess whether the mean of the distribution has changed (a two-sided test), or whether it has increased (a one-sided test). Given the actual 2010 average \\(\\hat{\\mu} =7.804\\), we may use the one-sample \\(t\\) test to assess whether this is a significant departure from \\(\\mu_0 = 8.517\\) (i.e., in testing \\(H_0:\\mu=8.517\\)). The test statistic \\(t\\text{-stat}=(8.517-7.804)/(1.683/\\sqrt{1377}) = 15.72&gt;t_{1376}\\left(0.975\\right)\\). Hence, we reject the two-sided test at \\(\\alpha=5\\%\\). Similarly, we will reject the one-sided test at \\(\\alpha=5\\%\\). Show Wisconsin Property Fund Example - Continued Example – Wisconsin Property Fund. For numerical stability and extensions to regression applications, statistical packages often work with transformed versions of parameters. The following estimates are from the R package VGAM (the function). More details on the MLE of other distribution families are given in Appendix Chapter 17. Distribution Parameter Standard \\(t\\)-stat Estimate Error Gamma 10.190 0.050 203.831 -1.236 0.030 -41.180 Lognormal 7.804 0.045 172.089 0.520 0.019 27.303 Pareto 7.733 0.093 82.853 -0.001 0.054 -0.016 GB2 2.831 1.000 2.832 1.203 0.292 4.120 6.329 0.390 16.220 1.295 0.219 5.910 15.4.3 Likelihood Ratio Test In the previous subsection, we have introduced the Student-\\(t\\) test on a single parameter, based on the properties of the MLE. In this section, we define an alternative test called the likelihood ratio test (LRT). The LRT may be used to test multiple parameters from the same statistical model. Given the likelihood function \\(L(\\theta|\\mathbf{x})\\) and \\(\\Theta_0 \\subset \\Theta\\), the likelihood ratio test statistic for testing \\(H_0:\\theta\\in\\Theta_0\\) against \\(H_a:\\theta\\notin\\Theta_0\\) is given by \\[L=\\frac{\\sup_{\\theta\\in\\Theta_0}L(\\theta|\\mathbf{x})}{\\sup_{\\theta\\in\\Theta}L(\\theta|\\mathbf{x})},\\] and that for testing \\(H_0:\\theta=\\theta_0\\) versis \\(H_a:\\theta\\neq\\theta_0\\) is \\[L=\\frac{L(\\theta_0|\\mathbf{x})}{\\sup_{\\theta\\in\\Theta}L(\\theta|\\mathbf{x})}.\\] The LRT rejects the null hypothesis when \\(L &lt; c\\), with the threshold depending on the level of significance \\(\\alpha\\), the sample size \\(n\\), and the number of parameters in \\(\\theta\\). Based on the Neyman–Pearson Lemma, the LRT is the uniformly most powerful (UMP) test for testing \\(H_0:\\theta=\\theta_0\\) versis \\(H_a:\\theta=\\theta_a\\). That is, it provides the largest power \\(\\beta\\) for a given \\(\\alpha\\) and a given alternative value \\(\\theta_a\\). Based on the Wilks’s Theorem, the likelihood ratio test statistic \\(-2\\ln(L)\\) converges in distribution to a Chi-square distribution with the degree of freedom being the difference between the dimensionality of the parameter spaces \\(\\Theta\\) and \\(\\Theta_0\\), when the sample size goes to infinity and when the null model is nested within the alternative model. That is, when the null model is a special case of the alternative model containing a restricted sample space, we may approximate \\(c\\) by \\(\\chi^2_{p_1 - p_2}(1-\\alpha)\\), the \\(100\\times(1-\\alpha)\\) th percentile of the Chi-square distribution, with \\(p_1-p_2\\) being the degrees of freedom, and \\(p_1\\) and \\(p_2\\) being the numbers of parameters in the alternative and null models, respectively. Note that the LRT is also a large-sample test that will not be valid for small samples. 15.4.4 Information Criteria In real-life applications, the LRT has been commonly used for comparing two nested models. The LRT approach as a model selection tool, however, has two major drawbacks: 1) It typically requires the null model to be nested within the alternative model; 2) models selected from the LRT tends to provide in-sample over-fitting, leading to poor out-of-sample prediction. In order to overcome these issues, model selection based on information criteria, applicable to non-nested models while taking into consideration the model complexity, is more widely used for model selection. Here, we introduce the two most widely used criteria, the Akaike’s information criterion and the Bayesian information criterion. In particular, the Akaike’s information criterion (\\(AIC\\)) is defined as \\[AIC = -2\\ln L(\\hat{\\boldsymbol \\theta}) + 2p,\\] where \\(\\hat{\\boldsymbol \\theta}\\) denotes the MLE of \\({\\boldsymbol \\theta}\\), and \\(p\\) is the number of parameters in the model. The additional term \\(2 p\\) represents a penalty for the complexity of the model. That is, with the same maximized likelihood function, the \\(AIC\\) favors model with less parameters. We note that the \\(AIC\\) does not consider the impact from the sample size \\(n\\). Alternatively, people use the Bayesian information criterion (\\(BIC\\)) that takes into consideration the sample size. The \\(BIC\\) is defined as \\[BIC = -2\\ln L(\\hat{\\boldsymbol \\theta}) + p\\,\\ln(n).\\] We observe that the \\(BIC\\) generally puts a higher weight on the number of parameters. With the same maximized likelihood function, the \\(BIC\\) will suggest a more parsimonious model than the \\(AIC\\). Show Wisconsin Property Fund Example - Continued Example – Wisconsin Property Fund. Both the \\(AIC\\) and \\(BIC\\) statistics suggest that the GB2 is the best fitting model whereas gamma is the worst. Distribution AIC BIC Gamma 28,305.2 28,315.6 Lognormal 26,837.7 26,848.2 Pareto 26,813.3 26,823.7 GB2 26,768.1 26,789.0 In this graph, black represents actual (smoothed) logarithmic claims Best approximated by green which is fitted GB2 Pareto (purple) and Lognormal (lightblue) are also pretty good Worst are the exponential (in red) and gamma (in dark blue) Figure 15.2: Fitted Claims Distribution ## Sample size: 6258 Show R Code R Code for Fitted Claims Distributions # R Code to fit several claims distributions ClaimLev &lt;- read.csv(&quot;Data/CLAIMLEVEL.csv&quot;, header=TRUE); nrow(ClaimLev) ClaimData&lt;-subset(ClaimLev,Year==2010); #Use &quot;VGAM&quot; library for estimation of parameters library(VGAM) fit.LN &lt;- vglm(Claim ~ 1, family=lognormal, data = ClaimData) fit.gamma &lt;- vglm(Claim ~ 1, family=gamma2, data = ClaimData) theta.gamma&lt;-exp(coef(fit.gamma)[1])/exp(coef(fit.gamma)[2]) alpha.gamma&lt;-exp(coef(fit.gamma)[2]) fit.exp &lt;- vglm(Claim ~ 1, exponential, data = ClaimData) fit.pareto &lt;- vglm(Claim ~ 1, paretoII, loc=0, data = ClaimData) ################################################### # Inference assuming a GB2 Distribution - this is more complicated # The likelihood functon of GB2 distribution (negative for optimization) likgb2 &lt;- function(param) { a1 &lt;- param[1] a2 &lt;- param[2] mu &lt;- param[3] sigma &lt;- param[4] yt &lt;- (log(ClaimData$Claim)-mu)/sigma logexpyt&lt;-ifelse(yt&gt;23,yt,log(1+exp(yt))) logdens &lt;- a1*yt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpyt -log(ClaimData$Claim) return(-sum(logdens)) } # &quot;optim&quot; is a general purpose minimization function gb2bop &lt;- optim(c(1,1,0,1),likgb2,method=c(&quot;L-BFGS-B&quot;), lower=c(0.01,0.01,-500,0.01),upper=c(500,500,500,500),hessian=TRUE) ################################################### # Plotting the fit using densities (on a logarithmic scale) plot(density(log(ClaimData$Claim)), ylim=c(0,0.36),main=&quot;&quot;, xlab=&quot;Log Expenditures&quot;) x &lt;- seq(0,15,by=0.01) fexp_ex = dgamma(exp(x), scale = exp(-coef(fit.exp)), shape = 1)*exp(x) lines(x,fexp_ex, col=&quot;red&quot;) fgamma_ex = dgamma(exp(x), shape = alpha.gamma, scale=theta.gamma)*exp(x) lines(x,fgamma_ex,col=&quot;blue&quot;) fpareto_ex = dparetoII(exp(x),loc=0,shape = exp(coef(fit.pareto)[2]), scale = exp(coef(fit.pareto)[1]))*exp(x) lines(x,fpareto_ex,col=&quot;purple&quot;) flnorm_ex = dlnorm(exp(x), mean = coef(fit.LN)[1], sd = exp(coef(fit.LN)[2]))*exp(x) lines(x,flnorm_ex, col=&quot;lightblue&quot;) # density for GB II gb2density &lt;- function(x){ a1 &lt;- gb2bop$par[1] a2 &lt;- gb2bop$par[2] mu &lt;- gb2bop$par[3] sigma &lt;- gb2bop$par[4] xt &lt;- (log(x)-mu)/sigma logexpxt&lt;-ifelse(xt&gt;23,yt,log(1+exp(xt))) logdens &lt;- a1*xt - log(sigma) - log(beta(a1,a2)) - (a1+a2)*logexpxt -log(x) exp(logdens) } fGB2_ex = gb2density(exp(x))*exp(x) lines(x,fGB2_ex, col=&quot;green&quot;) "],
["C-AppB.html", "Chapter 16 Appendix B: Iterated Expectations 16.1 Conditional Distribution and Conditional Expectation 16.2 Iterated Expectations and Total Variance", " Chapter 16 Appendix B: Iterated Expectations This appendix introduces the laws related to iterated expectations. In particular, Section 16.1 introduces the concepts of conditional distribution and conditional expectation. Section 16.2 introduces the Law of Iterated Expectations and the Law of Total Variance. In some situations, we only observe a single outcome but can conceptualize an outcome as resulting from a two (or more) stage process. Such types of statistical models are called two-stage, or hierarchical models. Some special cases of hierarchical models include: models where the parameters of the distribution are random variables; mixture distribution, where Stage 1 represents the draw of a sub-population and Stage 2 represents a random variable from a distribution that is determined by the sub-population drew in Stage 1; an aggregate distribution, where Stage 1 represents the draw of the number of events and Stage two represents the loss amount occurred per event. In these situations, the process gives rise to a conditional distribution of a random variable (the Stage 2 outcome) given the other (the Stage 1 outcome). The Law of Iterated Expectations can be useful for obtaining the unconditional expectation or variance of a random variable in such cases. 16.1 Conditional Distribution and Conditional Expectation In this section, you learn the concepts related to the conditional distribution of a random variable given another how to define the conditional expectation and variance based on the conditional distribution function The iterated expectations are the laws regarding calculation of the expectation and variance of a random variable using a conditional distribution of the variable given another variable. Hence, we first introduce the concepts related to the conditional distribution, and the calculation of the conditional expectation and variable based on a given conditional distribution. 16.1.1 Conditional Distribution Here we introduce the concept of conditional distribution respectively for discrete and continuous random variables. 16.1.1.1 Discrete Case Suppose that \\(X\\) and \\(Y\\) are both discrete random variables, meaning that they can take a finite or countable number of possible values with a positive probability. The joint probability (mass) function of (\\(X\\), \\(Y\\)) is defined as \\[p(x,y) = \\Pr[X=x, Y=y]\\]. When \\(X\\) and \\(Y\\) are independent (the value of \\(X\\) does not depend on that of \\(Y\\)), we have \\[p(x,y)=p(x)p(y),\\] with \\(p(x)=\\Pr[X=x]\\) and \\(p(y)=\\Pr[Y=y]\\) being the marginal probability function of \\(X\\) and \\(Y\\), respectively. Given the joint probability function, we may obtain the marginal probability functions of \\(Y\\) as \\[p(y)=\\sum_x p(x,y),\\] where the summation is over all possible values of \\(x\\), and the marginal probability function of \\(X\\) can be obtained in a similar manner. The conditional probability (mass) function of \\((Y|X)\\) is defined as \\[p(y|x) =\\Pr[Y=y|X=x]= \\frac{p(x,y)}{\\Pr[X=x]},\\] where we may obtain the conditional probability function of \\((X|Y)\\) in a similar manner. In particular, the above conditional probability represents the probability of the event \\(Y=y\\) given the event \\(X=x\\). Hence, even in cases where \\(\\Pr[X=x]=0\\), the function may be given as a particular form, in real applications. 16.1.1.2 Continuous Case For continuous random variables \\(X\\) and \\(Y\\), we may define their joint probability (density) function based on the joint cumulative distribution function. The joint cumulative distribution function of (\\(X\\), \\(Y\\)) is defined as \\[F(x,y) = \\Pr[X\\leq x, Y\\leq y].\\] When \\(X\\) and \\(Y\\) are independent, we have \\[F(x,y)=F(x)F(y),\\] with \\(F(x)=\\Pr[X\\leq x]\\) and \\(F(y)=\\Pr[Y\\leq y]\\) being the cumulative distribution function (cdf) of \\(X\\) and \\(Y\\), respectively. The random variable \\(X\\) is referred to as a continuous random variable if its cdf is continuous on \\(x\\). When the cdf \\(F(x)\\) is continuous on \\(x\\), then we define \\(f(x)=\\partial F(x)/\\partial x\\) as the (marginal) probability density function (pdf) of \\(X\\). Similarly, if the joint cdf \\(F(x,y)\\) is continuous on both \\(x\\) and \\(y\\), we define \\[f(x,y)=\\frac{\\partial^2 F(x,y)}{\\partial x\\partial y}\\] as the joint probability density function of (\\(X\\), \\(Y\\)), in which case we refer to the random variables as jointly continuous. When \\(X\\) and \\(Y\\) are independent, we have \\[f(x,y)=f(x)f(y).\\] Given the joint density function, we may obtain the marginal density function of \\(Y\\) as \\[f(y)=\\int_x f(x,y)\\,dx,\\] where the integral is over all possible values of \\(x\\), and the marginal probability function of \\(X\\) can be obtained in a similar manner. Based on the joint pdf and the marginal pdf, we define the conditional probability density function of \\((Y|X)\\) as \\[f(y|x) = \\frac{f(x,y)}{f(x)},\\] where we may obtain the conditional probability function of \\((X|Y)\\) in a similar manner. Here, the conditional density function is the density function of \\(y\\) given \\(X=x\\). Hence, even in cases where \\(\\Pr[X=x]=0\\) or when \\(f(x)\\) is not defined, the function may be given in a particular form in real applications. 16.1.2 Conditional Expectation and Conditional Variance Now we define the conditional expectation and variance based on the conditional distribution defined in the previous subsection. 16.1.2.1 Discrete Case For a discrete random variable \\(Y\\), its expectation is defined as \\(\\mathrm{E}[Y]=\\sum_y y\\,p(y)\\) if its value is finite, and its variance is defined as \\(\\mathrm{Var}[Y]=\\mathrm{E}\\{(Y-\\mathrm{E}[Y])^2\\}=\\sum_y y^2\\,p(y)-\\{\\mathrm{E}[Y]\\}^2\\) if its value is finite. For a discrete random variable \\(Y\\), the conditional expectation of the random variable \\(Y\\) given the event \\(X=x\\) is defined as \\[\\mathrm{E}[Y|X=x]=\\sum_y y\\,p(y|x),\\] where \\(X\\) does not have to be a discrete variable, as far as the conditional probability function \\(p(y|x)\\) is given. Note that the conditional expectation \\(\\mathrm{E}[Y|X=x]\\) is a fixed number. When we replace \\(x\\) with \\(X\\) on the right hand side of the above equation, we can define the expectation of \\(Y\\) given the random variable \\(X\\) as \\[\\mathrm{E}[Y|X]=\\sum_y y\\,p(y|X),\\] which is still a random variable, and the randomness comes from \\(X\\). In a similar manner, we can define the conditional variance of the random variable \\(Y\\) given the event \\(X=x\\) as \\[\\mathrm{Var}[Y|X=x]=\\mathrm{E}[Y^2|X=x]-\\{\\mathrm{E}[Y|X=x]\\}^2=\\sum_y y^2\\,p(y|x)-\\{\\mathrm{E}[Y|X=x]\\}^2.\\] The variance of \\(Y\\) given \\(X\\), \\(\\mathrm{Var}[Y|X]\\) can be defined by replacing \\(x\\) by \\(X\\) in the above equation, and \\(\\mathrm{Var}[Y|X]\\) is still a random variable and the randomness comes from \\(X\\). 16.1.2.2 Continuous Case For a continuous random variable \\(Y\\), its expectation is defined as \\(\\mathrm{E}[Y]=\\int_y y\\,f(y)dy\\) if the integral exists, and its variance is defined as \\(\\mathrm{Var}[Y]=\\mathrm{E}\\{(X-\\mathrm{E}[Y])^2\\}=\\int_y y^2\\,f(y)dy-\\{\\mathrm{E}[Y]\\}^2\\) if its value is finite. For jointly continuous random variables \\(X\\) and \\(Y\\), the conditional expectation of the random variable \\(Y\\) given \\(X=x\\) is defined as \\[\\mathrm{E}[Y|X=x]=\\int_y y\\,f(y|x)dy.\\] where \\(X\\) does not have to be a continuous variable, as far as the conditional probability function \\(f(y|x)\\) is given. Similarly, the conditional expectation \\(\\mathrm{E}[Y|X=x]\\) is a fixed number. When we replace \\(x\\) with \\(X\\) on the right-hand side of the above equation, we can define the expectation of \\(Y\\) given the random variable \\(X\\) as \\[\\mathrm{E}[Y|X]=\\int_y y\\,p(y|X)\\,dy,\\] which is still a random variable, and the randomness comes from \\(X\\). In a similar manner, we can define the conditional variance of the random variable \\(Y\\) given the event \\(X=x\\) as \\[\\mathrm{Var}[Y|X=x]=\\mathrm{E}[Y^2|X=x]-\\{\\mathrm{E}[Y|X=x]\\}^2=\\int_y y^2\\,f(y|x)\\,dy-\\{\\mathrm{E}[Y|X=x]\\}^2.\\] The variance of \\(Y\\) given \\(X\\), \\(\\mathrm{Var}[Y|X]\\) can then be defined by replacing \\(x\\) by \\(X\\) in the above equation, and similarly \\(\\mathrm{Var}[Y|X]\\) is also a random variable and the randomness comes from \\(X\\). 16.2 Iterated Expectations and Total Variance In this section, you learn the Law of Iterated Expectations for calculating the expectation of a random variable based on its conditional distribution given another random variable the Law of Total Variance for calculating the variance of a random variable based on its conditional distribution given another random variable how to calculate the expectation and variance based on an example of a two-stage model 16.2.1 Law of Iterated Expectations Consider two random variables \\(X\\) and \\(Y\\), and \\(h(X,Y)\\), a random variable depending on the function \\(h\\), \\(X\\) and \\(Y\\). Assuming all the expectations exist and are finite, the Law of Iterated Expectations states that \\[\\mathrm{E}[h(X,Y)]= \\mathrm{E} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\},\\] where the first (inside) expectation is taken with respect to the random variable \\(Y\\) and the second (outside) expectation is taken with respect to \\(X\\). For the Law of Iterated Expectations, the random variables may be discrete, continuous, or a hybrid combination of the two. We use the example of discrete variables of \\(X\\) and \\(Y\\) to illustrate the calculation of the unconditional expectation using the Law of Iterated Expectations. For continuous random variables, we only need to replace the summation with the integral, as illustrated earlier in the appendix. Given \\(p(y|x)\\) the joint pmf of \\(X\\) and \\(Y\\), the conditional expectation of \\(h(X,Y)\\) given the event \\(X=x\\) is defined as \\[\\mathrm{E} \\left[ h(X,Y) | X=x \\right] = \\sum_y h(x,y) p(y|x),\\] and the conditional expectation of \\(h(X,Y)\\) given \\(X\\) being a random variable can be written as \\[\\mathrm{E} \\left[ h(X,Y) | X \\right] = \\sum_y h(X,y) p(y|X).\\] The unconditional expectation of \\(h(X,Y)\\) can then be obtained by taking the expectation of \\(\\mathrm{E} \\left[ h(X,Y) | X \\right]\\) with respect to the random variable \\(X\\). That is, we can obtain \\(\\mathrm{E}[ h(X,Y)]\\) as \\[\\begin{aligned} \\mathrm{E} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\} &amp;= \\sum_x \\left\\{\\sum_y h(x,y) p(y|x) \\right \\} p(x) \\\\ &amp;= \\sum_x \\sum_y h(x,y) p(y|x)p(x) \\\\ &amp;= \\sum_x \\sum_y h(x,y) p(x,y) = \\mathrm{E}[h(X,Y)] \\end{aligned}.\\] The Law of Iterated Expectations for the continuous and hybrid cases can be proved in a similar manner, by replacing the corresponding summation(s) by integral(s). 16.2.2 Law of Total Variance Assuming that all the variances exist and are finite, the Law of Total Variance states that \\[\\mathrm{Var}[h(X,Y)]= \\mathrm{E} \\left\\{ \\mathrm{Var} \\left[h(X,Y) | X \\right] \\right \\} +\\mathrm{Var} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\},\\] where the first (inside) expectation/variance is taken with respect to the random variable \\(Y\\) and the second (outside) expectation/variance is taken with respect to \\(X\\). Thus, the unconditional variance equals to the expectation of the conditional variance plus the variance of the conditional expectation. Show Technical Detail In order to verify this rule, first note that we can calculate a conditional variance as \\[\\mathrm{Var} \\left[ h(X,Y) | X \\right] = \\mathrm{E} [ h(X,Y)^2 | X ] -\\left\\{\\mathrm{E} \\left[ h(X,Y) | X \\right] \\right\\}^2.\\] From this, the expectation of the conditional variance is \\[\\begin{align} \\mathrm{E}\\{\\mathrm{Var} \\left[ h(X,Y) | X \\right] \\} &amp;= \\mathrm{E}\\left\\{\\mathrm{E} \\left[ h(X,Y)^2 | X \\right] \\right\\} - \\mathrm{E}\\left(\\left\\{\\mathrm{E} \\left[ h(X,Y) | X \\right] \\right\\}^2\\right) \\notag \\\\ &amp;=\\mathrm{E} \\left[ h(X,Y)^2\\right] - \\mathrm{E}\\left(\\left\\{\\mathrm{E} \\left[ h(X,Y) | X \\right] \\right\\}^2\\right).\\tag{16.1} \\end{align}\\] Further, note that the conditional expectation, \\(\\mathrm{E} \\left[ h(X,Y) | X \\right]\\), is a function of \\(X\\), denoted \\(g(X)\\). Thus, \\(g(X)\\) is a random variable with mean \\(\\mathrm{E}[h(X,Y)]\\) and variance \\[\\begin{align} \\mathrm{Var} \\left\\{ \\mathrm{E} \\left[ h(X,Y) | X \\right] \\right \\} &amp;=\\mathrm{Var}[g(X)] \\notag \\\\ &amp;= \\mathrm{E}[g(X)^2]\\ - \\left\\{\\mathrm{E}[g(X)]\\right\\}^2 \\nonumber\\\\ &amp;= \\mathrm{E}\\left(\\left\\{\\mathrm{E} \\left[ h(X,Y) | X \\right] \\right\\}^2\\right) - \\left\\{\\mathrm{E}[h(X,Y)]\\right\\}^2.\\tag{16.2} \\end{align}\\] Thus, adding Equations (16.1) and (16.2) leads to the unconditional variance \\(\\mathrm{Var} \\left[ h(X,Y) \\right]\\). 16.2.3 Application To apply the Law of Iterated Expectations and the Law of Total Variance, we generally adopt the following procedure. Identify the random variable that is being conditioned upon, typically a stage 1 outcome (that is not observed). Conditional on the stage 1 outcome, calculate summary measures such as a mean, variance, and the like. There are several results of the step 2, one for each stage 1 outcome. Then, combine these results using the iterated expectations or total variance rules. Mixtures of Finite Populations. Suppose that the random variable \\(N_1\\) represents a realization of the number of claims in a policy year from the population of good drivers and \\(N_2\\) represents that from the population of bad drivers. For a specific driver, there is a probability \\(\\alpha\\) that (s)he is a good driver. For a specific draw \\(N\\), we have \\[N = \\begin{cases} N_1, &amp; \\text{if (s)he is a good driver;}\\\\ N_2, &amp; \\text{otherwise}.\\\\ \\end{cases}\\] Let \\(T\\) be the indicator whether (s)he is a good driver, with \\(T=1\\) representing that the driver is a good driver with \\(\\Pr[T=1]=\\alpha\\) and \\(T=2\\) representing that the driver is a bad driver with \\(\\Pr[T=2]=1-\\alpha\\). From the Law of Iterated Expectations, we can obtain the expected number of claims as \\[ \\mathrm{E}[N]= \\mathrm{E} \\left\\{ \\mathrm{E} \\left[ N | T \\right] \\right \\}= \\mathrm{E}[N_1] \\times \\alpha + \\mathrm{E}[N_2] \\times (1-\\alpha).\\] From the Law of Total Variance, we can obtain the variance of \\(N\\) as \\[\\mathrm{Var}[N]= \\mathrm{E} \\left\\{ \\mathrm{Var} \\left[ N | T \\right] \\right \\} +\\mathrm{Var} \\left\\{ \\mathrm{E} \\left[ N | T \\right] \\right \\}.\\] To be more concrete, suppose that \\(N_j\\) follows a Poisson distribution with the mean \\(\\lambda_j\\), \\(j=1,2\\). Then we have \\[\\mathrm{Var}[N|T=j]= \\mathrm{E}[N|T=j] = \\lambda_j, \\quad j = 1,2.\\] Thus, we can derive the expectation of the conditional variance as \\[\\mathrm{E} \\left\\{ \\mathrm{Var} \\left[ N | T \\right] \\right \\} = \\alpha \\lambda_1+ (1-\\alpha) \\lambda_2\\] and the variance of the conditional expectation as \\[\\mathrm{Var} \\left\\{ \\mathrm{E} \\left[ N | T \\right] \\right \\} = (\\lambda_1-\\lambda_2)^2 \\alpha (1-\\alpha).\\] Note that the later is the variance for a Bernoulli with outcomes \\(\\lambda_1\\) and \\(\\lambda_2\\), and the binomial probability \\(\\alpha\\). Based on the Law of Total Variance, the unconditional variance of \\(N\\) is given by \\[\\mathrm{Var}[N]= \\alpha \\lambda_1+ (1-\\alpha) \\lambda_2 + (\\lambda_1-\\lambda_2)^2 \\alpha (1-\\alpha).\\] "],
["C-AppC.html", "Chapter 17 Appendix C: Maximum Likelihood Theory 17.1 Likelihood Function 17.2 Maximum Likelihood Estimators 17.3 Statistical Inference Based on Maximum Likelhood Estimation", " Chapter 17 Appendix C: Maximum Likelihood Theory Chapter preview. Appendix Chapter 15 introduced the maximum likelihood theory regarding estimation of parameters from a parametric family. This appendix gives more specific examples and expands some of the concepts. Section 17.1 reviews the definition of the likelihood function, and introduces its properties. Section 17.2 reviews the maximum likelihood estimators, and extends their large-sample properties to the case where there are multiple parameters in the model. Section 17.3 reviews statistical inference based on maximum likelihood estimators, with specific examples on cases with multiple parameters. 17.1 Likelihood Function In this section, you learn the definitions of the likelihood function and the log-likelihood function the properties of the likelihood function From Appendix 15, the likelihood function is a function of parameters given the observed data. Here, we review the concepts of the likelihood function, and introduces its properties that are bases for maximum likelihood inference. 17.1.1 Likelihood and Log-likelihood Functions Here, we give a brief review of the likelihood function and the log-likelihood function from Appendix 15. Let \\(f(\\cdot|\\boldsymbol\\theta)\\) be the probability function of \\(X\\), the probability mass function (pmf) if \\(X\\) is discrete or the probability density function (pdf) if it is continuous. The likelihood is a function of the parameters (\\(\\boldsymbol \\theta\\)) given the data (\\(\\mathbf{x}\\)). Hence, it is a function of the parameters with the data being fixed, rather than a function of the data with the parameters being fixed. The vector of data \\(\\mathbf{x}\\) is usually a realization of a random sample as defined in Appendix 15. Given a realized of a random sample \\(\\mathbf{x}=(x_1,x_2,\\cdots,x_n)\\) of size \\(n\\), the likelihood function is defined as \\[L(\\boldsymbol{\\theta}|\\mathbf{x})=f(\\mathbf{x}|\\boldsymbol{\\theta})=\\prod_{i=1}^nf(x_i|\\boldsymbol{\\theta}),\\] with the corresponding log-likelihood function given by \\[l(\\boldsymbol{\\theta}|\\mathbf{x})=\\ln L(\\boldsymbol{\\theta}|\\mathbf{x})=\\sum_{i=1}^n\\ln f(x_i|\\boldsymbol{\\theta}),\\] where \\(f(\\mathbf{x}|\\boldsymbol{\\theta})\\) denotes the joint probability function of \\(\\mathbf{x}\\). The log-likelihood function leads to an additive structure that is easy to work with. In Appendix 15, we have used the normal distribution to illustrate concepts of the likelihood function and the log-likelihood function. Here, we derive the likelihood and corresponding log-likelihood functions when the population distribution is from the Pareto distribution family. Show Example Example – Pareto Distribution. Suppose that \\(X_1, \\ldots, X_n\\) represents a random sample from a single-parameter Pareto distribution with the cumulative distribution function given by \\[F(x) = \\Pr(X_i\\leq x)=1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500,\\] where the parameter \\(\\theta = \\alpha\\). The corresponding probability density function is \\(f(x) = 500^{\\alpha} \\alpha x^{-\\alpha-1}\\) and the log-likelihood function can be derived as \\[l(\\boldsymbol \\alpha|\\mathbf{x}) = \\sum_{i=1}^n \\ln f(x_i;\\alpha) = n \\alpha \\ln 500 +n \\ln \\alpha -(\\alpha+1) \\sum_{i=1}^n \\ln x_i .\\] 17.1.2 Properties of Likelihood Functions In mathematical statistics, the first derivative of the log-likelihood function with respect to the parameters, \\(u(\\boldsymbol\\theta)=\\partial l(\\boldsymbol \\theta|\\mathbf{x})/\\partial \\boldsymbol \\theta\\), is referred to as the score function, or the score vector when there are multiple parameters in \\(\\boldsymbol\\theta\\). The score function or score vector can be written as \\[u(\\boldsymbol\\theta)=\\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) =\\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln \\prod_{i=1}^n f(x_i;\\boldsymbol \\theta ) =\\sum_{i=1}^n \\frac{ \\partial}{\\partial \\boldsymbol \\theta} \\ln f(x_i;\\boldsymbol \\theta ),\\] where \\(u(\\boldsymbol\\theta)=(u_1(\\boldsymbol\\theta),u_2(\\boldsymbol\\theta),\\cdots,u_p(\\boldsymbol\\theta))\\) when \\(\\boldsymbol\\theta=(\\theta_1,\\cdots,\\theta_p)\\) contains \\(p&gt;2\\) parameters, with the element \\(u_k(\\boldsymbol\\theta)=\\partial l(\\boldsymbol \\theta|\\mathbf{x})/\\partial \\theta_k\\) being the partial derivative with respect to \\(\\theta_k\\) (\\(k=1,2,\\cdots,p\\)). The likelihood function has the following properties: One basic property of the likelihood function is that the expectation of the score function with respect to \\(\\mathbf{x}\\) is 0. That is, \\[\\mathrm{E}[u(\\boldsymbol\\theta)]=\\mathrm{E} \\left[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) \\right] = \\mathbf 0\\] To illustrate this, we have \\[\\begin{aligned} \\mathrm{E} \\left[ \\frac{ \\partial}{\\partial \\boldsymbol \\theta} l(\\boldsymbol \\theta|\\mathbf{x}) \\right] &amp;= \\mathrm{E} \\left[ \\frac{\\frac{\\partial}{\\partial \\boldsymbol \\theta}f(\\mathbf{x};\\boldsymbol \\theta)}{f(\\mathbf{x};\\boldsymbol \\theta )} \\right] = \\int\\frac{\\partial}{\\partial \\boldsymbol \\theta} f(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y \\\\ &amp;= \\frac{\\partial}{\\partial \\boldsymbol \\theta} \\int f(\\mathbf{y};\\boldsymbol \\theta ) d \\mathbf y = \\frac{\\partial}{\\partial \\boldsymbol \\theta} 1 = \\mathbf 0.\\end{aligned}\\] Denote by \\({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}}={ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta^{2}}\\) the second derivative of the log-likelihood function when \\(\\boldsymbol\\theta\\) is a single parameter, or by \\({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}}=(h_{jk})=({ \\partial^2 l(\\boldsymbol \\theta|\\mathbf{x}) }/\\partial x_j\\partial x_k)\\) the hessian matrix of the log-likelihood function when it contains multiple parameters. Denote \\([{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta}][{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta&#39;}]=u^2(\\boldsymbol \\theta)\\) when \\(\\boldsymbol\\theta\\) is a single parameter, or let \\([{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta}][{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta&#39;}]=(uu_{jk})\\) be a \\(p\\times p\\) matrix when \\(\\boldsymbol\\theta\\) contains a total of \\(p\\) parameters, with each element \\(uu_{jk}=u_j(\\boldsymbol \\theta)u_k(\\boldsymbol \\theta)\\) and \\(u_j(\\boldsymbol \\theta)\\) being the \\(k\\)th element of the score vector as defined earlier. Another basic property of the likelihood function is that sum of the expectation of the hessian matrix and the expectation of the kronecker product of the score vector and its transpose is \\(\\mathbf 0\\). That is, \\[\\mathrm{E} \\left( \\frac{ \\partial^2 }{\\partial \\boldsymbol \\theta\\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right) + \\mathrm{E} \\left( \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta} \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial\\boldsymbol \\theta^{\\prime}}\\right) = \\mathbf 0.\\] Define the Fisher information matrix as \\[ \\mathcal{I}(\\boldsymbol \\theta) = \\mathrm{E} \\left( \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial \\boldsymbol \\theta} \\frac{ \\partial l(\\boldsymbol \\theta|\\mathbf{x})}{\\partial \\boldsymbol \\theta^{\\prime}} \\right) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right).\\] As the sample size \\(n\\) goes to infinity, the score function (vector) converges in distribution to a normal distribution (or multivariate normal distribution when \\(\\boldsymbol \\theta\\) contains multiple parameters) with mean 0 and variance (or covariance matrix in the multivariate case) given by \\(\\mathcal{I}(\\boldsymbol \\theta)\\). 17.2 Maximum Likelihood Estimators In this section, you learn the definition and derivation of the maximum likelihood estimator (MLE) for parameters from a specific distribution family the properties of maximum likelihood estimators that ensure valid large-sample inference of the parameters why using the MLE-based method, and what caution that needs to be taken In statistics, maximum likelihood estimators are values of the parameters \\(\\boldsymbol \\theta\\) that are most likely to have been produced by the data. 17.2.1 Definition and Derivation of MLE Based on the definition given in Appendix 15, the value of \\(\\boldsymbol \\theta\\), say \\(\\hat{\\boldsymbol \\theta}_{MLE}\\), that maximizes the likelihood function, is called the maximum likelihood estimator (MLE) of \\(\\boldsymbol \\theta\\). Because the log function \\(\\ln(\\cdot)\\) is a one-to-one function, we can also determine \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\) by maximizing the log-likelihood function, \\(l(\\boldsymbol \\theta|\\mathbf{x})\\). That is, the MLE is defined as \\[\\hat{\\boldsymbol \\theta}_{MLE}={\\mbox{argmax}}_{\\boldsymbol{\\theta}\\in\\Theta}l(\\boldsymbol{\\theta}|\\mathbf{x}).\\] Given the analytical form of the likelihood function, the MLE can be obtained by taking the first derivative of the log-likelihood function with respect to \\(\\boldsymbol{\\theta}\\), and setting the values of the partial derivatives to zero. That is, the MLE are the solutions of the equations of \\[\\frac{\\partial l(\\hat{\\boldsymbol{\\theta}}|\\mathbf{x})}{\\partial\\hat{\\boldsymbol{\\theta}}}=\\mathbf 0.\\] Show Example Example. Course C/Exam 4. May 2000, 21. You are given the following five observations: 521, 658, 702, 819, 1217. You use the single-parameter Pareto with cumulative distribution function: \\[F(x) = 1- \\left(\\frac{500}{x}\\right)^{\\alpha}, ~~~~ x&gt;500 .\\] Calculate the maximum likelihood estimate of the parameter \\(\\alpha\\). Show Solution Solution. With \\(n=5\\), the log-likelihood function is \\[l(\\alpha|\\mathbf{x} ) = \\sum_{i=1}^5 \\ln f(x_i;\\alpha ) = 5 \\alpha \\ln 500 + 5 \\ln \\alpha -(\\alpha+1) \\sum_{i=1}^5 \\ln x_i.\\] Solving for the root of the score function yields \\[\\frac{ \\partial}{\\partial \\alpha } l(\\alpha |\\mathbf{x}) = 5 \\ln 500 + 5 / \\alpha - \\sum_{i=1}^5 \\ln x_i =_{set} 0 \\Rightarrow \\hat{\\alpha}_{MLE} = \\frac{5}{\\sum_{i=1}^5 \\ln x_i - 5 \\ln 500 } = 2.453 .\\] 17.2.2 Asymptotic Properties of MLE From Appendix 15, the MLE has some nice large-sample properties, under certain regularity conditions. We presented the results for a single parameter in Appendix 15, but results are true for the case when \\(\\boldsymbol{\\theta}\\) contains multiple parameters. In particular, we have the following results, in a general case when \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots,\\theta_p)\\). The MLE of a parameter \\(\\boldsymbol{\\theta}\\), \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\), is a consistent estimator. That is, the MLE \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\) converges in probability to the true value \\(\\boldsymbol{\\theta}\\), as the sample size \\(n\\) goes to infinity. The MLE has the asymptotic normality property, meaning that the estimator will converge in distribution to a multivariate normal distribution centered around the true value, when the sample size goes to infinity. Namely, \\[\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}_{MLE}-\\boldsymbol{\\theta})\\rightarrow N\\left(\\mathbf 0,\\,\\boldsymbol{V}\\right),\\quad \\mbox{as}\\quad n\\rightarrow \\infty,\\] where \\(\\boldsymbol{V}\\) denotes the asymptotic variance (or covariance matrix) of the estimator. Hence, the MLE \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\) has an approximate normal distribution with mean \\(\\boldsymbol{\\theta}\\) and variance (covariance matrix when \\(p&gt;1\\)) \\(\\boldsymbol{V}/n\\), when the sample size is large. The MLE is efficient, meaning that it has the smallest asymptotic variance \\(\\boldsymbol{V}\\), commonly referred to as the Cramer–Rao lower bound. In particular, the Cramer–Rao lower bound is the inverse of the Fisher information (matrix) \\(\\mathcal{I}(\\boldsymbol{\\theta})\\) defined earlier in this appendix. Hence, \\(\\mathrm{Var}(\\hat{\\boldsymbol{\\theta}}_{MLE})\\) can be estimated based on the observed Fisher information. Based on the above results, we may perform statistical inference based on the procedures defined in Appendix 15. Show Example Example. Course C/Exam 4. Nov 2000, 13. A sample of ten observations comes from a parametric family \\(f(x,; \\theta_1, \\theta_2)\\) with log-likelihood function \\[l(\\theta_1, \\theta_2)= \\sum_{i=1}^{10} f(x_i; \\theta_1, \\theta_2) = -2.5 \\theta_1^2 - 3 \\theta_1 \\theta_2 - \\theta_2^2 + 5 \\theta_1 + 2 \\theta_2 + k,\\] where \\(k\\) is a constant. Determine the estimated covariance matrix of the maximum likelihood estimator, \\(\\hat{\\theta_1}, \\hat{\\theta_2}\\). Show Solution Solution. Denoting \\(l=l(\\theta_1, \\theta_2)\\), the hessian matrix of second derivatives is \\[\\left( \\begin{array}{cc} \\frac{ \\partial ^2}{\\partial \\theta_1 ^2 } l &amp; \\frac{ \\partial ^2}{\\partial \\theta_1 \\partial \\theta_2 } l \\\\ \\frac{ \\partial ^2}{\\partial \\theta_1 \\partial \\theta_2 } l &amp; \\frac{ \\partial ^2}{\\partial \\theta_1 ^2 } l \\end{array} \\right) = \\left( \\begin{array}{cc} -5 &amp; -3 \\\\ -3 &amp; -2 \\end{array} \\right)\\] Thus, the information matrix is: \\[\\mathcal{I}(\\theta_1, \\theta_2) = -\\mathrm{E} \\left( \\frac{ \\partial^2}{\\partial \\boldsymbol \\theta \\partial \\boldsymbol \\theta^{\\prime}} l(\\boldsymbol \\theta|\\mathbf{x}) \\right) = \\left( \\begin{array}{cc} 5 &amp; 3 \\\\ 3 &amp; 2 \\end{array} \\right)\\] and \\[\\mathcal{I}^{-1}(\\theta_1, \\theta_2) = \\frac{1}{5(2) - 3(3)}\\left( \\begin{array}{cc} 2 &amp; -3 \\\\ -3 &amp; 5 \\end{array} \\right) = \\left( \\begin{array}{cc} 2 &amp; -3 \\\\ -3 &amp; 5 \\end{array} \\right) .\\] 17.2.3 Use of Maximum Likelihood Estimation The method of maximum likelihood has many advantages over alternative methods such as the method of moment method introduced in Appendix 15. It is a general tool that works in many situations. For example, we may be able to write out the closed-form likelihood function for censored and truncated data. Maximum likelihood estimation can be used for regression models including covariates, such as survival regression, generalized linear models and mixed models, that may include covariates that are time-dependent. From the efficiency of the MLE, it is optimal, the best, in the sense that it has the smallest variance among the class of all unbiased estimators for large sample sizes. From the results on the asymptotic normality of the MLE, we can obtain a large-sample distribution for the estimator, allowing users to assess the variability in the estimation and perform statistical inference on the parameters. The approach is less computationally extensive than re-sampling methods that require a large of fittings of the model. Despite its numerous advantages, MLE has its drawback in cases such as generalized linear models when it does not have a closed analytical form. In such cases, maximum likelihood estimators are computed iteratively using numerical optimization methods. For example, we may use the Newton-Raphson iterative algorithm or its variations for obtaining the MLE. Iterative algorithms require starting values. For some problems, the choice of a close starting value is critical, particularly in cases where the likelihood function has local minimums or maximums. Hence, there may be a convergence issue when the starting value is far from the maximum. Hence, it is important to start from different values across the parameter space, and compare the maximized likelihood or log-likelihood to make sure the algorithms have converged to a global maximum. 17.3 Statistical Inference Based on Maximum Likelhood Estimation In this section, you learn how to perform hypothesis testing based on MLE for cases where there are multiple parameters in \\(\\boldsymbol\\theta\\) perform likelihood ratio test for cases where there are multiple parameters in \\(\\boldsymbol\\theta\\) In Appendix 15, we have introduced maximum likelihood-based methods for statistical inference when \\(\\boldsymbol\\theta\\) contains a single parameter. Here, we will extend the results to cases where there are multiple parameters in \\(\\boldsymbol\\theta\\). 17.3.1 Hypothesis Testing In Appendix 15, we defined hypothesis testing concerning the null hypothesis, a statement on the parameter(s) of a distribution or model. One important type of inference is to assess whether a parameter estimate is statistically significant, meaning whether the value of the parameter is zero or not. We have learned earlier that the MLE \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\) has a large-sample normal distribution with mean \\(\\boldsymbol \\theta\\) and the variance covariance matrix \\(\\mathcal{I}^{-1}(\\boldsymbol \\theta)\\). Based on the multivariate normal distribution, the \\(j\\)th element of \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\), say \\(\\hat{\\theta}_{MLE,j}\\), has a large-sample univariate normal distribution. Define \\(se(\\hat{\\theta}_{MLE,j})\\), the standard error (estimated standard deviation) to be the square root of the \\(j\\)th diagonal element of \\(\\mathcal{I}^{-1}(\\boldsymbol \\theta)_{MLE}\\). To assess the null hypothesis that \\(\\theta_j=\\theta_0\\), we define the \\(t\\)-statistic or \\(t\\)-ratio to be \\(t(\\hat{\\theta}_{MLE,j})=(\\hat{\\theta}_{MLE,j}-\\theta_0)/se(\\hat{\\theta}_{MLE,j})\\). Under the null hypothesis, it has a Student-\\(t\\) distribution with degrees of freedom equal to \\(n-p\\), with \\(p\\) being the dimension of \\(\\boldsymbol{\\theta}\\). For most actuarial applications, we have a large sample size \\(n\\), so the \\(t\\)-distribution is very close to the (standard) normal distribution. In the case when \\(n\\) is very large or when the standard error is known, the \\(t\\)-statistic can be referred to as a \\(z\\)-statistic or \\(z\\)-score. Based on the results from Appendix 15, if the \\(t\\)-statistic \\(t(\\hat{\\theta}_{MLE,j})\\) exceeds a cut-off (in absolute value), then the test for the \\(j\\) parameter \\(\\theta_j\\) is said to be statistically significant. If \\(\\theta_j\\) is the regression coefficient of the \\(j\\) th independent variable, then we say that the \\(j\\)th variable is statistically significant. For example, if we use a 5% significance level, then the cut-off value is 1.96 using a normal distribution approximation for cases with a large sample size. More generally, using a \\(100 \\alpha \\%\\) significance level, then the cut-off is a \\(100(1-\\alpha/2)\\%\\) quantile from a Student-\\(t\\) distribution with the degree of freedom being \\(n-p\\). Another useful concept in hypothesis testing is the \\(p\\)-value, shorthand for probability value. From the mathematical definition in Appendix 15, a \\(p\\)-value is defined as the smallest significance level for which the null hypothesis would be rejected. Hence, the \\(p\\)-value is a useful summary statistic for the data analyst to report because it allows the reader to understand the strength of statistical evidence concerning the deviation from the null hypothesis. 17.3.2 MLE and Model Validation In addition to hypothesis testing and interval estimation introduced in Appendix 15 and the previous subsection, another important type of inference is selection of a model from two choices, where one choice is a special case of the other with certain parameters being restricted. For such two models with one being nested in the other, we have introduced the likelihood ratio test (LRT) in Appendix 15. Here, we will briefly review the process of performing a LRT based on a specific example of two alternative models. Suppose that we have a (large) model under which we derive the maximum likelihood estimator, \\(\\hat{\\boldsymbol{\\theta}}_{MLE}\\). Now assume that some of the \\(p\\) elements in \\(\\boldsymbol \\theta\\) are equal to zero and determine the maximum likelihood estimator over the remaining set, with the resulting estimator denoted \\(\\hat{\\boldsymbol{\\theta}}_{Reduced}\\). Based on the definition in Appendix 15, the statistic, \\(LRT= 2 \\left( l(\\hat{\\boldsymbol{\\theta}}_{MLE}) - l(\\hat{\\boldsymbol{\\theta}}_{Reduced}) \\right)\\), is called the likelihood ratio statistic. Under the null hypothesis that the reduced model is correct, the likelihood ratio has a Chi-square distribution with degrees of freedom equal to \\(d\\), the number of variables set to zero. Such a test allows us to judge which of the two models is more likely to be correct, given the observed data. If the statistic \\(LRT\\) is large relative to the critical value from the chi-square distribution, then we reject the reduced model in favor of the larger one. Details regarding the critical value and alternative methods based on information criteria are given in Appendix 15. "],
["bibliography.html", "Bibliography", " Bibliography "]
]
